{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "jail_web_scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Links\n",
        "\n",
        "---\n",
        "\n",
        "Harrison - https://omsweb.public-safety-cloud.com/jtclientweb/jailtracker/index/HARRISON_COUNTY_JAIL_MS/\n",
        "\n",
        "Marion - https://omsweb.public-safety-cloud.com/jtclientweb/jailtracker/index/Marion_County_MS/\n",
        "\n",
        "Pearl River - https://www.pearlrivercounty.net/sheriff/files/ICURRENT.HTM\n",
        "\n",
        "Perry - https://omsweb.public-safety-cloud.com/jtclientweb/jailtracker/index/Perry_County_MS/\n",
        "\n",
        "Yazoo - https://omsweb.public-safety-cloud.com/jtclientweb/jailtracker/index/Yazoo_County_MS/"
      ],
      "metadata": {
        "id": "fvO3wm680ICC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union, Dict\n",
        "from enum import Enum, unique\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "import urllib\n",
        "import requests\n",
        "import ssl\n",
        "import certifi\n",
        "\n",
        "from random import randint\n",
        "\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import multiprocessing\n",
        "import IPython\n",
        "\n",
        "# Used for debugging; pretty print JSON strings\n",
        "import json"
      ],
      "metadata": {
        "id": "OS7Po5Gqy7tr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lhqLGdsry29O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35498c02-934d-463b-e3bb-89301cf18a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/drive/MyDrive/'):\n",
        "    raise Exception(\"ERROR: Mount Google Drive before continuing!\")\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/Data Science for Social Good - Spring 2022/data/'\n",
        "\n",
        "# Define directory that contains intermediate SSL certificates\n",
        "CERT_DIR = BASE_DIR + 'certificates/'\n",
        "\n",
        "# Define directories to save data\n",
        "SCRAPE_DIR = BASE_DIR + 'scraped_files/'\n",
        "DATA_DIR = SCRAPE_DIR + 'DATA/'\n",
        "\n",
        "CURRENT_DATE = datetime.now(timezone('US/Eastern')).strftime('%m-%d-%Y')\n",
        "\n",
        "# Create all directories on the given paths if needed\n",
        "os.makedirs(f'{DATA_DIR}{CURRENT_DATE}', exist_ok=True)\n",
        "print(\"Date used for scraping:\", CURRENT_DATE)\n",
        "\n",
        "# Each scraper will print its progress at the given increment\n",
        "INMATE_PROGRESS_INCREMENT = 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9MsX6May6ms",
        "outputId": "bf70769f-5848-4958-fbc4-0f65e6bd86ed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date used for scraping: 04-14-2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions and Classes\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "fR8wSuIfzlDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@unique\n",
        "class CountyWithCaptcha(Enum):\n",
        "    DESOTO = 'DeSoto'\n",
        "    FORREST = 'Forrest'\n",
        "    HANCOCK = 'Hancock'\n",
        "    HARRISON = 'Harrison'\n",
        "    LAMAR = 'Lamar'\n",
        "    MARION = 'Marion'\n",
        "    PERRY = 'Perry'\n",
        "    YAZOO = 'Yazoo'\n",
        "\n",
        "    # Return Name, rather than CountyWithCaptcha.Name\n",
        "    def __str__(self) -> str:\n",
        "        return self.name\n",
        "\n",
        "\n",
        "@unique\n",
        "class CountyWithoutCaptcha(Enum):\n",
        "    # The commented out counties only have the total bond, not bond by crime\n",
        "    PEARL_RIVER = 'PearlRiver'\n",
        "    JACKSON = 'Jackson'\n",
        "    MADISON = 'Madison'\n",
        "    LEE = 'Lee'\n",
        "    # ADAMS = 'Adams'\n",
        "    # CLAY = 'Clay'\n",
        "    # JONES = 'Jones'\n",
        "    # TUNICA = 'Tunica'\n",
        "\n",
        "    # Return Name, rather than CountyWithoutCaptcha.Name\n",
        "    def __str__(self) -> str:\n",
        "        return self.name\n",
        "\n",
        "\n",
        "# Define a custom type to pass into functions\n",
        "County = Union[CountyWithCaptcha, CountyWithoutCaptcha]\n",
        "\n",
        "\n",
        "def get_county_filename(county: County) -> str:\n",
        "    return f'{str(CURRENT_DATE)}_{county.value}.csv'"
      ],
      "metadata": {
        "id": "6a2YyDv2zpw4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jail_links = {\n",
        "    CountyWithCaptcha.DESOTO: 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/DeSoto_County_MS/',\n",
        "    CountyWithCaptcha.FORREST: 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Forrest_County_MS/',\n",
        "    CountyWithCaptcha.HANCOCK: 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/HANCOCK_COUNTY_MS/',\n",
        "    CountyWithCaptcha.HARRISON: 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/HARRISON_COUNTY_JAIL_MS/',\n",
        "    CountyWithCaptcha.LAMAR: 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Lamar_County_MS/',\n",
        "    CountyWithCaptcha.MARION: 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Marion_County_MS/',\n",
        "    CountyWithCaptcha.PERRY: 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Perry_County_MS/',\n",
        "    CountyWithCaptcha.YAZOO: 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Yazoo_County_MS/',\n",
        "    CountyWithoutCaptcha.PEARL_RIVER: 'https://www.pearlrivercounty.net/sheriff/files/',\n",
        "    CountyWithoutCaptcha.MADISON: 'https://mydcstraining.com/agencyinfo/MS/4360/inmate/',\n",
        "    CountyWithoutCaptcha.LEE: 'https://tcsi-roster.azurewebsites.net/',\n",
        "}\n"
      ],
      "metadata": {
        "id": "1FnFWhPZ4NQx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_url(request: Union[str, urllib.request.Request]) -> str:\n",
        "    NUM_SECONDS_TIMEOUT = 10\n",
        "\n",
        "    ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH, cafile=certifi.where())\n",
        "    response = urllib.request.urlopen(request, timeout=NUM_SECONDS_TIMEOUT, context=ctx)\n",
        "    data = response.read()\n",
        "    response.close()\n",
        "    return data\n",
        "\n",
        "\n",
        "def read_url_with_retries(url: str, use_headers: bool = False) -> str:\n",
        "    if use_headers:\n",
        "        user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
        "        headers = {'User-Agent': user_agent}\n",
        "        request = urllib.request.Request(url, headers=headers)\n",
        "    else:\n",
        "        request = url\n",
        "\n",
        "    # Try the connection until success or NUM_ATTEMPTS is exceeded\n",
        "    NUM_ATTEMPTS = 5\n",
        "    for _ in range(NUM_ATTEMPTS):\n",
        "        try:\n",
        "            return read_url(request)\n",
        "        except urllib.error.URLError as str_error:\n",
        "            time.sleep(0.5)\n",
        "            print(\"WARNING: Failed to connect to:\", url, str_error)\n",
        "\n",
        "    print(\"ERROR: Request failed for\", url)\n",
        "    return None"
      ],
      "metadata": {
        "id": "akoAUMqFzDvo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add missing intermediate SSL certificates\n",
        "\n",
        "---\n",
        "\n",
        "This should only be run once per session.\n",
        "\n",
        "According to https://stackoverflow.com/a/64835339, Python cannot automatically download intermediate SSL certificates. \n",
        "\n",
        "For Jackson, this results in an SSL: CERTIFICATE_VERIFY_FAILED error. We add the intermediate certificates found at: https://services.co.jackson.ms.us/jaildocket/_inmateList.php"
      ],
      "metadata": {
        "id": "yTJ_imq6k-BT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(certifi.where(), 'a') as global_cert_file:\n",
        "    for filename in os.listdir(CERT_DIR):\n",
        "        with open(CERT_DIR + filename, 'r') as missing_cert_file:\n",
        "            cert_data = missing_cert_file.read()\n",
        "            global_cert_file.write('\\n' + cert_data)"
      ],
      "metadata": {
        "id": "Y2Q6Hac-k62j"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counties with Captchas Scraper\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "pg782r6e4lKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def county_with_captcha_scraper(county: CountyWithCaptcha, captcha_key: str) -> None:\n",
        "    jail_url = jail_links[county]\n",
        "    jail_record_data = requests.post(\n",
        "        jail_url,\n",
        "        json={'captchaKey': captcha_key}\n",
        "    )\n",
        "    jail_record_data_json = jail_record_data.json()\n",
        "    offender_view_key = jail_record_data_json['offenderViewKey']\n",
        "    num_offenders = len(jail_record_data_json['offenders'])\n",
        "    print(county, \"Number of offenders:\", num_offenders)\n",
        "\n",
        "    def get_inmate_url(arrest_num: str) -> str:\n",
        "        def gen_random_num_with_n_digits(n: int) -> int:\n",
        "            \"\"\"Based on: https://stackoverflow.com/a/2673399\"\"\"\n",
        "            range_start = 10 ** (n - 1)\n",
        "            range_end = (10 ** n) - 1\n",
        "            return randint(range_start, range_end)\n",
        "\n",
        "        # Apparently the offender view key is just a random number (and the length doesn't matter)?\n",
        "        # We need an offender view key to make our request\n",
        "        # We use the default length that the website uses, which is 9\n",
        "        LEN_VIEW_KEY = 9\n",
        "        random_offender_view_key = gen_random_num_with_n_digits(LEN_VIEW_KEY)\n",
        "        return f'{jail_url}{arrest_num}/offenderbucket/{random_offender_view_key}'\n",
        "\n",
        "    offender_data = []\n",
        "    cur_num_processed = 0\n",
        "    for offender in jail_record_data_json['offenders']:\n",
        "        arrest_num = offender['arrestNo']\n",
        "        inmate_url = get_inmate_url(arrest_num)\n",
        "        inmate_data = requests.post(\n",
        "            inmate_url,\n",
        "            json={'captchaKey': captcha_key}\n",
        "        )\n",
        "        inmate_data_json = inmate_data.json()\n",
        "\n",
        "        # Add basic data\n",
        "        inmate = {\n",
        "            'Arrest Number': arrest_num,\n",
        "            'Cases': inmate_data_json['cases'],\n",
        "            'Charges': inmate_data_json['charges'],\n",
        "        }\n",
        "\n",
        "        # Add special fields\n",
        "        for field in inmate_data_json['offenderSpecialFields']:\n",
        "            field_name = field['labelText'].strip(':')\n",
        "            inmate[field_name] = field['offenderValue']\n",
        "\n",
        "        offender_data.append(inmate)\n",
        "        cur_num_processed += 1\n",
        "        if cur_num_processed % INMATE_PROGRESS_INCREMENT == 0:\n",
        "            print(county, \"Processed\", cur_num_processed)\n",
        "\n",
        "    filename = get_county_filename(county)\n",
        "    df = pd.DataFrame(offender_data)\n",
        "    df.to_csv(f'{DATA_DIR}{CURRENT_DATE}/{filename}', index=False)"
      ],
      "metadata": {
        "id": "6qjLmPVc4qsA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jackson Script\n",
        "\n",
        "---\n",
        "\n",
        "Occasionally, Jackson will time out. We aren't sure what causes this, but the issue is usually gone when rerunning the scraper later in the day."
      ],
      "metadata": {
        "id": "KQcsHsOo9HuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jackson() -> None:\n",
        "    county = CountyWithoutCaptcha.JACKSON\n",
        "    def parse_inmate_card(page_data: BeautifulSoup) -> Dict:\n",
        "        \"\"\"Return info card and offense card details.\n",
        "\n",
        "        Example html scraped of an offense box:\n",
        "        [<div class=\"offenseItem\"><p class=\"offenseTitle\">POSSESSION WITH INTENT TO DISTRIBUTE A CONTROLLED SUBSTANCE</p><p class=\"offenseDetails\">Felony - Bond: $0.00</p></div>, <div class=\"offenseItem\"><p class=\"offenseTitle\">POSSESSION OF A CONTROLLED SUBSTANCE - ALL OTHERS</p><p class=\"offenseDetails\">Felony - Bond: $0.00</p></div>, <div class=\"offenseItem\"><p class=\"offenseTitle\">TRAFFICKING IN CONTROLLED SUBSTANCES</p><p class=\"offenseDetails\">Felony - Bond: $0.00</p></div>]\n",
        "\n",
        "        Example html scraped of an info box (the fields are always in the same order):\n",
        "        [<p class=\"ilFieldName\">Black Male</p>, <p class=\"ilFieldName\">170 Pounds</p>, <p class=\"inmateInfo\">5 Ft. 08 In. </p>, <p class=\"inmateInfo\">Brown      Eyes </p>, <p class=\"inmateInfo\">35 Years Old</p>, <p class=\"inmateInfo\">Booking #:NJCADC0000023672</p>, <p class=\"inmateInfo red\">Not Bondable</p>]\n",
        "\n",
        "        \"\"\"\n",
        "        # Parse offenses \n",
        "        # Offenses is a list of dictionaries, where each dictionary contains information about the crime\n",
        "        offenses = []\n",
        "        offense_box = page_data.find('article', class_='ofcard').find_all('div', class_='offenseItem')\n",
        "        for offense in offense_box:\n",
        "            offense_title = offense.find('p', class_='offenseTitle').text\n",
        "            offense_details = offense.find('p', class_='offenseDetails').text.split()\n",
        "            offenses.append({\n",
        "                'chargeDescription': offense_title, \n",
        "                'bondAmount': offense_details[-1][1:],\n",
        "                'crimeType': offense_details[0],\n",
        "            })\n",
        "\n",
        "        def get_formatted_height(height: str) -> str:\n",
        "            \"\"\"Return height as:\n",
        "            {feet}' {inches}\"\n",
        "            \"\"\"\n",
        "            return f'''{height[0]}' {height[2]}\"'''\n",
        "\n",
        "        # Parse basic details: height, eye color, age etc.\n",
        "        # Some of the basic details are out of order since some are missing (default missing to empty string)\n",
        "        inmate_info_box = page_data.find_all('p', class_='inmateInfo')\n",
        "        if len(inmate_info_box) == 5:\n",
        "            height_string = inmate_info_box[0].text.split()\n",
        "            height = get_formatted_height(height_string)\n",
        "            eye_color = inmate_info_box[1].text.split()[0]\n",
        "            age = inmate_info_box[2].text.split()[0]\n",
        "        else:\n",
        "            height, eye_color, age = '', '', ''\n",
        "            for detail in inmate_info_box:\n",
        "                if detail.text[-4:] == 'Eyes':\n",
        "                    eye_color = detail.text.split()[0]\n",
        "                elif detail.text[-3:] == 'Old':\n",
        "                    age = detail.text.split()[0]\n",
        "                elif detail.text[-4:] == 'In. ':  # There is a random space in this string\n",
        "                    height_string = detail.text.split()\n",
        "                    height = get_formatted_height(height_string)\n",
        "\n",
        "        # Race and sex format: \"race sex\", where we can have \"not available\" for race and some information can be missing (default missing to empty string)\n",
        "        # Weight format: \"x Pounds\"\n",
        "        inmate_field_box = page_data.find_all('p', class_='ilFieldName')\n",
        "        if len(inmate_field_box) == 2:\n",
        "            race_gender = inmate_field_box[0].text.split()\n",
        "            race = race_gender[0] if race_gender[0] != 'Not' else 'N/A'\n",
        "            sex = race_gender[-1] if race_gender[-1] != 'Available' else 'N/A'\n",
        "            weight = f'{inmate_field_box[1].text.split()[0]} lbs'\n",
        "        else:\n",
        "            weight, sex, race = '', 'N/A', 'N/A'\n",
        "            for detail in inmate_field_box:\n",
        "                if detail.text[-6:] == 'Pounds':\n",
        "                    weight = detail.text.split()[0] + ' lbs'\n",
        "                else:\n",
        "                    if str.lower(detail.text[-3:]) == 'male':\n",
        "                        gender = detail.text.split()[-1]\n",
        "                    if detail.text[:3] != 'Not':\n",
        "                        race = detail.text.split()[0]\n",
        "\n",
        "        return {\n",
        "            'Current Age': age,\n",
        "            'Height': height,\n",
        "            'Weight': weight,\n",
        "            'Eye Color': eye_color,\n",
        "            'Race': race,\n",
        "            'Sex': sex,\n",
        "            'Charges': offenses,\n",
        "        }\n",
        "\n",
        "    # Example of inmate in website_data which is a dictionary\n",
        "    # {\n",
        "    #     \"0\":\"421\",\n",
        "    #     \"RowNum\":\"421\",\n",
        "    #     \"1\":\"NJCADC0000026510\",\n",
        "    #     \"Book_Number\":\"NJCADC0000026510\",\n",
        "    #     \"2\":\"STALLWORTH\",\n",
        "    #     \"Name_Last\":\"STALLWORTH\",\n",
        "    #     \"3\":\"DESHAWN\",\n",
        "    #     \"Name_Middle\":\"DESHAWN\",\n",
        "    #     \"4\":\"TREVION\",\n",
        "    #     \"Name_First_MI\":\"TREVION\",\n",
        "    #     \"5\":\"08\\/16\\/2019\",\n",
        "    #     \"BookDate\":\"08\\/16\\/2019\",\n",
        "    #     \"6\":\"08\\/23\\/2019\",\n",
        "    #     \"ArrestDate\":\"08\\/23\\/2019\",\n",
        "    #     \"7\":\"PPD\",\n",
        "    #     \"Arrest_Agency\":\"PPD\",\n",
        "    #     \"8\":\"\",\n",
        "    #     \"Name_Suffix\":\"\",\n",
        "    #     \"9\":\"200978747                \",\n",
        "    #     \"ID_Number\":\"200978747                \"\n",
        "    # }\n",
        "\n",
        "    # Obtain the total count of inmates\n",
        "    # count_html is a bytes string containing the number of inmates\n",
        "    count_url = 'https://services.co.jackson.ms.us/jaildocket/_inmateList.php?Function=count'\n",
        "    count_html = read_url_with_retries(count_url)\n",
        "    num_offenders = count_html.decode('utf-8')\n",
        "    print(county, \"Number of offenders:\", num_offenders)\n",
        "\n",
        "    offender_data = []\n",
        "    cur_num_processed = 0\n",
        "\n",
        "    # While there is a page with inmate data, collect the inmate IDs\n",
        "    page = 1\n",
        "    while True:\n",
        "        url = f'https://services.co.jackson.ms.us/jaildocket/_inmateList.php?Function=list&Page={page}'\n",
        "        website_html = read_url_with_retries(url)\n",
        "        website_data = json.loads(BeautifulSoup(website_html, 'html.parser').prettify())\n",
        "\n",
        "        # Exit when there is no content on the page\n",
        "        if len(website_data) < 1:\n",
        "            break\n",
        "\n",
        "        # Get details from the inmate card page and the inmate list page\n",
        "        for inmate in website_data:\n",
        "            inmate_id = inmate['ID_Number'].strip()\n",
        "            url = f'https://services.co.jackson.ms.us/jaildocket/inmate/_inmatedetails.php?id={inmate_id}'\n",
        "            inmate_html = read_url_with_retries(url)\n",
        "            inmate_card_data = BeautifulSoup(inmate_html, 'html.parser')\n",
        "            inmate_info = parse_inmate_card(inmate_card_data)\n",
        "\n",
        "            # Store a row of information on an inmate\n",
        "            inmate = {\n",
        "                'Arrest Number': inmate_id,\n",
        "                'First Name': str.upper(inmate['Name_First_MI']),\n",
        "                'Middle Name': str.upper(inmate['Name_Middle']),\n",
        "                'Last Name': str.upper(inmate['Name_Last']),\n",
        "                'Suffix': str.upper(inmate['Name_Suffix']),\n",
        "                'Arrest Date': inmate['ArrestDate'],\n",
        "                'Booking Date': inmate['BookDate'],\n",
        "                'Arrest Agency': str.upper(inmate['Arrest_Agency']),\n",
        "                'Booking Number': inmate['Book_Number'],\n",
        "            }\n",
        "\n",
        "            # Merge the two dictionaries\n",
        "            inmate.update(inmate_info)\n",
        "\n",
        "            offender_data.append(inmate)\n",
        "            cur_num_processed += 1\n",
        "            if cur_num_processed % INMATE_PROGRESS_INCREMENT == 0:\n",
        "                print(county, \"Processed\", cur_num_processed)\n",
        "\n",
        "        page += 1\n",
        "\n",
        "    filename = get_county_filename(county)\n",
        "    df = pd.DataFrame(offender_data)\n",
        "    df.to_csv(f'{DATA_DIR}{CURRENT_DATE}/{filename}', index=False)"
      ],
      "metadata": {
        "id": "ZyTxokJW8yGP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lee Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "c6gTktizd2Iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lee() -> None:\n",
        "    county = CountyWithoutCaptcha.LEE\n",
        "    base_url = jail_links[county]\n",
        "    jail_url = f'{base_url}Default.aspx?i=26&code=Lee'\n",
        "    inmate_info_base_url = f'{base_url}InmateInfo.aspx?i=26&code=Lee&type=&iid='\n",
        "\n",
        "    def get_list_of_inmates(url):\n",
        "        web_html = read_url_with_retries(url, True)\n",
        "        soup = BeautifulSoup(web_html, 'html.parser')\n",
        "        return soup\n",
        "\n",
        "    def find_id_from_tag(tag):\n",
        "        start = str(tag.contents[0]).find('iid=') + 4\n",
        "        end = str(tag.contents[0]).find('\" title')\n",
        "        return inmate_info_base_url + str(tag.contents[0])[start:end]\n",
        "\n",
        "    def get_inmate_id(soup):\n",
        "        ids = soup.find_all('td', {'class': 'hide-for-small-only'})\n",
        "        return list(map(find_id_from_tag, ids))\n",
        "\n",
        "    def check_non(arr):\n",
        "        if len(arr) == 0:\n",
        "            arr.append('N/A')\n",
        "        return arr\n",
        "\n",
        "    def get_info(url):\n",
        "        inmate_detail = {}\n",
        "        web_html = read_url_with_retries(url, True)\n",
        "        soup = BeautifulSoup(web_html, 'html.parser')\n",
        "\n",
        "        full_name = check_non(soup.find('span', {'id': 'lblInmateName'}).contents)\n",
        "        full_name = full_name[0].split(' ')\n",
        "        inmate_detail['First Name'] = full_name[0]\n",
        "        inmate_detail['Last Name'] = full_name[-1]\n",
        "        full_mid = '' if len(full_name[1:len(full_name) - 1]) == 0 else full_name[1:len(full_name) - 1]\n",
        "        tmp_mid = ''\n",
        "        for i in full_mid:\n",
        "            tmp_mid += i\n",
        "\n",
        "        inmate_detail['Middle Name'] = tmp_mid\n",
        "        inmate_detail['Booking Number'] = check_non(soup.find('span', {'id': 'lblBookingNumber'}).contents)[0]\n",
        "        inmate_detail['Booking Date'] = check_non(soup.find('span', {'id': 'lblBookingDate'}).contents)[0]\n",
        "        inmate_detail['Primary Offense'] = check_non(soup.find('span', {'id': 'lblOffense'}).contents)[0]\n",
        "        inmate_detail['Arresting Agency'] = check_non(soup.find('span', {'id': 'lblArrestingAgency'}).contents)[0]\n",
        "        inmate_detail['Current Age'] = check_non(soup.find('span', {'id': 'lblAge'}).contents)[0]\n",
        "        inmate_detail['Sex'] = check_non(soup.find('span', {'id': 'lblSex'}).contents)[0]\n",
        "        height = check_non(soup.find('span', {'id': 'lblHeight'}).contents)[0]\n",
        "        inmate_detail['Height'] = height.replace('ft', \"'\").replace('in', '\"')\n",
        "        inmate_detail['Weight'] = check_non(soup.find('span', {'id': 'lblWeight'}).contents)[0].replace('lbs', ' lbs')\n",
        "        inmate_detail['Race'] = check_non(soup.find('span', {'id': 'lblRace'}).contents)[0]\n",
        "\n",
        "        hair = check_non(soup.find('span', {'id': 'lblHair'}).contents)[0]\n",
        "        content = hair[hair.find('(') + 1:hair.find(')')]\n",
        "        color = hair[:hair.find('(') - 1]\n",
        "        inmate_detail['Hair Color'] = content if content == 'Bald' else color\n",
        "\n",
        "        inmate_detail['Eye Color'] = check_non(soup.find('span', {'id': 'lblEyes'}).contents)[0]\n",
        "        inmate_detail['Glasses'] = check_non(soup.find('span', {'id': 'lblGlasses'}).contents)[0]\n",
        "\n",
        "        inmate_detail['Charges'] = []\n",
        "        prev_charge = ''\n",
        "        curr_charge = ''\n",
        "        for tr in soup.find_all('tr')[1:]:\n",
        "            tds = tr.find_all('td')\n",
        "\n",
        "            if not tds[0].text and not tds[1].text:\n",
        "                continue\n",
        "\n",
        "            if not prev_charge:\n",
        "                prev_charge = tds[0].text\n",
        "            curr_charge = tds[0].text\n",
        "            if not curr_charge:\n",
        "                curr_charge = prev_charge\n",
        "\n",
        "            inmate_detail['Charges'].append({'chargeDescription': curr_charge, 'chargeType': tds[1].text,\n",
        "                                             'bondAmount': tds[2].text.replace('$', ''), 'fineAmount': tds[3].text.replace('$', '')})\n",
        "\n",
        "        return inmate_detail\n",
        "\n",
        "    all_info = pd.DataFrame()\n",
        "    detail_urls = get_inmate_id(get_list_of_inmates(jail_url))\n",
        "    print(county, \"Number of offenders:\", str(len(detail_urls)))\n",
        "\n",
        "    cur_num_processed = 0\n",
        "    for inmate_url in detail_urls:\n",
        "        info_arr = get_info(inmate_url)\n",
        "        all_info = all_info.append(info_arr, ignore_index=True)\n",
        "\n",
        "        cur_num_processed += 1\n",
        "        if cur_num_processed % INMATE_PROGRESS_INCREMENT == 0:\n",
        "            print(county, \"Processed\", cur_num_processed)\n",
        "    \n",
        "    filename = get_county_filename(county)\n",
        "    all_info.to_csv(f'{DATA_DIR}{CURRENT_DATE}/{filename}', index=False)"
      ],
      "metadata": {
        "id": "IJxfZHiWd1Vf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pearl River and Madison Script\n",
        "\n",
        "---\n",
        "\n",
        "Frequently, Pearl River will time out. We aren't sure what causes this, but it may be the web scraper being temporarily blocked by the website. This shouldn't be an issue, as we retry the connection automatically, and it usually works after retrying."
      ],
      "metadata": {
        "id": "s6Cto76ZDLjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pearlriver_madison(county: CountyWithoutCaptcha) -> None:\n",
        "    base_url = jail_links[county]\n",
        "    all_url = base_url + 'ICURRENT.HTM'\n",
        "\n",
        "    def get_list_of_inmates_url(url):\n",
        "        detail_urls = []\n",
        "        web_html = read_url_with_retries(url, True)\n",
        "        soup = BeautifulSoup(web_html, 'html.parser')\n",
        "        suffix_set = soup.findAll('a')\n",
        "        for inmate_entry in suffix_set:\n",
        "            detail_urls.append(base_url + inmate_entry['href'])\n",
        "        return detail_urls\n",
        "\n",
        "    def get_info(url):\n",
        "        inmate_detail = {}\n",
        "        web_html = read_url_with_retries(url, True)\n",
        "        soup = BeautifulSoup(web_html, 'html.parser')\n",
        "\n",
        "        # Check if the link has content (check name is the fastest way)\n",
        "        if soup.find('b', text='Name: ') == None:\n",
        "            return None\n",
        "        full_name = soup.find('b', text='Name: ').next_sibling.split()\n",
        "\n",
        "        inmate_detail['First Name'] = full_name[0]\n",
        "        inmate_detail['Last Name'] = full_name[len(full_name) - 1]\n",
        "\n",
        "        # Middle name, changing it from an array to just a string\n",
        "        full_mid = '' if len(full_name[1:len(full_name) - 1]) == 0 else full_name[1:len(full_name) - 1]\n",
        "        tmp_mid = ''\n",
        "        for i in full_mid:\n",
        "            tmp_mid += i\n",
        "        inmate_detail['Middle Name'] = tmp_mid\n",
        "\n",
        "        inmate_detail['Current Age'] = soup.find('b', text='AGE:').next_sibling.strip()\n",
        "\n",
        "        # Add the ' for the height (page shows 600, making it 6'00)\n",
        "        height = soup.find('b', text='HGT: ').next_sibling.strip()\n",
        "        inmate_detail['Height'] = height[0] + \"' \" + height[1:] + '\"'\n",
        "        inmate_detail['Weight'] = soup.find('b', text='WGT: ').next_sibling.strip() + ' lbs'\n",
        "        inmate_detail['Race'] = soup.find('b', text='RACE: ').next_sibling.strip()\n",
        "        inmate_detail['Sex'] = soup.find('b', text='SEX: ').next_sibling.strip()\n",
        "        inmate_detail['Hair Color'] = soup.find('b', text='HAIR: ').next_sibling.strip()\n",
        "        inmate_detail['Eye Color'] = soup.find('b', text='EYE: ').next_sibling.strip()\n",
        "        inmate_detail['Facial Hair'] = soup.find('b', text='FHAIR: ').next_sibling.strip()\n",
        "        inmate_detail['Complexion'] = soup.find('b', text='COMPL: ').next_sibling.strip()\n",
        "\n",
        "        # A lot of inconsistency with this, so I just took it out\n",
        "        inmate_detail['Case Number'] = soup.find('b', text=' - Case#:').next_sibling.strip()\n",
        "        inmate_detail['Off Date'] = soup.find('b', text='OFF DATE: ').next_sibling.strip()\n",
        "        inmate_detail['Booking Date'] = soup.find('b', text='INTAKE DATE: ').next_sibling.strip() + ' ' + soup.find('b',text='TIME:').next_sibling.strip()\n",
        "        inmate_detail['Booking Number'] = soup.find('b', text='INTAKE #: ').next_sibling.strip()\n",
        "        inmate_detail['Arresting Agency'] = soup.find('b', text='ARRESTING AGENCY: ').next_sibling.strip()\n",
        "\n",
        "        charge_list_tag = soup.find('table', {'width': '100%', 'align': 'center', 'cellspacing': '2'}).findAll('tr')[2:]\n",
        "        inmate_detail['Charges'] = []\n",
        "        # Append each charges into an dict\n",
        "        for charge in charge_list_tag:\n",
        "            bail_info = {}\n",
        "            charge_info = charge.findAll('td')\n",
        "\n",
        "            # Freaking weird symbol that wont go away with decode >:(\n",
        "            bail_info['chargeDescription'] = charge_info[0].contents[0].strip().replace('ã', ' ')\n",
        "            bail_info['chargeCourt'] = charge_info[1].contents[0].strip().replace('ã', ' ')\n",
        "            if len(charge_info[2]) == 0:\n",
        "                bail_info['bondAmount'] = 0\n",
        "            else:\n",
        "                bail_info['bondAmount'] = charge_info[2].contents[0].strip()\n",
        "\n",
        "            inmate_detail['Charges'].append(bail_info)\n",
        "\n",
        "        return inmate_detail\n",
        "\n",
        "    all_info = pd.DataFrame()\n",
        "    detail_urls = get_list_of_inmates_url(all_url)\n",
        "    print(county, \"Number of offenders:\", str(len(detail_urls)))\n",
        "    \n",
        "    cur_num_processed = 0\n",
        "    for inmate_url in detail_urls:\n",
        "        info_arr = get_info(inmate_url)\n",
        "        if info_arr != None:\n",
        "            all_info = all_info.append(info_arr, ignore_index=True)\n",
        " \n",
        "        cur_num_processed += 1\n",
        "        if cur_num_processed % INMATE_PROGRESS_INCREMENT == 0:\n",
        "            print(county, \"Processed\", cur_num_processed)\n",
        "        \n",
        "    filename = get_county_filename(county)\n",
        "    all_info.to_csv(f'{DATA_DIR}{CURRENT_DATE}/{filename}', index=False)"
      ],
      "metadata": {
        "id": "LJ4G7zXWDYS7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Web Scraper\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "etUaTa_B62nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_scraper_for_county(lock: multiprocessing.Lock, county: County, captcha_key: str = None) -> None:\n",
        "    lock.acquire()\n",
        "    try:\n",
        "        print(\"Starting\", county)\n",
        "    finally:\n",
        "        lock.release()\n",
        "\n",
        "    if county in CountyWithCaptcha:\n",
        "        county_with_captcha_scraper(county, captcha_key)\n",
        "    elif county in {CountyWithoutCaptcha.PEARL_RIVER, CountyWithoutCaptcha.MADISON}:\n",
        "        pearlriver_madison(county) \n",
        "    elif county == CountyWithoutCaptcha.JACKSON:\n",
        "        jackson()\n",
        "    elif county == CountyWithoutCaptcha.LEE:\n",
        "        lee()\n",
        "    # elif county == CountyWithoutCaptcha.ADAMS:\n",
        "    #     adams()\n",
        "    # elif county == CountyWithoutCaptcha.CLAY:\n",
        "    #     clay()\n",
        "    # elif county == CountyWithoutCaptcha.JONES:\n",
        "    #     jones()\n",
        "    # elif county == CountyWithoutCaptcha.TUNICA:\n",
        "    #     tunica()\n",
        "    else:\n",
        "        raise Exception(f\"County not found: {county}\")\n",
        "\n",
        "    lock.acquire()\n",
        "    try:\n",
        "        print(\"Exiting\", county, end='\\n\\n')\n",
        "    finally:\n",
        "        lock.release()"
      ],
      "metadata": {
        "id": "vESTDSTqdyBZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a captcha key for the counties that require captchas\n",
        "is_captcha_matched = False\n",
        "while not is_captcha_matched:\n",
        "    captcha_data = requests.get('https://omsweb.public-safety-cloud.com/jtclientweb/captcha/getnewcaptchaclient')\n",
        "    image = captcha_data.json()['captchaImage']\n",
        "\n",
        "    # To be used later for automated captcha solver\n",
        "    # # Save the image to a file\n",
        "    # import base64\n",
        "    # img = image.split('base64,')[-1]\n",
        "    # x = base64.decodebytes(img.encode('ascii'))\n",
        "    # with open('captcha.jpg', 'wb') as f:\n",
        "    #     f.write(x)\n",
        "\n",
        "    html = f'<img src=\"{image}\"/>'\n",
        "    IPython.display.display(IPython.display.HTML(html))\n",
        "    user_code = input()\n",
        "\n",
        "    captcha_response_data = requests.post(\n",
        "        'https://omsweb.public-safety-cloud.com/jtclientweb/Captcha/validatecaptcha',\n",
        "        json={'userCode': user_code, 'captchaKey': captcha_data.json()['captchaKey']}\n",
        "    )\n",
        "    is_captcha_matched = captcha_response_data.json()['captchaMatched']\n",
        "    captcha_key = captcha_response_data.json()['captchaKey']\n",
        "\n",
        "    if not is_captcha_matched:\n",
        "        print(\"Incorrect captcha\")\n",
        "\n",
        "jobs = []\n",
        "lock = multiprocessing.Lock()\n",
        "for county in CountyWithCaptcha:\n",
        "    process = multiprocessing.Process(\n",
        "        target=run_scraper_for_county,\n",
        "        args=(lock, county, captcha_key)\n",
        "    )\n",
        "    jobs.append(process)\n",
        "\n",
        "for county in CountyWithoutCaptcha:\n",
        "    process = multiprocessing.Process(\n",
        "        target=run_scraper_for_county,\n",
        "        args=(lock, county,)\n",
        "    )\n",
        "    jobs.append(process)\n",
        "\n",
        "for j in jobs:\n",
        "    j.start()\n",
        "\n",
        "# Wait for all processes to complete\n",
        "for j in jobs:\n",
        "    j.join()\n",
        "\n",
        "print(\"Finished scraping\")"
      ],
      "metadata": {
        "id": "kgGiJkcO652e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6d3f6cf-3fea-4948-866f-9280d353c15b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAOiSURBVGhD7ZhNSxwxGIB76qnFXupv6KH/QHDxUvDQPyBIcde7UBb04MGhBwsi9aMn17WIsiCK62V1DwtevAr9R28nM0nmTfJuJrMzs5ulOTwISUYwj+9X3vx9fYaAPwQhnhGEeEYQ4hlBiGcEIZ4RhHhGEOIZQYhnBCGeEYQgtt7+IdenSRCCYEJmLSUI4QgZQchM6MLuwldY/ZAhZOx2xJk+nG4cwMj4luA8gh8f11UWIxhSZ3P4z4RoIhrphWeREUtomPv070oZtggBSNDFuXo+Dycho7Pf0Ph2oNLuuf33+EJnJ7voGBEJQkYqJF0b7bXchKCLP4oelD1SlAN2IaMerCcCutATa3ddJAatT4Evn5sJ1J6VwQFsosjY3OvLPV2G4GYjT8gD3K7wSyeEvERbXMgW3A7wd3asQnr74uJj9vkfISWlbN+Z3+Vy1YF3Sz8V3i91oEud5QgZOtRZFS0NLezADd+jokPCJBYQokdCLUJwNKyfPfH1PmwjIdm6G902IQAJal6p5ykKidFS1epGV+6NlZEwvqg3Pn1Pfsq0JFg5hpfkDJIl19yYoKiXEIIufvnwUdkjRTmQJydJPUiISFd2GXaYkETK4BiOFpEQBouIiNeWCTqtAkJUEcWFPEK0ll46JWR4eMKFnEA0xN+5QYlhKOmqAiFChogSsuVltC6Nb11wEiJqSVovJo0QVYgeCWWFYCgxQkg2Z0yGIkNyCRd6pDAKpitGjhB0+aKol0hZIi1JKWvXPKQzWdlaeSgxDOqsC0Z0METaitPThV5TGAUjxSoEd1nZxZeoIcNrWEZCEgEsIg7T2lK0fuSTdVhViDFkUAMgkcKKDIcWIVrN4IOgPiQWEsIgWt5ERvuePl8SOeQJ4i5rEjmGDNxJaa2tbHk5+oxiwyLkCX61kRApoKeKYsh05so9NCkpFaYriTYU4jmEEsMwfkeMIQR3WEY3pdaUioTEKEPgmGm9qAyetlh6amo1JZFSQ6ToUYIndQEl5pSfM6ODgS49R0hFKasGqAGQSGEuw2FRlHlkoQWnY6ZnSgzDFDI+Zdll2ZmiENRJaa2taHkF+oxSFS6RkojjwigxynmctlCLmz0sFns2YUxPCOqwWLpSuym1ptQlRKI/pyD0OUWkK5sc4wmFUedgWA3ZpecJqSNlVQUlhkGdnQQvUpZdlr/MuZAYnLZQi5s9LJZ/Npl3piuEoz+hJDJqGgznjZkICYwnCPGKZ/gHVThravxBX+wAAAAASUVORK5CYII=\"/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a8C8\n",
            "Starting DESOTO\n",
            "Starting FORREST\n",
            "Starting HANCOCK\n",
            "Starting HARRISON\n",
            "Starting LAMAR\n",
            "Starting MARION\n",
            "Starting PERRY\n",
            "Starting YAZOO\n",
            "Starting PEARL_RIVER\n",
            "Starting JACKSON\n",
            "Starting MADISON\n",
            "Starting LEE\n",
            "JACKSON Number of offenders: 421\n",
            "PERRY Number of offenders: 38\n",
            "LAMAR Number of offenders: 91\n",
            "MADISON Number of offenders: 359\n",
            "HANCOCK Number of offenders: 261\n",
            "FORREST Number of offenders: 306\n",
            "PEARL_RIVER Number of offenders: 218\n",
            "HARRISON Number of offenders: 650\n",
            "MARION Number of offenders: 253\n",
            "DESOTO Number of offenders: 407\n",
            "LEE Number of offenders: 307\n",
            "MADISON Processed 50\n",
            "JACKSON Processed 50\n",
            "LEE Processed 50\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0009.HTM <urlopen error timed out>\n",
            "MADISON Processed 100\n",
            "LEE Processed 100\n",
            "JACKSON Processed 100\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0025.HTM <urlopen error timed out>\n",
            "MADISON Processed 150\n",
            "Exiting PERRY\n",
            "\n",
            "LEE Processed 150\n",
            "HANCOCK Processed 50\n",
            "JACKSON Processed 150\n",
            "FORREST Processed 50\n",
            "MARION Processed 50\n",
            "HARRISON Processed 50\n",
            "MADISON Processed 200\n",
            "LAMAR Processed 50\n",
            "DESOTO Processed 50\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0025.HTM <urlopen error timed out>\n",
            "LEE Processed 200\n",
            "JACKSON Processed 200\n",
            "YAZOO Number of offenders: 0\n",
            "Exiting YAZOO\n",
            "\n",
            "MADISON Processed 250\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0029.HTM <urlopen error timed out>\n",
            "LEE Processed 250\n",
            "JACKSON Processed 250\n",
            "MADISON Processed 300\n",
            "HANCOCK Processed 100\n",
            "LEE Processed 300\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0031.HTM <urlopen error timed out>\n",
            "FORREST Processed 100\n",
            "Exiting LEE\n",
            "\n",
            "MARION Processed 100\n",
            "HARRISON Processed 100\n",
            "JACKSON Processed 300\n",
            "MADISON Processed 350\n",
            "DESOTO Processed 100\n",
            "Exiting LAMAR\n",
            "\n",
            "Exiting MADISON\n",
            "\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0041.HTM <urlopen error timed out>\n",
            "JACKSON Processed 350\n",
            "HANCOCK Processed 150\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0045.HTM <urlopen error timed out>\n",
            "JACKSON Processed 400\n",
            "PEARL_RIVER Processed 50\n",
            "FORREST Processed 150\n",
            "Exiting JACKSON\n",
            "\n",
            "MARION Processed 150\n",
            "HARRISON Processed 150\n",
            "DESOTO Processed 150\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0051.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0061.HTM <urlopen error timed out>\n",
            "HANCOCK Processed 200\n",
            "FORREST Processed 200\n",
            "MARION Processed 200\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0075.HTM <urlopen error timed out>\n",
            "HARRISON Processed 200\n",
            "DESOTO Processed 200\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0080.HTM <urlopen error timed out>\n",
            "HANCOCK Processed 250\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0080.HTM <urlopen error timed out>\n",
            "Exiting HANCOCK\n",
            "\n",
            "FORREST Processed 250\n",
            "MARION Processed 250\n",
            "Exiting MARION\n",
            "\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0087.HTM <urlopen error timed out>\n",
            "DESOTO Processed 250\n",
            "HARRISON Processed 250\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0093.HTM <urlopen error timed out>\n",
            "FORREST Processed 300\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0095.HTM <urlopen error timed out>\n",
            "Exiting FORREST\n",
            "\n",
            "DESOTO Processed 300\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0095.HTM <urlopen error timed out>\n",
            "HARRISON Processed 300\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0096.HTM <urlopen error timed out>\n",
            "PEARL_RIVER Processed 100\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0104.HTM <urlopen error timed out>\n",
            "DESOTO Processed 350\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0108.HTM <urlopen error timed out>\n",
            "HARRISON Processed 350\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0109.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0113.HTM <urlopen error timed out>\n",
            "DESOTO Processed 400\n",
            "Exiting DESOTO\n",
            "\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0114.HTM <urlopen error timed out>\n",
            "HARRISON Processed 400\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0114.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0115.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0119.HTM <urlopen error timed out>\n",
            "HARRISON Processed 450\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0127.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0134.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0135.HTM <urlopen error timed out>\n",
            "HARRISON Processed 500\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0144.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0147.HTM <urlopen error timed out>\n",
            "HARRISON Processed 550\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0148.HTM <urlopen error timed out>\n",
            "PEARL_RIVER Processed 150\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0156.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0165.HTM <urlopen error timed out>\n",
            "HARRISON Processed 600\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0169.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0175.HTM <urlopen error timed out>\n",
            "HARRISON Processed 650\n",
            "Exiting HARRISON\n",
            "\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0175.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0182.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0185.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0186.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0187.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0196.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0197.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0199.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0205.HTM <urlopen error timed out>\n",
            "PEARL_RIVER Processed 200\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0206.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0208.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0209.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0210.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0215.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0219.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0220.HTM <urlopen error timed out>\n",
            "WARNING: Failed to connect to: https://www.pearlrivercounty.net/sheriff/files/ICUD0221.HTM <urlopen error timed out>\n",
            "Exiting PEARL_RIVER\n",
            "\n",
            "Finished scraping\n"
          ]
        }
      ]
    }
  ]
}