{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCRxhuoF9rJE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime, date\n",
        "from pytz import timezone\n",
        "import threading\n",
        "import math\n",
        "import re\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import requests\n",
        "import urllib.request\n",
        "from urllib.error import URLError\n",
        "import json\n",
        "from decimal import Decimal\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QwEP_Mksqqxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLLFpjTJmiY9",
        "outputId": "0ee71d9b-afac-4275-deaf-cb0b4e2a2a02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date used for scraping: 02-20-2022\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('/content/drive/MyDrive/'):\n",
        "    raise Exception(\"Error: Mount Google Drive before continuing!\")\n",
        "\n",
        "# Define directories to save data\n",
        "SCRAPE_DIR = '/content/drive/MyDrive/Data Science for Social Good - Spring 2022/scraped_files/'\n",
        "RAW_DIR = SCRAPE_DIR + 'RAW/'\n",
        "CLEAN_DIR = SCRAPE_DIR + 'CLEAN/'\n",
        "DAILY_SUMMARY_DIR = SCRAPE_DIR + 'DAILY_SUMMARY/'\n",
        "\n",
        "today_date = datetime.now(timezone('US/Eastern')).strftime(\"%m-%d-%Y\")\n",
        "\n",
        "# Create all directories on the given paths if needed\n",
        "os.makedirs(RAW_DIR + today_date, exist_ok=True)\n",
        "os.makedirs(CLEAN_DIR + today_date, exist_ok=True)\n",
        "os.makedirs(DAILY_SUMMARY_DIR, exist_ok=True)\n",
        "print(\"Date used for scraping:\", today_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0xjFRLk7M6CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def request_url(request: str, town: str):\n",
        "    NUM_ATTEMPTS = 6\n",
        "\n",
        "    if town in {'adams', 'madison', 'jackson', 'pearlRiver'}:\n",
        "        user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
        "        headers = {'User-Agent':user_agent} \n",
        "        request = urllib.request.Request(request, headers=headers)  # The assembled request\n",
        "\n",
        "    # Try the connection until success or NUM_ATTEMPTS is exceeded\n",
        "    for _ in range(NUM_ATTEMPTS):\n",
        "        try:\n",
        "            response = urllib.request.urlopen(request, timeout=10)\n",
        "            data = response.read() \n",
        "            response.close()\n",
        "            return data\n",
        "        except URLError as str_error:\n",
        "            time.sleep(2)\n",
        "            print(\"Exception:\", town, str_error)\n",
        "\n",
        "    print(\"Request failed for\", town)\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_filename(date: str, town_name: str) -> str:\n",
        "    return CLEAN_DIR + date + '/' + date + '_' + town_name + '.csv'\n",
        "\n",
        "\n",
        "jail_captcha = {\n",
        "    'DeSoto': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/DeSoto_County_Ms/'},\n",
        "    'Forrest': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Forrest_County_MS/'},\n",
        "    'Hancock': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/HANCOCK_COUNTY_MS/'},\n",
        "    'Harrison': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/HARRISON_COUNTY_JAIL_MS/'},\n",
        "    'Lamar': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Lamar_County_MS/'},\n",
        "    'Marion': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Marion_County_MS/'},\n",
        "    'Perry': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Perry_County_MS/'},\n",
        "    'Yazoo': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Yazoo_County_MS/'}\n",
        "}"
      ],
      "metadata": {
        "id": "QSOcMIogM6Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Townscraper Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4xDTR2DF_Q-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def town_scraper(town, validate_r):\n",
        "    print(\"County:\", town)\n",
        "    captchaKey_aftervalidation = validate_r.json()['captchaKey']\n",
        "\n",
        "    # Get offender information\n",
        "    records_r = requests.post(\n",
        "        jail_captcha[town]['URL'],\n",
        "        json={'captchaKey': validate_r.json()['captchaKey']}\n",
        "    )\n",
        "    offenderViewKey = records_r.json()['offenderViewKey']\n",
        "    total = len(records_r.json()['offenders'])\n",
        "    print(town, \"No. of offenders:\", total)\n",
        "\n",
        "    # Loop through information and store in Dataframe\n",
        "    inmates = {}\n",
        "    RAW_inmates = {}\n",
        "\n",
        "    for offender in records_r.json()['offenders']:\n",
        "        inmates[offender['arrestNo']] = {}\n",
        "        inmates[offender['arrestNo']]['Arrest Number'] = offender['arrestNo']\n",
        "        for j in ['firstName', 'lastName', 'agencyName', 'originalBookDateTime']:\n",
        "            inmates[offender['arrestNo']][j] = offender[j]\n",
        "\n",
        "        RAW_inmates[offender['arrestNo']] = {}\n",
        "        RAW_inmates[offender['arrestNo']]['Arrest Number'] = offender['arrestNo']\n",
        "        RAW_inmates[offender['arrestNo']]['offenders'] = offender\n",
        "\n",
        "    df = pd.DataFrame(columns=['Arrest Number', 'firstName', 'lastName', 'agencyName', 'originalBookDateTime',\n",
        "                               'bondAmount', 'bondType', 'chargeDescription', 'chargeStatus',\n",
        "                               'crimeType', 'Bond Total Amount', 'charges', 'cases'])\n",
        "    RAW_df = pd.DataFrame(columns=['Arrest Number', 'charges', 'cases'])\n",
        "\n",
        "    arrestNos = {}\n",
        "    for arrestNo in list(inmates.keys()):\n",
        "        arrestNos[str(arrestNo)] = {}\n",
        "        arrestNos[str(arrestNo)][\"Number\"] = arrestNo\n",
        "        URL = jail_captcha[town]['URL'] + str(arrestNo) + '/offenderbucket/' + str(offenderViewKey)\n",
        "        response = requests.post(URL, json={'captchaImage': image, 'captchaKey': captchaKey_aftervalidation})\n",
        "\n",
        "        # Iterate through charges in response\n",
        "        bond_total = 0\n",
        "        for column in ['charges', 'cases', 'bondType', 'bondAmount', 'chargeDescription', 'chargeStatus', 'crimeType']:\n",
        "            inmates[str(arrestNo)][column] = []\n",
        "\n",
        "        RAW_inmates[str(arrestNo)]['charges'] = []\n",
        "        for charge in response.json()['charges']:\n",
        "            # print(charge)\n",
        "            inmates[str(arrestNo)]['charges'].append(charge)\n",
        "            RAW_inmates[str(arrestNo)]['charges'].append(charge)\n",
        "            bondAmt = charge['bondAmount']\n",
        "            if bondAmt == None:\n",
        "                bondAmt = 0\n",
        "            bond_total = bond_total + float(bondAmt)\n",
        "            for column in ['bondType', 'bondAmount', 'chargeDescription', 'chargeStatus', 'crimeType']:\n",
        "                inmates[str(arrestNo)][column].append(charge[column])\n",
        "\n",
        "        inmates[str(arrestNo)]['Bond Total Amount'] = bond_total\n",
        "\n",
        "        inmates[str(arrestNo)]['Potentially Bondable?'] = ''\n",
        "        arrestNos[str(arrestNo)]['Potentially Bondable?'] = ''\n",
        "        if all((x == 'WRITTEN BOND' or\n",
        "                x == 'SURETY BOND' or\n",
        "                x == 'OWN RECOGNIZANCE BOND' or\n",
        "                x == 'OFF BOND' or\n",
        "                x == 'SURETY BOND WITH CONDITIONS' or\n",
        "                x == 'SURETY'\n",
        "               ) for x in inmates[str(arrestNo)]['bondType']) and len(inmates[str(arrestNo)]['bondType']) > 0:\n",
        "            inmates[str(arrestNo)]['Potentially Bondable?'] = 'Yes'\n",
        "            arrestNos[str(arrestNo)]['Potentially Bondable?'] = 'Yes'\n",
        "\n",
        "        inmates[str(arrestNo)]['Sex/DV charge'] = ''\n",
        "        arrestNos[str(arrestNo)]['Sex/DV charge'] = ''\n",
        "        if any(re.search(\"sex\", str(x), re.IGNORECASE) for x in inmates[str(arrestNo)]['chargeDescription']):\n",
        "            inmates[str(arrestNo)]['Sex/DV charge'] = 'Yes'\n",
        "            arrestNos[str(arrestNo)]['Sex/DV charge'] = 'Yes'\n",
        "\n",
        "        inmates[str(arrestNo)]['Over 5k?'] = ''\n",
        "        arrestNos[str(arrestNo)]['Over 5k?'] = ''\n",
        "        if int(inmates[str(arrestNo)]['Bond Total Amount']) > 5000:\n",
        "            inmates[str(arrestNo)]['Over 5k?'] = 'Yes'\n",
        "            arrestNos[str(arrestNo)]['Over 5k?'] = 'Yes'\n",
        "\n",
        "        # Iterate through cases in response\n",
        "        RAW_inmates[str(arrestNo)]['cases'] = []\n",
        "        for case in response.json()['cases']:\n",
        "            RAW_inmates[str(arrestNo)]['cases'].append(case)\n",
        "\n",
        "        # Iterate through offenderSpecialFields in response\n",
        "        total = []\n",
        "        for item in response.json()['offenderSpecialFields']:\n",
        "            temp = {}\n",
        "            temp[item['labelText']] = item['offenderValue']\n",
        "            total.append(temp)\n",
        "        RAW_inmates[str(arrestNo)]['offenderSpecialFields'] = total\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "        # Get offenderViewKey for next iteration of for loop  \n",
        "        offenderViewKey = response.json()['offenderViewKey']\n",
        "\n",
        "        # Add inmate to dataframe\n",
        "        df = df.append(inmates[str(arrestNo)], ignore_index=True)\n",
        "        RAW_df = RAW_df.append(RAW_inmates[str(arrestNo)], ignore_index=True)\n",
        "\n",
        "    # Save inmate info for town to Google Drive\n",
        "    file_name = str(today_date) + \"_\" + town + '_inmates_' + '.csv'\n",
        "    df.to_csv(CLEAN_DIR + today_date + \"/\" + file_name)\n",
        "    print(town, \"Saved CLEAN\", file_name)\n",
        "\n",
        "    # Save RAW inmate info for town to Google Drive\n",
        "    file_name = str(today_date) + \"_\" + town + '_inmates_' + '.csv'\n",
        "    RAW_df.to_csv(RAW_DIR + today_date + \"/\" + file_name)\n",
        "    print(town, \"Saved RAW\", file_name)\n",
        "\n",
        "    daily_summary = pd.DataFrame(columns=[\"Date\", \"Arrest Numbers\", \"Total arrest Numbers\"])\n",
        "    temp = {}\n",
        "    temp[\"Date\"] = today_date\n",
        "    temp[\"Arrest Numbers\"] = arrestNos\n",
        "    temp[\"Total arrest Numbers\"] = len(arrestNos)\n",
        "    daily_summary = daily_summary.append(temp, ignore_index=True)\n",
        "\n",
        "    daily_summary_filename = DAILY_SUMMARY_DIR + str(town) + '.csv'\n",
        "    try:\n",
        "        daily_summary = pd.read_csv(daily_summary_filename).append(daily_summary)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Daily summary file not found; creating new file:\", daily_summary_filename)\n",
        "\n",
        "    daily_summary = daily_summary[[\"Date\", \"Arrest Numbers\", \"Total arrest Numbers\"]]\n",
        "    daily_summary.to_csv(daily_summary_filename)\n",
        "    print(town, \"saved DAILY SUMMARY for\", town)\n",
        "\n",
        "# Audio notification so you know when to input the captcha code again\n",
        "# output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n"
      ],
      "metadata": {
        "id": "Vix_8hbpsnU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PearlRiver Script - website without bond amounts\n",
        "\n",
        "---\n",
        "\n",
        "Looks like this isn't used?\n",
        "\n"
      ],
      "metadata": {
        "id": "LdOKe3GJ_L-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def townScraperPearl():\n",
        "    town = 'PearlRiver'\n",
        "    jail = 'PEARLRIVERMS'\n",
        "\n",
        "    print(\"County:\", town)\n",
        "\n",
        "    headers = CaseInsensitiveDict()\n",
        "    headers[\"Content-Type\"] = \"application/json\"\n",
        "    headers[\"Content-Length\"] = \"0\"\n",
        "    result = requests.post(\"http://inmates.bluhorse.com/Default.aspx/GetPasskey\", headers=headers).json()['d']\n",
        "    keyAns = result.split(\"|||\");\n",
        "    key = keyAns[0]\n",
        "    ans = keyAns[1]\n",
        "\n",
        "    InmateList = 'http://inmates.bluhorse.com/' + \"InmateService.svc/GetInmates2?Jail=\" + jail + \"&key=\" + key + \"&ans=\" + ans\n",
        "    GetInmateBookNo = requests.get(InmateList).json()['GetInmates2Result']\n",
        "    total = len(GetInmateBookNo)\n",
        "    print(town, \"No. of offenders:\", total)\n",
        "\n",
        "    isLogin = 'false'\n",
        "    rootURL = \"http://inmates.bluhorse.com/InmateService.svc\"\n",
        "\n",
        "    WebFields_url = 'http://inmates.bluhorse.com/' + \"InmateService.svc/GetJailInfo1?Jail=\" + jail + \"&key=\" + key + \"&Answer=\" + ans\n",
        "    WebFields = requests.get(WebFields_url).json()['GetJailInfo1Result']['WebFields']\n",
        "\n",
        "    inmates = {}\n",
        "    df = pd.DataFrame(columns=[\"Arrest Number\", 'firstName', 'lastName', 'agencyName', 'originalBookDateTime',\n",
        "                               'bondAmount', 'bondType', 'chargeDescription', 'chargeStatus',\n",
        "                               'crimeType', 'Bond Total Amount', 'charges', 'cases'])\n",
        "\n",
        "    for offender in GetInmateBookNo:\n",
        "        bookNO = offender['BookNo']\n",
        "\n",
        "        inmates[bookNO] = {}\n",
        "        for column in ['charges', 'cases', 'bondType', 'bondAmount', 'chargeDescription', 'chargeStatus', 'crimeType']:\n",
        "            inmates[bookNO][column] = []\n",
        "\n",
        "        inmates[bookNO][\"Arrest Number\"] = offender['BookNo']\n",
        "        inmates[bookNO]['firstName'] = offender['FName']\n",
        "        inmates[bookNO]['lastName'] = offender['LName']\n",
        "\n",
        "    for i in GetInmateBookNo:\n",
        "        BookNO = i['BookNo']\n",
        "\n",
        "        GetInmateCharges = requests.get(\n",
        "            rootURL + \"/GetInmateCharges?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmate = requests.get(\n",
        "            rootURL + \"/GetInmate?Jail=\" + jail + \"&bookno=\" + BookNO + \"&key=\" + key + \"&answer=\" + ans + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmate_Arrest_Info = requests.get(\n",
        "            rootURL + \"/GetInmate_Arrest_Info?Jail=\" + jail + \"&bookno=\" + BookNO + \"&key=\" + key + \"&answer=\" + ans + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmateHolds = requests.get(\n",
        "            rootURL + \"/GetInmateHolds?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmateCourtHistory = requests.get(\n",
        "            rootURL + \"/GetInmateCourtHistory?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmateBonds = requests.get(\n",
        "            rootURL + \"/GetInmateBonds?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "\n",
        "        GetInmateChargesResult = GetInmateCharges.json()['GetInmateChargesResult']\n",
        "        GetInmateResult = GetInmate.json()['GetInmateResult']\n",
        "        GetInmate_Arrest_InfoResult = GetInmate_Arrest_Info.json()['GetInmate_Arrest_InfoResult']\n",
        "        GetInmateHoldsResult = GetInmateHolds.json()['GetInmateHoldsResult']\n",
        "        GetInmateCourtHistoryResult = GetInmateCourtHistory.json()['GetInmateCourtHistoryResult']\n",
        "        GetInmateBondsResult = GetInmateBonds.json()['GetInmateBondsResult']\n",
        "\n",
        "        inmates[BookNO]['agencyName'] = GetInmateResult['ArrestAgency']\n",
        "        inmates[BookNO]['originalBookDateTime'] = GetInmateResult['ArrestDate']\n",
        "        # inmates[BookNO]['cases'] = \n",
        "\n",
        "        bond_total = 0\n",
        "        for charge in GetInmateChargesResult:\n",
        "            inmates[BookNO]['charges'].append(charge)\n",
        "            bondAmt = charge['Bond']\n",
        "            if bondAmt == None:\n",
        "                bondAmt = 0\n",
        "            bond_total = bond_total + float(bondAmt)\n",
        "\n",
        "            # inmates[BookNO]['bondType'].append(charge['ChargeType'])\n",
        "            inmates[BookNO]['bondAmount'].append(charge['Bond'])\n",
        "            inmates[BookNO]['chargeDescription'].append(charge['Description'])\n",
        "            inmates[BookNO]['chargeStatus'].append(charge['Disposition'].strip('\\n').strip('\\r'))\n",
        "            inmates[BookNO]['crimeType'].append(charge['ChargeType'])\n",
        "\n",
        "            inmates[BookNO]['Bond Total Amount'] = bond_total\n",
        "\n",
        "        inmates[BookNO]['GetInmateChargesResult'] = GetInmateChargesResult\n",
        "        inmates[BookNO]['GetInmateBondsResult'] = GetInmateBondsResult\n",
        "        inmates[BookNO]['GetInmateResult'] = GetInmateResult\n",
        "        inmates[BookNO]['GetInmate_Arrest_InfoResult'] = GetInmate_Arrest_InfoResult\n",
        "        inmates[BookNO]['GetInmateHoldsResult'] = GetInmateHoldsResult\n",
        "        inmates[BookNO]['GetInmateCourtHistoryResult'] = GetInmateCourtHistoryResult\n",
        "\n",
        "        df = df.append(inmates[BookNO], ignore_index=True)\n",
        "\n",
        "    # Save inmate info for town to Google Drive\n",
        "    df.to_csv(get_filename(date=today_date, town_name=(town + '_inmates_')))\n"
      ],
      "metadata": {
        "id": "v6IkY85Atdum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PearlRiver Script - website with bonds\n",
        "\n",
        "---\n",
        "\n",
        "Notes:\n",
        "\n",
        "*   We will often have to retry our GET requests. It may be because the scraper is getting temporarily blocked/throttled. This doesn't mean there is an error, but it does slow down the scraping.\n",
        "*   Some pages we scrape are blank, and we print out a link to that page when it happens."
      ],
      "metadata": {
        "id": "7BNaN1iD_VRb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G0gLY9x7sh0"
      },
      "outputs": [],
      "source": [
        "def pearlRiver():\n",
        "    pearlRiver_html = request_url('https://www.pearlrivercounty.net/sheriff/files/ICURRENT.HTM', \"pearlRiver\")\n",
        "    mad_soup = soup(pearlRiver_html, \"html.parser\")\n",
        "    prefix = 'https://www.pearlrivercounty.net/sheriff/files/ICUD'\n",
        "\n",
        "    # Determine the number of people detained\n",
        "    inmate_num = int(mad_soup.find_all('b')[4].get_text())\n",
        "\n",
        "    # Generate a list of links to intake reports\n",
        "    # Intake reports are in the form https://www.pearlrivercounty.net/sheriff/files/ICUD0001.HTM\n",
        "    links = []\n",
        "    for i in range(inmate_num):\n",
        "        links.append(prefix + str(i + 1).rjust(4, '0') + '.HTM')\n",
        "\n",
        "    # Initialize lists\n",
        "    # Note that the index is offset by one from the list of individuals in the url (starts at 1 rather than zero)\n",
        "    names, ages, birthdays, arrest_dates, heights, weights, race, sex, hair_colors = ([] for _ in range(9))\n",
        "    eye_colors, facial_hair, complexions, off_dates, case_nums, intake_dates, intake_times = ([] for _ in range(7))\n",
        "    intake_nums, arrest_agencies, written_bonds, cash_bonds, bondable, bondable_names, offenses = ([] for _ in range(7))\n",
        "    num_bondable = 0\n",
        "    index = 0\n",
        "\n",
        "    for link in links:\n",
        "        # print(link)\n",
        "        inmate_html = request_url(link, \"pearlRiver\")\n",
        "        inmate_soup = soup(inmate_html, \"html.parser\")\n",
        "        fieldsets = inmate_soup.find_all(\"fieldset\")\n",
        "\n",
        "        # Some pages are empty so we have this here to prevent any errors\n",
        "        if len(fieldsets) == 0:\n",
        "            print(\"Fieldsets empty for page:\", link)\n",
        "            continue\n",
        "\n",
        "        # Find names and split among respective lists according to length\n",
        "        # Note that this produces an array for the first name when there are three names, is this an issue?\n",
        "        name_dob_age = fieldsets[0].find_all(\"tr\")\n",
        "        name = name_dob_age[0].get_text()[9:]\n",
        "        names.append(name)\n",
        "\n",
        "        # Find age and birthday from same fieldset as name\n",
        "        dob_age = name_dob_age[3].find_all(\"td\")\n",
        "        birthdays.append(dob_age[0].get_text()[5:].strip())\n",
        "        ages.append(dob_age[1].get_text()[5:].strip())\n",
        "\n",
        "        # Scrape the \"description\" box\n",
        "        appearance_fields = fieldsets[1].find_all(\"tr\")\n",
        "        heights.append(appearance_fields[0].find_all(\"td\")[0].get_text()[5:].strip())\n",
        "        weights.append(appearance_fields[0].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "        race.append(appearance_fields[1].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "        sex.append(appearance_fields[1].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "        hair_colors.append(appearance_fields[2].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "        eye_colors.append(appearance_fields[2].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "        facial_hair.append(appearance_fields[3].find_all(\"td\")[0].get_text()[7:].strip())\n",
        "        complexions.append(appearance_fields[3].find_all(\"td\")[1].get_text()[7:].strip())\n",
        "\n",
        "        # Scrape the \"intake/booking\" box\n",
        "        intake_fields = fieldsets[2].find_all(\"tr\")\n",
        "        off_date = intake_fields[0].get_text()[11:].split()[0]\n",
        "        if off_date == \"00/00/0000\":\n",
        "            off_date = \"\"\n",
        "        off_dates.append(off_date)\n",
        "        case_num = intake_fields[0].get_text()[11:].split()[3]\n",
        "        if case_num == \"OTHER\":\n",
        "            case_num = \"\"\n",
        "        case_nums.append(case_num)\n",
        "\n",
        "        intake_dates.append(intake_fields[1].get_text().split()[2])\n",
        "        intake_times.append(intake_fields[1].get_text().split()[4])\n",
        "        intake_nums.append(intake_fields[2].get_text()[11:].strip())\n",
        "        arrest_agencies.append(intake_fields[3].get_text()[19:].strip())\n",
        "\n",
        "        # Bond Information box\n",
        "        written_bond = (float(fieldsets[4].find_all(\"tr\")[0].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "        written_bonds.append(written_bond)\n",
        "        bondable_ = bool(written_bond)\n",
        "        bondable.append(bondable_)\n",
        "        if bondable_:\n",
        "            num_bondable += 1\n",
        "            bond_names = [name.strip(), index]\n",
        "            bondable_names.append(bond_names)\n",
        "        cash_bonds.append(float(fieldsets[4].find_all(\"tr\")[1].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "\n",
        "        # Offense text box\n",
        "        offs = fieldsets[5].find_all(\"tr\")[2:]\n",
        "        desc = []\n",
        "        court = []\n",
        "        bond_amt_ = []\n",
        "        for o in offs:\n",
        "            off = o.find_all(\"td\")\n",
        "            desc.append(off[0].get_text())\n",
        "            court.append(off[1].get_text().strip() + \" COURT\")\n",
        "            bond_amt = off[2].get_text().strip().replace(\",\", \"\")\n",
        "            if bond_amt == \"\":\n",
        "                bond_amt = \"0\"\n",
        "            bond_amt_.append(float(bond_amt.strip(\"$s\")))\n",
        "\n",
        "        off_dict = {\"Description\": desc, \"Court\": court, \"Bond amount\": bond_amt_}\n",
        "        offenses.append(off_dict)\n",
        "        index += 1\n",
        "\n",
        "    # Generate a dataframe with dictionary dic, then print dataframe as specified\n",
        "    dic = {\"Name\": names, \"Ages\": ages, \"Birthdays\": birthdays,\n",
        "           \"Height\": heights, \"Weight\": weights, \"Race\": race, \"Sex\": sex, \"Hair color\": hair_colors,\n",
        "           \"Eye color\": eye_colors, \"Facial hair\": facial_hair, \"Complexion\": complexions,\n",
        "           \"Off date\": off_dates, \"Case number\": case_nums, \"Intake date\": intake_dates, \"Intake number\": intake_nums,\n",
        "           \"Arresting agency\": arrest_agencies,\n",
        "           \"Bondable?\": bondable, \"Written bond\": written_bonds, \"Cash bond\": cash_bonds,\n",
        "           \"Offense(s)\": offenses}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='PearlRiver'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clay Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IW6qryIG_YhW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mp4WpQWnMkz"
      },
      "outputs": [],
      "source": [
        "def clay():\n",
        "    # Open the first page of the roster to generate links to inmate pages\n",
        "    roster_prefix = \"http://www.claysheriffms.org/roster.php\"\n",
        "    u_clay = requests.get(roster_prefix)\n",
        "    clay_soup = soup(u_clay.text, \"html.parser\")\n",
        "    u_clay.close()\n",
        "\n",
        "    inmate_prefix = \"http://www.claysheriffms.org/\"\n",
        "    inmate_links, name, bookNo, age, sex, race, arrest_agency, arrest_date, bond, bondable, offenses = ([] for _ in range(11))\n",
        "    num_bondable = 0\n",
        "\n",
        "    num_inmates = int(clay_soup.select(\".ptitles\")[0].get_text().split()[2].replace(\"(\", \"\").replace(\")\", \"\"))\n",
        "    num_pages = math.ceil(num_inmates / 10)\n",
        "\n",
        "    # Step through the pages of the roster and scrape for urls to inmate pages\n",
        "    for page in range(num_pages):\n",
        "        u_clay = requests.get(roster_prefix + \"?grp=\" + str(page * 10))\n",
        "        clay_soup = soup(u_clay.text, \"html.parser\")\n",
        "        u_clay.close()\n",
        "\n",
        "        inmate_table = clay_soup.select(\".inmateTable\")\n",
        "        for i in inmate_table:\n",
        "            inmate_links.append(inmate_prefix + i.select(\".text2\")[-1].get(\"href\"))\n",
        "\n",
        "    # Iterate through list of links to inmate pages and find information\n",
        "    for inmate in inmate_links:\n",
        "        u_inmate = requests.get(inmate)\n",
        "        inmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "        u_inmate.close()\n",
        "\n",
        "        table = inmate_soup.find_all(\"table\")[6].select(\".text2\")\n",
        "        name.append(' '.join(inmate_soup.select('.ptitles')[0].get_text().split()))\n",
        "        bookNo.append(table[0].get_text().strip())\n",
        "        age.append(int(table[1].get_text().strip()))\n",
        "        sex.append(table[2].get_text().strip())\n",
        "        race.append(table[3].get_text().strip())\n",
        "        arrest_agency.append(table[4].get_text().strip())\n",
        "        arrest_date.append(table[5].get_text().split()[0])\n",
        "\n",
        "        bond_ = float(table[8].get_text().strip(\"$\"))\n",
        "        bond.append(bond_)\n",
        "        bondable.append(bool(bond_))\n",
        "        if (bool(bond_)):\n",
        "            num_bondable += 1\n",
        "\n",
        "        offenses.append(str(table[7]).strip(\"<span class=\\\"text2\\\">\").strip(\"</\").split(\"<br/>\"))\n",
        "\n",
        "    # Generate a dictionary, then create a dataframe to print\n",
        "    dic = {\"Name\": name, \"Booking number\": bookNo, \"Arrest agency\": arrest_agency, \"Arrest date\": arrest_date,\n",
        "           \"Bondable?\": bondable, \"Bond\": bond, \"Age\": age, \"Sex\": sex, \"Race\": race, \"Offense(s)\": offenses}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Clay'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adams Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "byk-b-sj_ck7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0fynMX1znuH"
      },
      "outputs": [],
      "source": [
        "def adams():\n",
        "    Adams_html = request_url('http://www.adamscosheriff.org/inmate-roster/', \"adams\")\n",
        "    adams_soup = soup(Adams_html, \"html.parser\")\n",
        "    page_num = adams_soup.select(\".page-numbers\")[-2].get_text()\n",
        "    page_num = adams_soup.select(\".page-numbers\")[-2].get_text()\n",
        "\n",
        "    names, book_Nos, age, gender, race, booking_date, charges, bonds, bondable, bondable_names = ([] for _ in range(10))\n",
        "    num_bondable = 0\n",
        "    num_errs = 0\n",
        "\n",
        "    for page in range(int(page_num)):\n",
        "        page_html = request_url('http://www.adamscosheriff.org/inmate-roster/page/' + str(page + 1) + \"/\", \"adams\")\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        page_profile_links = page_soup.select(\".profile-link\")\n",
        "\n",
        "        for profile in page_profile_links:\n",
        "            try:\n",
        "                inmate_html = request_url(profile.a.get(\"href\"), \"adams\")\n",
        "                inmate_soup = soup(inmate_html, \"html.parser\")\n",
        "\n",
        "                p_vals = inmate_soup.find_all(\"p\")\n",
        "                name = (p_vals[0].get_text().strip(\"Full Name:\").strip())\n",
        "                names.append(name)\n",
        "                book_Nos.append((p_vals[1].get_text().strip(\"Booking Number:\").strip()))\n",
        "                age.append(p_vals[2].get_text().strip(\"Age:\").strip())\n",
        "                gender.append(p_vals[3].get_text().strip(\"Gender:\").strip())\n",
        "                race.append(p_vals[4].get_text().strip(\"Race:\").strip())\n",
        "                booking_date.append(p_vals[7].get_text().strip(\"Booking Date:\").strip())\n",
        "                charges.append(p_vals[8].get_text().strip(\"Charges:\").strip())\n",
        "                b = p_vals[9].get_text().strip(\"Bond:\").replace(\",\", \"\").strip()\n",
        "                b_able = False\n",
        "                if b == \"\":\n",
        "                    bond = 0.0\n",
        "                else:\n",
        "                    bond = float(b)\n",
        "                    b_able = True\n",
        "                    num_bondable += 1\n",
        "                    bondable_names.append(name)\n",
        "                bonds.append(bond)\n",
        "                bondable.append(b_able)\n",
        "\n",
        "            except IndexError:\n",
        "                num_errs += 1\n",
        "                print(\"Clay - IndexError\")\n",
        "\n",
        "    dic = {\"Name\": names, \"Booking Number\": book_Nos, \"Age\": age, \"Gender\": gender, \"Race\": race,\n",
        "           \"Booking Date\": booking_date, \"Charges\": charges, \"Bondable?\": bondable, \"Bond\": bonds}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Adams'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hinds Script"
      ],
      "metadata": {
        "id": "m4UNyZSA_gJ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZ-_t3epzdm0"
      },
      "outputs": [],
      "source": [
        "def hinds():\n",
        "    # print(\"here\")\n",
        "    # Iterate through all pages of the database to find links to the inmate pages\n",
        "    # Is there a way to do this that doesn't open the first page twice? Especially without rewriting a bunch of code outside the loop\n",
        "    url_link = 'http://www.co.hinds.ms.us/pgs/apps/inmate/inmate_list.asp?name_sch=&SS1=1&search_by_city=&search_by=&ScrollAction=Page+1'\n",
        "    Hinds_html = request_url(url_link, \"hinds\")\n",
        "    hinds_soup = soup(Hinds_html, \"html.parser\")\n",
        "\n",
        "    num_pages = int(hinds_soup.h3.get_text().split()[3])\n",
        "    hinds_prefix = \"http://www.co.hinds.ms.us/pgs/apps/inmate/inmate_list.asp?name_sch=&SS1=1&search_by_city=&search_by=&ScrollAction=Page+\"\n",
        "    inmate_prefix = \"http://www.co.hinds.ms.us/pgs/apps/inmate/\"\n",
        "    inmate_links = []\n",
        "\n",
        "    name, address, dob, sex, race, height, weight, eye_col, hair_col, arrest_agency = ([] for _ in range(10))\n",
        "    arrest_date, offense1, offense2, offense3, offense4, pin, location = ([] for _ in range(7))\n",
        "\n",
        "    # Find links to all inmate pages\n",
        "    for i in range(num_pages):\n",
        "        Hinds_html = request_url(hinds_prefix + str(i + 1), \"hinds\")\n",
        "        hinds_soup = soup(Hinds_html, \"html.parser\")\n",
        "        lin = hinds_soup.find_all(\"a\")\n",
        "\n",
        "        last_link = len(lin) - 6\n",
        "        page_links = lin[6:last_link]\n",
        "\n",
        "        for link in page_links:\n",
        "            inmate_links.append(inmate_prefix + link.get('href'))\n",
        "\n",
        "    increment = 0\n",
        "    for inmate_link in inmate_links:\n",
        "        if increment % 50 == 0: print(\"Hinds\", increment, len(inmate_links))\n",
        "        try:\n",
        "            Hinds_html = request_url(inmate_link, \"hinds\")\n",
        "            inmate_soup = soup(Hinds_html, \"html.parser\")\n",
        "\n",
        "            # Generate a list of all the classes which contain relevant information\n",
        "            left_txt = inmate_soup.select(\".normaltxtleft\")\n",
        "\n",
        "            # Generate information available, exclude those which do not seem to have anything listed\n",
        "            name.append(left_txt[0].get_text().strip())\n",
        "            address.append(\" \".join(left_txt[1].get_text().split()))\n",
        "            dob.append(left_txt[3].get_text().strip())\n",
        "            sex.append(left_txt[5].get_text().strip())\n",
        "            race.append(left_txt[7].get_text().strip())\n",
        "            height.append(left_txt[9].get_text().strip())\n",
        "            weight.append(left_txt[11].get_text().strip())\n",
        "            eye_col.append(left_txt[13].get_text().strip())\n",
        "            hair_col.append(left_txt[15].get_text().strip())\n",
        "            arrest_agency.append(left_txt[17].get_text().strip())\n",
        "            arrest_date.append(left_txt[19].get_text().strip())\n",
        "            offense1.append(left_txt[20].get_text().strip())\n",
        "            offense2.append(left_txt[27].get_text().strip())\n",
        "            offense3.append(left_txt[34].get_text().strip())\n",
        "            offense4.append(left_txt[41].get_text().strip())\n",
        "            pin.append(left_txt[49].get_text().strip())\n",
        "            location.append(left_txt[51].get_text().strip())\n",
        "            increment += 1\n",
        "\n",
        "        except IndexError:\n",
        "            print(\"Hinds - Index error at \", str(increment + 1), len(inmate_link))\n",
        "            increment += 1\n",
        "\n",
        "    dic = {\"Name\": name, \"Address\": address, \"DoB\": dob, \"Sex\": sex, \"Race\": race, \"Height\": height, \"Weight\": weight,\n",
        "           \"Eye color\": eye_col,\n",
        "           \"Hair color\": hair_col, \"Arresting agency\": arrest_agency, \"Arrest date\": arrest_date, \"Pin\": pin,\n",
        "           \"Location\": location,\n",
        "           \"First offense\": offense1, \"Second offense\": offense2, \"Third offense\": offense3, \"Fourth offense\": offense4}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Hinds'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jackson Script\n",
        "\n",
        "---\n",
        "\n",
        "Currently has an SSL error when attempting to connect to the website"
      ],
      "metadata": {
        "id": "z1PTD-q9_ig-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrCjhOnazV0v"
      },
      "outputs": [],
      "source": [
        "def jackson():\n",
        "    # Regex formatting to calculate total bond\n",
        "    regex = re.compile('Bond:\\ \\$[0-9]*.[0-9]*')\n",
        "    regex_money = re.compile('\\$[0-9]*.[0-9]*')\n",
        "    regex_num = re.compile('[0-9]*.[0-9]*')\n",
        "\n",
        "    # Obtain the total count of inmates\n",
        "    count_url = \"https://services.co.jackson.ms.us/jaildocket/_inmateList.php?Function=count\"\n",
        "    count_html = request_url(count_url, 'jackson')\n",
        "    total_count = soup(count_html, \"html.parser\")\n",
        "    print(\"Jackson - Total Inmate Count:\", total_count)\n",
        "\n",
        "    # To state details of inmates\n",
        "    inmates = {}\n",
        "    inmate_ID_list = []\n",
        "    page = 0\n",
        "    y = []\n",
        "    # Iterate through the pages of inmates and get all the inmate IDs\n",
        "    while len(y) > 0 or page == 0:  # Increase the page count\n",
        "        page = page + 1\n",
        "        inmate_ID = \"https://services.co.jackson.ms.us/jaildocket/_inmateList.php?Function=list&Page=\" + str(page)\n",
        "        uClient = urllib.request.urlopen(inmate_ID)\n",
        "        inmate_ID = uClient.read()\n",
        "        uClient.close()\n",
        "        y = json.loads(soup(inmate_ID, \"html.parser\").prettify())\n",
        "        for i in y:\n",
        "            inmate_ID_list.append(i['ID_Number'].strip())\n",
        "            for k in range(10):\n",
        "                del i[str(k)]\n",
        "            del i['RowNum']\n",
        "            del i['Name_Suffix']\n",
        "            inmates[i['ID_Number'].strip()] = i\n",
        "    print(\"Jackson - Total Count of ID Numbers Obtained:\", len(inmate_ID_list))\n",
        "    print(\"Jackson - # of Pages of inmates on website:\", page)\n",
        "\n",
        "    bond_count = 0\n",
        "    bondable_count = 0\n",
        "    # Iterate through inmate cards with the inmate IDs and store in inmates dict\n",
        "    for inmate_ID in inmate_ID_list:\n",
        "        try:\n",
        "            my_url = 'https://services.co.jackson.ms.us/jaildocket/inmate/_inmatedetails.php?id=' + inmate_ID\n",
        "\n",
        "            # Open connection, grepping the page\n",
        "            uClient = urllib.request.urlopen(my_url)\n",
        "            page_html = uClient.read()\n",
        "            uClient.close()\n",
        "            page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "            # Obtain inmate details (race, height, ... whether they are bondable)\n",
        "            container = page_soup.select(\"[class~=iltext] p\")\n",
        "            name = []\n",
        "            bondable = \"No\"\n",
        "            for i in container:\n",
        "                item = ' '.join(i.string.split())\n",
        "                if item == 'Bondable':\n",
        "                    bondable_count = bondable_count + 1\n",
        "                    bondable = \"Yes\"\n",
        "                name.append(item)\n",
        "\n",
        "            # Obtain their offense charge and bond amount\n",
        "            container = page_soup.select(\"[class~=offenseItem] p\")\n",
        "            offense = []\n",
        "            for i in container:\n",
        "                item = ' '.join(i.string.split())\n",
        "                offense.append(item)\n",
        "\n",
        "            # Calculate the total bond amount for the inmate\n",
        "            total = 0\n",
        "            bonds = regex_money.findall(str(regex.findall(str(offense))))\n",
        "            for b in bonds:\n",
        "                total = total + Decimal(re.sub(r'[^\\d.]', '', b))\n",
        "\n",
        "            # Store all values in dictionary for the inmate\n",
        "            inmates[inmate_ID][\"Total Bond($)\"] = total\n",
        "            inmates[inmate_ID][\"Bondable?\"] = bondable\n",
        "            inmates[inmate_ID][\"inmate_info\"] = name\n",
        "            inmates[inmate_ID][\"inmate_offense\"] = offense\n",
        "\n",
        "            # Calculate the amount of inmates that are Bondable\n",
        "            if inmates[inmate_ID][\"Total Bond($)\"] > 0:  bond_count = bond_count + 1\n",
        "\n",
        "        except:\n",
        "            print(\"Jackson ERROR\", inmate_ID)\n",
        "\n",
        "    print(\"Jackson - # of inmates with bond:\", bond_count)\n",
        "    print(\"Jackson - # of inmates that are bondable:\", bondable_count)\n",
        "\n",
        "    # Store Values in CSV\n",
        "    csv_columns = list(list(inmates.values())[0].keys())\n",
        "    dict_data = list(inmates.values())\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dict_data)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Jackson'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jones Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ri4DV3W2_lyo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIB-JkvrzOrR"
      },
      "outputs": [],
      "source": [
        "def jones():\n",
        "    # Initialize fields\n",
        "    Fname, Lname, bookNum, ages, gender, race, addresses, agency, book_date, charges, bonds, bondable = ([] for _ in range(12))\n",
        "    num_bondable = 0\n",
        "\n",
        "    u_jones = requests.get(\"https://www.jonesso.com/roster.php\")\n",
        "    jones_soup = soup(u_jones.text, \"html.parser\")\n",
        "    u_jones.close()\n",
        "\n",
        "    num_inmates = int(jones_soup.find_all(\"h2\")[0].get_text().replace(\"Inmate Roster (\", \"\").replace(\")\", \"\").strip())\n",
        "    num_pages = math.ceil(num_inmates / 20)\n",
        "    page_range = range(num_pages + 1)[1:]\n",
        "    inmate_links = []\n",
        "    page_prefix = \"https://www.jonesso.com/roster.php?&grp=\"\n",
        "    inmate_prefix = \"https://www.jonesso.com/\"\n",
        "\n",
        "    for page in page_range:\n",
        "        # print(page, page_range)\n",
        "        u_page = requests.get(page_prefix + str(page * 20))\n",
        "        page_soup = soup(u_page.text, \"html.parser\")\n",
        "        u_page.close()\n",
        "\n",
        "        a_tags = (page_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].find_all(\"a\"))\n",
        "        for a in a_tags:\n",
        "            inmate_links.append(a.get(\"href\"))\n",
        "\n",
        "    for inmate in inmate_links:\n",
        "        try:\n",
        "            cgs = []\n",
        "            u_inmate = requests.get(inmate_prefix + inmate)\n",
        "            inmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "            u_inmate.close()\n",
        "\n",
        "            name = inmate_soup.select(\".ptitles\")[0].get_text().split()\n",
        "            fname_var = name[0]\n",
        "            lname_var = name[1]\n",
        "            rows = inmate_soup.find(attrs={\"id\": \"cms-body-content\"}).select(\".row\")[0].select(\".row\")\n",
        "            booknum_var = rows[0].find_all(\"div\")[1].get_text()\n",
        "            age_var = rows[1].get_text().strip().strip(\"Age:\").strip()\n",
        "            gender_var = rows[2].get_text().strip().strip(\"Gender:\").strip()\n",
        "            race_var = rows[3].get_text().strip().strip(\"Race:\").strip()\n",
        "            addresses_var = rows[4].get_text().strip().strip(\"Address:\").strip()\n",
        "            agency_var = rows[5].get_text().strip(\"Arresting Agency:\").strip()\n",
        "            book_date_var = rows[6].get_text().replace(\"Booking Date:\", \"\").strip().split()[0]\n",
        "            off_str = str(rows[8]).strip(\"<div class=\\\"row\\\">\").strip().strip(\n",
        "                \"<div class=\\\"cell inmate_profile_data_content\\\"><span class=\\\"text2\\\">\")\n",
        "            off_str = off_str.replace(\"<br/></span></div>\\n</\", \"\").strip().split(\"<br/>\")\n",
        "            bond_var = rows[9].get_text().strip().strip(\"Bond:\").replace(\"$\", \"\").strip()\n",
        "            for off in off_str:\n",
        "                cgs.append(off.strip())\n",
        "\n",
        "            Fname.append(fname_var)\n",
        "            Lname.append(lname_var)\n",
        "            bookNum.append(booknum_var)\n",
        "            ages.append(age_var)\n",
        "            gender.append(gender_var)\n",
        "            race.append(race_var)\n",
        "            addresses.append(addresses_var)\n",
        "            agency.append(agency_var)\n",
        "            book_date.append(book_date_var)\n",
        "            charges.append(cgs)\n",
        "            bond = int(bond_var)\n",
        "            bonds.append(bond)\n",
        "            if bond != 0:\n",
        "                bondable.append(True)\n",
        "                num_bondable += 1\n",
        "            else:\n",
        "                bondable.append(False)\n",
        "        except:\n",
        "            print(\"Jones Error\", inmate)\n",
        "\n",
        "    # print(\"Jones - There are currently\", num_bondable, \"bondable detainees.\")\n",
        "\n",
        "    dic = {\"First name\": Fname, \"Last name\": Lname, \"Booking Number\": bookNum, \"Age\": ages, \"Gender\": gender,\n",
        "           \"Race\": race, \"Address\": addresses, \"Arrest agency\": agency, \"Book date\": book_date, \"Charges\": charges,\n",
        "           \"Bond\": bonds, \"Bondable?\": bondable}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Jones'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kemper Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "npyUSYg2_o-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afzz9Al6y_jE"
      },
      "outputs": [],
      "source": [
        "def kemper():\n",
        "    first_names, middle_names, last_names, booking_nums, ages, gender, race, addresses, book_dates, charges = ([] for _ in range(10))\n",
        "\n",
        "    u_Kemper = requests.get(\"https://www.kempercountysheriff.com/roster.php?&grp=10\")\n",
        "    kemper_soup = soup(u_Kemper.text, \"html.parser\")\n",
        "    u_Kemper.close()\n",
        "\n",
        "    # Scrape the first page to get the number of inmates, compute the number of pages, then generate links\n",
        "    page_links = []\n",
        "    inmate_links = []\n",
        "    num_inmates = int(kemper_soup.h2.get_text().strip(\"Inmate Roster (\").strip(\")\"))\n",
        "    num_pages = math.ceil(num_inmates / 10)\n",
        "\n",
        "    for n in range(num_pages):\n",
        "        page_links.append(\"https://www.kempercountysheriff.com/roster.php?&grp=\" + str((n + 1) * 10))\n",
        "\n",
        "    # Scrape each page for inmate links\n",
        "    for page in page_links:\n",
        "        u_page = requests.get(page)\n",
        "        page_soup = soup(u_page.text, \"html.parser\")\n",
        "        u_page.close()\n",
        "\n",
        "        inmate_table = page_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].find_all(\"a\")\n",
        "        for inmate in inmate_table:\n",
        "            inmate_links.append(\"https://www.kempercountysheriff.com/\" + inmate.get(\"href\"))\n",
        "\n",
        "    for inmate in inmate_links:\n",
        "        try:\n",
        "            u_inmate = requests.get(inmate)\n",
        "            inmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "            u_inmate.close()\n",
        "\n",
        "            name = inmate_soup.select(\".ptitles\")[0].get_text().split()\n",
        "            fname_var = name[0]\n",
        "            lname_var = name[-1]\n",
        "            mname_var = name[1:-1]\n",
        "            rows = inmate_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].select(\".row\")\n",
        "            bookNum_var = rows[1].find_all(\"div\")[1].get_text()\n",
        "            age_var = rows[2].get_text().replace(\"Age:\", \"\").strip()\n",
        "            gen_var = rows[3].get_text().replace(\"Gender:\", \"\").strip()\n",
        "            race_var = rows[4].get_text().replace(\"Race:\", \"\").strip()\n",
        "\n",
        "            if len(rows) == 9:\n",
        "                book_var = rows[5].get_text().split()[2]\n",
        "                chargeOne_var = rows[7].get_text().strip()\n",
        "            else:\n",
        "                add_var = rows[5].get_text().replace(\"Address:\", \"\").strip()\n",
        "                bookdate_var = rows[6].get_text().split()[2]\n",
        "                charge_var = rows[8].get_text().strip()\n",
        "\n",
        "            first_names.append(fname_var)\n",
        "            last_names.append(lname_var)\n",
        "            middle_names.append(mname_var)\n",
        "\n",
        "            # Note that not all pages have the address listed, this messes up indexing\n",
        "            booking_nums.append(bookNum_var)\n",
        "            ages.append(age_var)\n",
        "            gender.append(gen_var)\n",
        "            race.append(race_var)\n",
        "            if len(rows) == 9:\n",
        "                addresses.append(\"\")\n",
        "                book_dates.append(book_var)\n",
        "                charges.append(chargeOne_var)\n",
        "            else:\n",
        "                addresses.append(add_var)\n",
        "                book_dates.append(bookdate_var)\n",
        "                charges.append(charge_var)\n",
        "\n",
        "        except:\n",
        "            print(\"Kemper Error\", inmate)\n",
        "\n",
        "    dic = {\"First name\": first_names, \"Middle name\": middle_names, \"Last name\": last_names,\n",
        "           \"Booking number\": booking_nums, \"Age\": ages, \"Gender\": gender, \"Race\": race,\n",
        "           \"Address\": addresses, \"Booking date\": book_dates, \"Charges\": charges}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Kemper'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Madison Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KAhtgAww_sM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vefx1gAay3OB"
      },
      "outputs": [],
      "source": [
        "def madison():\n",
        "    # Open connection and read page\n",
        "    Madison_html = request_url('http://mydcstraining.com/agencyinfo/MS/4360/inmate/ICURRENT.HTM', \"madison\")\n",
        "    mad_soup = soup(Madison_html, \"html.parser\")\n",
        "    prefix = 'http://mydcstraining.com/agencyinfo/MS/4360/inmate/ICUD'\n",
        "\n",
        "    # Determine the number of people detained\n",
        "    inmate_num = int(mad_soup.find_all('b')[4].get_text())\n",
        "\n",
        "    # Generate a list of links to intake reports\n",
        "    links = []\n",
        "    for i in range(inmate_num):\n",
        "        links.append(prefix + str(i + 1).rjust(4, '0') + '.HTM')\n",
        "\n",
        "    # Initialize lists\n",
        "    # Note that the index is offset by one from the list of inmates in the url (starts at 1 rather than zero)\n",
        "    names, ages, birthdays, arrest_dates, heights, weights, race, sex, hair_colors = ([] for _ in range(9))\n",
        "    eye_colors, facial_hair, complexions, off_dates, case_nums, intake_dates, intake_times = ([] for _ in range(7))\n",
        "    intake_nums, arrest_agencies, written_bonds, cash_bonds, bondable, bondable_names, offenses = ([] for _ in range(7))\n",
        "    num_bondable = 0\n",
        "    index = 0\n",
        "\n",
        "    for link in links:\n",
        "        try:\n",
        "            inmate_html = request_url(link, \"madison\")\n",
        "            inmate_soup = soup(inmate_html, \"html.parser\")\n",
        "            fieldsets = inmate_soup.find_all(\"fieldset\")\n",
        "\n",
        "            # Find names and split among respective lists according to length\n",
        "            # Note that this produces an array for the first name when there are three names, is this an issue?\n",
        "            name_dob_age = fieldsets[0].find_all(\"tr\")\n",
        "            name = name_dob_age[0].get_text()[9:]\n",
        "            names.append(name)\n",
        "\n",
        "            # Find age and birthday from same fieldset as name\n",
        "            dob_age = name_dob_age[3].find_all(\"td\")\n",
        "            birthdays.append(dob_age[0].get_text()[5:].strip())\n",
        "            ages.append(dob_age[1].get_text()[5:].strip())\n",
        "\n",
        "            # Scrape the \"description\" box\n",
        "            appearance_fields = fieldsets[1].find_all(\"tr\")\n",
        "            heights.append(appearance_fields[0].find_all(\"td\")[0].get_text()[5:].strip())\n",
        "            weights.append(appearance_fields[0].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "            race.append(appearance_fields[1].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "            sex.append(appearance_fields[1].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "            hair_colors.append(appearance_fields[2].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "            eye_colors.append(appearance_fields[2].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "            facial_hair.append(appearance_fields[3].find_all(\"td\")[0].get_text()[7:].strip())\n",
        "            complexions.append(appearance_fields[3].find_all(\"td\")[1].get_text()[7:].strip())\n",
        "\n",
        "            # Scrapes the \"intake/booking\" box\n",
        "            intake_fields = fieldsets[2].find_all(\"tr\")\n",
        "            off_date = intake_fields[0].get_text()[11:].split()[0]\n",
        "            if off_date == \"00/00/0000\":\n",
        "                off_date = \"\"\n",
        "            off_dates.append(off_date)\n",
        "            case_num = intake_fields[0].get_text()[11:].split()[3]\n",
        "            if case_num == \"OTHER\":\n",
        "                case_num = \"\"\n",
        "            case_nums.append(case_num)\n",
        "\n",
        "            intake_dates.append(intake_fields[1].get_text().split()[2])\n",
        "            intake_times.append(intake_fields[1].get_text().split()[4])\n",
        "            intake_nums.append(intake_fields[2].get_text()[11:].strip())\n",
        "            arrest_agencies.append(intake_fields[3].get_text()[19:].strip())\n",
        "\n",
        "            # Bond Information box\n",
        "            written_bond = (float(fieldsets[4].find_all(\"tr\")[0].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "            written_bonds.append(written_bond)\n",
        "            bondable_ = bool(written_bond)\n",
        "            bondable.append(bondable_)\n",
        "            if bondable_:\n",
        "                num_bondable += 1\n",
        "                bond_names = [name.strip(), index]\n",
        "                bondable_names.append(bond_names)\n",
        "            cash_bonds.append(\n",
        "                float(fieldsets[4].find_all(\"tr\")[1].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "\n",
        "            # Offense text box\n",
        "            offs = fieldsets[5].find_all(\"tr\")[2:]\n",
        "            desc = []\n",
        "            court = []\n",
        "            bond_amt_ = []\n",
        "            for o in offs:\n",
        "                off = o.find_all(\"td\")\n",
        "                desc.append(off[0].get_text())\n",
        "                court.append(off[1].get_text().strip() + \" COURT\")\n",
        "                bond_amt = off[2].get_text().strip().replace(\",\", \"\")\n",
        "                if bond_amt == \"\":\n",
        "                    bond_amt = \"0\"\n",
        "                bond_amt_.append(float(bond_amt.strip(\"$s\")))\n",
        "\n",
        "            off_dict = {\"Description\": desc, \"Court\": court, \"Bond amount\": bond_amt_}\n",
        "            offenses.append(off_dict)\n",
        "            index += 1\n",
        "        except:\n",
        "            print(\"Madison - Error:\", link)\n",
        "\n",
        "    # Generate a dataframe with dictionary dic, then prints dataframe as specified\n",
        "    dic = {\"Name\": names, \"Ages\": ages, \"Birthdays\": birthdays,\n",
        "           \"Height\": heights, \"Weight\": weights, \"Race\": race, \"Sex\": sex, \"Hair color\": hair_colors,\n",
        "           \"Eye color\": eye_colors, \"Facial hair\": facial_hair,\n",
        "           \"Complexion\": complexions,\n",
        "           \"Off date\": off_dates, \"Case number\": case_nums, \"Intake date\": intake_dates, \"Intake number\": intake_nums,\n",
        "           \"Arresting agency\": arrest_agencies,\n",
        "           \"Bondable?\": bondable, \"Written bond\": written_bonds, \"Cash bond\": cash_bonds,\n",
        "           \"Offense(s)\": offenses}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Madison'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tunica Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b18R_Dvf_uvm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myLQsez6ytvd"
      },
      "outputs": [],
      "source": [
        "def tunica():\n",
        "    first_names, middle_names, last_names, booking_nums, ages, gender, race = ([] for _ in range(7))\n",
        "    arrest_agency, book_date, charges, bonds, bondable, page_links, inmate_links = ([] for _ in range(7))\n",
        "    num_bondable = 0\n",
        "\n",
        "    u_Tunica = requests.get(\"https://www.tunicamssheriff.com/roster.php\")\n",
        "    tunica_soup = soup(u_Tunica.text, \"html.parser\")\n",
        "    u_Tunica.close()\n",
        "\n",
        "    page_prefix = \"https://www.tunicamssheriff.com/roster.php?&grp=\"\n",
        "    inmate_prefix = \"https://www.tunicamssheriff.com/\"\n",
        "    num_inmates = int(tunica_soup.select(\".ptitles\")[0].get_text()[15:-1])\n",
        "    num_pages = math.ceil(num_inmates / 10)\n",
        "    for p in range(num_pages):\n",
        "        page_links.append(page_prefix + str((p + 1) * 10))\n",
        "\n",
        "    for page in page_links:\n",
        "        u_page = requests.get(page)\n",
        "        page_soup = soup(u_page.text, \"html.parser\")\n",
        "        u_page.close()\n",
        "\n",
        "        links = page_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].select(\"a\")\n",
        "        for link in links:\n",
        "            inmate_links.append(inmate_prefix + link.get(\"href\"))\n",
        "\n",
        "    for inmate in inmate_links:\n",
        "        try:\n",
        "            u_inmate = requests.get(inmate)\n",
        "            inmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "            u_inmate.close()\n",
        "\n",
        "            name = (inmate_soup.select(\".ptitles\")[0].get_text().split())\n",
        "            rows = inmate_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].select(\".row\")[0].select(\".row\")\n",
        "\n",
        "            booking_num = int(rows[0].find_all(\"div\")[1].get_text())\n",
        "            age = int(rows[1].find_all(\"div\")[1].get_text())\n",
        "            gender_person = rows[2].find_all(\"div\")[1].get_text()\n",
        "            race_person = rows[3].find_all(\"div\")[1].get_text()\n",
        "            arrestAgency = rows[4].find_all(\"div\")[1].get_text()\n",
        "            bookDate = rows[5].find_all(\"div\")[1].get_text().split()[0]\n",
        "            offs_str = (\n",
        "                str(rows[7].select(\".text2\")[0]).replace(\"<span class=\\\"text2\\\">\", \"\").replace(\"</span>\", \"\").split(\n",
        "                    \"<br/>\"))\n",
        "\n",
        "            firstname = name[0]\n",
        "            lastname = name[-1]\n",
        "            middlename = name[1:-1]\n",
        "\n",
        "            first_names.append(firstname)\n",
        "            last_names.append(lastname)\n",
        "            middle_names.append(middlename)\n",
        "            booking_nums.append(booking_num)\n",
        "            ages.append(age)\n",
        "            gender.append(gender_person)\n",
        "            race.append(race_person)\n",
        "            arrest_agency.append(arrestAgency)\n",
        "            book_date.append(bookDate)\n",
        "\n",
        "            offs = []\n",
        "            for off in offs_str:\n",
        "                offs.append(off.strip())\n",
        "            charges.append(offs)\n",
        "            try:\n",
        "                bd = rows[8].get_text().replace(\"Bond:\", \"\").strip()\n",
        "                if \"$\" in bd:\n",
        "                    bonds.append(float(bd.strip(\"$\")))\n",
        "                    bondable.append(True)\n",
        "                    num_bondable += 1\n",
        "                else:\n",
        "                    bonds.append(0.0)\n",
        "                    bondable.append(False)\n",
        "            except IndexError:\n",
        "                bonds.append(0.0)\n",
        "                bondable.append(False)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    dic = {\"First name\": first_names, \"Middle name\": middle_names, \"Last name\": last_names,\n",
        "           \"Booking number\": booking_nums, \"Age\": ages, \"Gender\": gender, \"Race\": race,\n",
        "           \"Arrest agency\": arrest_agency, \"Booking date\": book_date, \"Charges\": charges,\n",
        "           \"Bond\": bonds, \"Bondable?\": bondable}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Tunica'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape Data\n",
        "\n",
        "---\n",
        "\n",
        "Note that captchas are case-sensitive\n"
      ],
      "metadata": {
        "id": "FYZ94qrO_xiB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Y_EaNPdCI9U",
        "outputId": "9d4a8e3d-7fed-4793-a3a5-1dbd82269f92"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAPDSURBVGhD7Zi/SyNBFMevvKvP5orjirsyV8txOa4QrCwFiVgIglxj5BotxNgkjcIRUCEWKlhpiIWFYBUEW7FKZafgv2D7bt8m2Z2d/c7M/kh2180Wnyb7ZpD3cd57M+9eu7dUkB0KIRmjEJIxCiEZoxAC+PDziw8UNw4mXghKPgKtHQcTJQQlGoHWJkVuhaBEI9DaNMmFEJRoBFqbNdITcr5Ni1NfqfTxm8vUCnVRrABKNAKtfQukfkK6S4IUSQhKNMLd75JuKj/o728V83TTcfdnHtZBXKVOz0JMkqQu5HFz1paBEo1Ae/g4WpOS7Bch44hZb8HvSZGKEJRoBFobFPk/f69xCeOG2PEBZHxamIag2CiMXQhKNOLf/KIWtLeWTp32BCH6MtSiM+kUoaTr8O4XnZELQcmW6feLWTo9d0vWsIcgGSbQ34H6ydmRPw4lV4e8ftTEEoKSjeBYMfG1Jq9v02l5lmpLXiGqKQuJMCH3EpRgFehvSIKRC0FxnhG3vE2P6HeDEB1Ihgkn8VzaQk5VTzvT9Of7ezVzG/RkiD0+9O/LhBLSq36m3YUtegHfdHhG26UD9beIQnQgGSbQPj7aG1QvSYkuTdN1G8R2m3Q8iK3vXIDvLiGE7NNV2RJSnqE7wwjphUuTm/TFzbbnu9xDRi1E1dyRCBO+vYEUnPALup5TnwqRwEJeGjO0+8sSwlT3YQzmgGpCWUpciIU8Ag+b+3NjHo7DSIYJUYqceC5bQWQwAYW06W5hIIMpL1MPxiHSF/IqXxTt+wZPYWv0gOIVIBEmWIapTIkEE9JadmUMOGl4E6sm5ZJlw/cMQQjfORqWpFHdyq3ShWQMgWsUBBLSq1p9oyFJCdHcdU09GSH98uQ5JQGeU8IAp6lShe5BrA6zkM4WndjJl8qWxVULxCOaK8qkJyUk3M09GverCQjhZj5MvKexMyGau3hKnLIlihLoXxxHj9jc0a09Pu5467DaBHFqDEJ41BUb+GD0HQoJOQJ7TgNjlS/nN/HCOC6c5h6umYcBla6gExajF2I1c7l525dD4ZQEb+5ZoP++ZXr5jcxhxT4RcUqXRgj3DHACuKd4TkmYETjH8CXReTKJXrrUQuTEawjc3HMLC5BOAZ8WUYhFrJs6lyZVon3NPcL7Vn7gZxH8hhWldGEhzqgLvtnEa+75of9Gpb6Jhy9dUAifDlOzjjMC54NgL7i+U2KQ4hPiJNqU4FjPKW+Z/qnwJdnCKwacDhFF+RKE+G/iNrIYU7Of6H4SH2VTL0iHQkimuKX/bTpKhD5Y5OEAAAAASUVORK5CYII=\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdVX\n",
            "Created new thread for DeSoto\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAANcSURBVGhD7Zk/a9tAFMA9unXdQofOJWvmBuo9XjrFkCFk6+J0KAlkMQSCM2QIHgzGjsHQEELcwZCAB4PG+juYQD5GP8GrzvazT0/vTidVkk+2h9+g3JMc7uf3x6fc+K8DW+xhK8QytkIsIzEhnz99mMKtvd/5soBb32QSEYIyZHBNlsEhP2cTSbRk6cQgnBSExm4CqfQQEzECTooMd8+6kWpTNxWDcFIQLn4dSFUIElaMgJMiw92TRVYiBOHEVMpXChrQfHXg/mZ2zUlB7pnPygorFYLoxbTZDUYxCCcGoffajBVCEFbMjzsYMrHjP22PkMrN78UaJwXxPCMUfajt78G7NzzF/AHUJt57WqdM3P4lDKQYilVCprzesWJ8cRohFE4MwsVrGZ14N5gRQUExxdMWuy5jpZCqtNFKMQohz71b+LrTVMJJkRExh73h8nMY6De/1O6zcYiIN5EhsF4IliyfmIAM8Yv5BT1pHaFC6DrL5BJK+aUQfRlqwbFBFiGZEYLrVIxKyPjlEQ5lIZVHeJbXCb1aE346/Joffz85HnFxLm6JM80OQeaEID4xqQpxob2E3XQhzjw7BLndtx/BlKrqWxAnVEgAVMziOUkLmZYiSQhXlkRpC5iqKF4hhSPo0qDJOXwrrFCI4djrE5O4EAcG7QNPltDmLpq5spQpyFXnm60UMqUBIs5mIViyqJgkheibu5tBIbNDYCjEgadOKRNCEJ+YJIS40BEYM0JkT9A4zKEV0u2cwxNeu6WrniEhWLKoGE+MRFQhfHMXzfwEWlx8ABohbpkqS0LSImYhWLKCxPRqt3D94v2bGUxzb4cbdWXUQkZHsLtGQrBkqcREF+Jv7ibHKSoUQmZNfBVChv2Gd6MNT3t94gKmLCpGEFVIuF/uerxCKCkK8YugeN+HqJkLNBx7OTE0xgS5uYcddWWsy5D4GMJ1RRKiOMuaMfh/MfPmXsxHa+aIfT0kRs6+XxghDiJxwoouZna+FWXUlVELWdWUFRPcxscB91lxohFCfocI3N8i1c7D8jor0H4iITKD2/g4YP+XALRCKN2zlI5PLATLF7fxOrhn6TAXInpKoQT1iPP1uhC+t4SDnPZyG/4A9bKBsC2xEOp9SNanrixg3xvDDWcrxCoc+AeTMm+kRa/0yQAAAABJRU5ErkJggg==\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EHUV\n",
            "Created new thread for Forrest\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAMHSURBVGhD7ZlBSxtBFIDTa/9B/4C92HvJNaE9eJVA6FUUBaU0x1IoLkXBPQa8hBxWAnrXJUEPQqEXi7eACB4M/QdCrs+M2c2+2Xk7uzu7m52uc/gOzpuJcT7eezNj7envFAz6YIRohhGiGUaIZpQq5OPnn+T4a6Y0IUyGESJSaoZ8f7tupIQoTQiT4UPFXyulZ4iRwlOqEIaRwlO6EIaREqCFENbYaSmPMGhuQuPdnK39RxSrJloIYYSlHHgS5vTgmlhTFIdf1pWgPist2ghhhKWoi7iFvfo3ctOKhP4u6dBOCFW+qD++SMLfS8bd8Q58aH1KxKr1m/wMjD5CLs5ha1aeKCnUpiWhOyR+j5c9bxBNe0LMS0dYDN78K4sX8+NMXO+jiZCgeftC2DiWIq6R4JxArXNLxsa2zcmo1U/AJeal5vIU2mjT+Wx4gP4uirV2oH+J1iL0EOJlx7yBW6QUYY0MZ0Rnx3AETSSDsecQ81SQCpFnEEYLIdcbvowZzUCIP6YkhcDt8DJq7RGMiXlKxAh5Ojv6X4Twd41G8xzuX8ZvuKNvZimzMsbJqNsRPUaRlBnSPn7g4j4aCOE3nrv89XqLcVbK1KWIjTyqxyhTnR4iEcL1lk046Kn1lMIaOUYiRPGUNYE/a1+h+x6xNoR/LOYOYbAyHxv8yn5E5JEIIWK+kMRSiEaexzFXICSEIqpMYYQMGW+HhCAZnKTckL1X0bLSSCm0kWPimnpCYoQ44G4zASh7chcyhft9a7HpjY0bFMNCLBhcBGsSSUnZyMf2SL2ULUXIDLfvxfrOy8/5lywG3nj0foV7yOL0FSCXMoFuO5Qd0kY+m9/JkD1LEbLiFJPeFOhE5ZemIHOiHxmjpKRu5Ox2n6WcLadk5V+e5ITuJAyuhNGIUlK+V3mNP9NRuJpC1MFS0jRyPDfT6csIEcFSsJCkZBEivFXtnsIdMS+OSglhYClUPG/i/h+y2jqCK2JdFJUTwli2lDyRC1nmKStnKiCEeDrxWNxFDIUjZIihXIwQrZjCM7zljDueMO1mAAAAAElFTkSuQmCC\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r6ZP\n",
            "Created new thread for Hancock\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAANzSURBVGhD7Zi/ahtBEIdTpHceIOgJnFRJwIQEXIS4dhMcQ3AKQ+xSLozlJs2BIepcqBHCCCNjgQ0u3QgXAkMav4CRkR7AvUEw2Tlp7+b2Zvf++O60iq74Ct8MZ7Efe7/ZfXH9NIYSeyiFWEYpJCGvX1bY51lRCkkICslTSikkAVLGzIR8+fjWhastInnLQLRCpAwK17dIzFQIpZRTjAwkUYYsshgrhUgWTUxRMpBUQiip5Dx0YPfzB/gWwIEm12sBcyVEkkZM88h+IUXKQDITIkkipnv2sxSikLkQSpQc24UULQPJVYhEJ2YWQuq1Jiyt38EpU1P5b4W49B1WDFKckAFsVISQyiXs3XN1n5CMGwe+vnmfmOpN8L1RBIRcXO3Ar++rBnbgeCT7b8FhevavRt77JHQnHPbx2ZAVk7eQ09YlvBJCkKXagO2RaHeHImZt2YGG2jPowI/lDIRIwmL+wAnTN2EqpnHL1AR0xN3uQFd5zokJvSMTHmFvfSLDFVLpQZ3t82WwQp76UJ0utlYI6ctEyPWoC/tUyEEXLri+KSeNVXD+8rXAaHvU19fEJytXOb2eJ0Oy0npke/UykLhCxnDe3rJNyBDq2/6i754NA3VdqOchpl4TudEKSuHC3bw7ELOQRrsD5/Jv8ely7BLSh0NvweMLkWQm5v4OVtzFD362kI1esNcsAzEJEbVNIiQFVguhPEcOhrlceBrsCA336N2BGISIwF+zW0i6T5aJ5GJw1KUBPhl9PSFkBI6WgeiETJ5bLsQc6mmESGKLEWGuhjceDukukfU0QlSsF4IHQt2iP0cIRS8HM4M5BGKmECGmETjMXOwQemAMQ3eJ99kKiPKZHBzTwYmhC29CDXc9OiECezLELAQJ7gaB+Hx5z+iBMQM4MWpPKNxj3m8ZhWBtXoQUznTU1cvRh7sZkxDlHIKIs0i1HRxmTGQgZATHB6arldmAwU3DnBNjGoH1mIWoNH4nuz7hhbiLTIRE3WVF7KCikQvNLTAnBpFSdNcpPgmEYKYsb4EzYGoaNELCF4y6KQr7tBNW4YRP4khIzHTK0okx5knotpdb8CE4m/F2kIpWCILjrC8lnBNY567b5w1OTKhPERGXpFOXUYiLmicEdWd8elcvFPq/syKWnByJFhITbsHyhvsdWTH3QkqyoRRiFWP4Bxj1BiSeP0xVAAAAAElFTkSuQmCC\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UdAE\n",
            "Created new thread for Harrison\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAARfSURBVGhD7Zi/SyNREMevufb6K+5fOPUPuPpSi1wVr7wiV1ocBIuACKKNnBBL0ySITRoLUwhGBREOwVIJglZWVlc792aTtzs7M7vvZfNrzW3xAc3OJvA+O995b9/d/32FgvxQCMkZhZCcUQjJGYWQnFEIyRlvRsiX0ocQ7fq8kGshVAJFq50XcidEE4BotfNILoRoAhCtdt6ZqRBNAqLV/i9MXYgmANFqs/MErepnqJR19q6w5hL2lGsRG3Ay+L7a4je4bq/Gr9cvye+Nj6kI0QQgWu04OamTBQyIFjnkaoPVWGERKASxEvn1cTIxIZoARKudHLwLVqH1IOu4uFr7KXY9LkSRmpHv738F0M+MkGc4Lv+ArUWd5ikW3kJzSb8esHQAZ+bLNAHIVu029qPTxCtqHppQozUsrvoyzP+mm7isUUgQ0v/jrKYvMi2+Pz2I1xhQmCYBQYl9oTMkZbEj5MyxsRQJwRrbYY+wW/0EC+UklmGXdeJRXdalChFdsLQJx3dRocWK0wQgUS1+nyI1id4fKJV+w8evCju9sK6104LtHrvXAY8kOQOUTYDppFh3oNhqE67pfVfrbJGlCE4opt51CXmFm8ZmJARhUaMJQNRFN92033iWn3OYiMo5r+lBhYoqDS9EDG6+sMFir4pOokIw+rS44k/+SvtR1FCCeiMD/3YKub/rwH6sSw5UAQgVJ2MJ55LeYTHOO94L3T1sZRfiGO52sWknWRl9ITSuGA8NWCFCFqoNuOA1IV1YI13kFmLgs4TPCHUTwIc2ii134IZ+xqEyDKXDF72O0NrJKkQO9+hpJzsn0km0O9S4CpHzZC1pW4wRN+gOxEuIGNx8YYPF3hSdRGMLoy89rngMdaCl1nHwvmxCxHC3C4wSwp1X1ElWCHZFUlyF8FlCFj0CxcVnjJ8Qx3C3i611Ur/GHVdh/FjI0HbRPexkE5KwkzqpK/E1kBGIaV+a+1xnD4wiIkQb7hhtLM48hcjhHj3tZOfEO8nGlkdcBdFDhPjE1TjgsVXBQa4NeCIkwOM1yUV7mQiRwx2HOY8yLgNRhYjhbhcYJYTzQu+koeNqikLkcKezpI+QYfB6VZI63E0HeXQHogtRBjdG0llNxhet2W/cmvtcZw8pRG51J0f8TCKjSApxxVUE3wLbjsDu4R0zpBC52Fs4yLUBT7sEcb4mmWWHGOiZhEWRlRGQUJOKOtxxmK/DEasdWoiIJIOMIr2T4jWcF9j+6SEk7eSOeO/MONFw51FkZaTVpKMM93Z8q2vJIISdSRJeg8Q6yfNVyVC7LEXMqB0VDHflXBEJSa5xwYd70uuUTEJiO6mkKKKx5YyrAXyRU5921lFZzyEOqIyR8Dy5ZxNCIik5inxqJLxLkgf7GxNioMM96dSeUcggkhznCp8ajZiUxC6ZvJBxyggIh7sc5pbMQiYOe6clO+UNChm830p785tfIRbHrmpSZ5XxyxiN/AiZEYWQnFEIKUjhFf4Bjh6XjQb/y7oAAAAASUVORK5CYII=\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WGWV\n",
            "Created new thread for Lamar\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAMfSURBVGhD7Zm9ThtBEICR8gSp0hJalJJHoHUQEq3JGwSDUCSEC8qAJazI0jWBSMilEcGCIgKJBrlA0IKAAlH5ESgnt+fbu9lfr+/Wl93LFV/h/TlZ+2lmdu5mhm8XUOEOpRTy/t2nCNmc65RKCBVRCXEA30VQxgr5Pr+aGdnzbINF+C6D4K2QsomgeJmyyiqD4JWQMougeCOk7CIohQl5CjrwY6Zlxlov2YdFlF0GofAIEcSgw79dQ+Mh/5MISvEp67ELR+jQsZDh2xlcLbIiXJPxcNCA5uwXM7Z+SZ+hQxTyEsBe7Stsfo6pNaH/wq3Jg0YIL0IU5g6CGHT4gy00HtK9FPerYIRc78cSGgHcR2NdOKRy9rvJulwohEhlOCxk+NyGNjp0NhqO4XwZzc024PwZ7dWQChnsJlGx1ztJFtz3mtLxzHBCeBF8jTkKzuTPMeF1HZbqM/AxZq6+AJ1XybosaIXoI0hHLOQE+o04EkIOB2gRErVZ24VrOp4VJASLGM2PakgqpANXj9x+Q06DWML2OtxFYy3YoGKClrB+YsYIGV7u5BGCUhMvRDuXgVAIFpHKEG9Zv/9we025WUmiYqmfptq7/oJ0/Nvqhwj624gJI6R9cMzMqxgJ4Qo5m5pYIXnSFi8CHz4mV5oK/29ne3TohI0bNIdEzdVX4DQety8kdw3RHbodIUoZ1ot2mpoEIYo520Ks3LKSGxYhuWUR8gnhReiuvVbgCjlOTbwQOmdFiATTNIVJhPBpK60V2WsII4KOT1uI4tB1c7YjJA+pkAj2tiVg2CRiEYwMwtSFpDcsQnrLIngnRAT3IWwqk6MUQSlACJ+20jpSTA3Jwxghmv6EA4tQyiAUISSCvW3x4CbRGyEm0WEsIkZ427vYhSfJummA+xCcyvwQYtChTyJj/PeQn3Ar2WcPdX/ivBAmMiQyJhHhCqroIGQRIryrWm7Dg2TdpDBCGBEhup7DFxERig49C+O/h+zAQLLPlJEQ2y8QHYKJjJwyikBb1H0GiyCwDaK7lE+IxfT0LyhthPhKJcQxKiGOUQlxjEqIU1zAX3K18PNivV1IAAAAAElFTkSuQmCC\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9P9P\n",
            "Created new thread for Marion\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAPgSURBVGhD7Zg/SxxBGIcPgpA2fot8gEAkVRKuMISA+AlEohYWgqSJxVmcXSCi5A4ttPDEQkWIhdqoZ+cH0IB+jfSTffec3Xdmfzt/NnPn7d0VT7MzOwfvc+/8ZrZy9/dCDCvVD19i0NhzMbRCpIyRkGeGi+g3GcRQCelnEZKhEVIGGcTAC+Ei+l0GEQt5Of0eok8uG2USIakgETbQQv1GGWUQxi0LybCB1uklXERYGQeiOfNGfH6bw8SUaD6o7xzXwbyZmmizOTqFMgSJsIHWCQ0X8ePda2f2r/F6kKsFtcBAhE4ipr4JxznBQx3JMIHWKAKXkTy/ntWKPysun8ZuW5Pq2OyKuJXvWdD/+fN7B3CeJJ7vIIPo2SkLybCB1tHhIhQZMetinxedCSF0KVutffaugYeamJ9gUozb0KZYdegiSc+E5IFE2JDv5ouQmIXcPa6ILdN4Ltk8Wb1C8yJoi3PsDuLZhZhAMmyoa1iEWMcN6FkCi07i3LuD6GshCOoGJMIGLLjeIR450tmKmBC0LdHWZjlV6ZRKiH2L8u+qtDs8T1sR7b0ppUv0cKcwz93KcqhMv/gpFMYaYvtenXSypM0hqi1xw+Z0GxcZWdItCcmwgddkGMM96iDP7iBSIUCETiJm6RCOd4tiMgg9IwBr61CGCf4b+hFYdgR1j+04jFA6ZLFxCidJYiE9lFFchEQTomSELmtS/H7k75q3v2QeDHcK8wVxzNZypbI4lgoxb0OHou7QRaH4fxmESQgYB1LsgHDf8zvqcirbVSYkon6GJ96d7fSsO8LIIGxCLsTlGhuPcL4cMvRwd/mckkclLjQTgot+Krar3e+OcCIk/kIoU/i4E143dzPRsZe2IiYEbUv3LbEY8FTVbn4Tn159VQgvg0iF/Pr4XUHOCdEhBA9336MuJ76H3DQaSpfo4U5hnruV+fBnQ8yNRwLG6+JIPjuvKzKS5wHQJXDiOZlPJ0Uy5AkZ7hPFwlzSuRhSB+SGe9RBgbrjaJl1xfKuIoKg57Vz/K4rqPgS5aaekeF/MVTpfN8qctTlJDd1/fInO4K6x3YcdibqBilkrnkcPyMRNeoa7bkrqPASXmwTRbepbpB+OoHhTmG+I07YC+HZ9RaCii9B88tEKgSFe6ObR11VhE0IKr4EzS8rTEg23F0+pxRBZkknL/I7BBWf4GsNGooQc7iHgBU/CnX9GSq+JLvWYKIKieDhHuSoy+CnLNkJqPgS/f1hICMkCfex0GHu1gm+p6xBIyskPlnZv/z6gAovmWtuZMM92c6GDyCkO3AJcdG12/pIRoeeCRnhxkhIX3Eh/gGtICCP1UDWuwAAAABJRU5ErkJggg==\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VaBV\n",
            "Created new thread for Perry\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAANnSURBVGhD7Zg7TxtBEIApSY2TP5F0SZEmTVJwhSlcOGUaJApkISEkaJAIBQ1KKIKusUzFS0ABQhSWjC06BJ370MDPcDHx+nbv5uZm7+W785pc8QmxO3vFfJ7Zx0x/8AAl5lAKMYxSiGGUQgyjFGIYpRDDKIUYRinEMEohhlEKMQwjhHSuFuBD/b2e9R3oRMSuPAa/O42YUyEvO1ALJHoBdl+Y2MEBrMiY2tUNMz+9+IXcH0N1/k8oSxd3vg/oePvuEzseCiOFT/gN7K6/nqrAaCqkDVtWOhECIQPDxejgWhJNvIh5jTIEGiF3YDf8Qrbuubhw0olxfv1+KctwKOeFDJPb1Le5Ly7cfBS5ClEkFsPtJ/YB9B+Xnb/cGg2d5m/4+OanH+vEPSSkodtqwPzc9wBYhkIXu9nlv12IEEUSKfxpyquUSP6eQH12mPzZJhypsdumJwWPp+HJhsWKP8lWpQH7T0zs4BQ2Zexiq83MexQqRIClRIk5tNMLOVqTiResXTvjSpIcX70NrksEI4VPeBv2q/qqwBQuRBFPjHe8dYnbslA11Js9OX4Nq0iIN54eriXRxIuYODIEExOiiBIT59QVn+yFqF8/FmJVtuFMzgsZUW0KM3EhClaM3MTHaV0OfhHZChnC7Scbp9Dvbjt/uTUajBEioFJq7pNJ+tal9hJnvxivQj7/+BpAzXGtC1dKXIwSoqBiRuOiWoiU8NaFkq829YRCOAEUHH+2UZQQqwX2MxeXB+Ji6LxhUTFJWhc+ZXmJDxfCJZyC44N4x1tXSuKW9XwJS/KZxKuCSQlxbun0Jk7F+KSwrYvsGfIiSC+JXMIxwe9GE+fUFcbM+R5KfOMSeqMJKuQYzsnC7Al/wU0mpQe/LCQkRwE+5CY+Tuua6V203MRrK8QVlQfc25WDX4wjTC+GtK/hJZBLOsbbWzJAnLSqNnRH/6dvXcM9RL7s4raE2lgx1ZEcKoZLOIX7TjYIAaQKhtWChQgS3dRxpbjsxb/QFA2XcAq3LnvExZB/w0rTujSnLPPgEk5R1cKtzwfnlq6/iSdvXcYK4RJOoWtwC8tfTLwXXFolUVKMEcIlHMOt0ZGvmODblcIvJlgdGF37mpgQLukYbk1S8hWTD4UJ4ZKO4dZkRSmEoUgB04yxm/r/yQP8A66KVDpKY4DDAAAAAElFTkSuQmCC\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JXaX\n",
            "Created new thread for Yazoo\n",
            "StartingThreadID: DeSoto\n",
            "County: DeSoto\n",
            " 1 TownName: DeSoto\n",
            "StartingThreadID: 2 TownName: Forrest\n",
            "Starting ThreadID: 3 TownName: Hancock\n",
            " Hancock\n",
            "County: Hancock\n",
            "Forrest\n",
            "County: Forrest\n",
            "Starting Harrison\n",
            "County: Harrison\n",
            "ThreadID: 4 TownName: Harrison\n",
            "Starting Lamar\n",
            "County: Lamar\n",
            "ThreadID: 5 TownName: Lamar\n",
            "StartingThreadID: 6 TownName: Marion\n",
            " Starting Perry\n",
            "County: Perry\n",
            "ThreadID: 7 TownName: Perry\n",
            "Marion\n",
            "County: Marion\n",
            "Starting ThreadID: 8 TownName: Yazoo\n",
            "Yazoo\n",
            "County: Yazoo\n",
            "Starting PearlRiver\n",
            "ThreadID: 9 TownName: PearlRiver\n",
            "Starting clay\n",
            "ThreadID: 10 TownName: clay\n",
            "StartingThreadID: 11 TownName: adams\n",
            " Starting hinds\n",
            "ThreadID: 12 TownName: hinds\n",
            "adams\n",
            "StartingThreadID: 13 TownName: jackson\n",
            " jackson\n",
            "Starting jones\n",
            "ThreadID: 14 TownName: jones\n",
            "StartingThreadID: 15 TownName: kemper\n",
            " kemper\n",
            "StartingThreadID: 16 TownName: madison\n",
            " madison\n",
            "Starting tunica\n",
            "ThreadID: 17 TownName: tunica\n",
            "Lamar No. of offenders: 81\n",
            "Perry Hancock No. of offenders: 206\n",
            "No. of offenders: 23\n",
            "Forrest No. of offenders: 258\n",
            "Marion No. of offenders: 251\n",
            "Yazoo No. of offenders: 257\n",
            "DeSoto No. of offenders: 458\n",
            "Harrison No. of offenders: 650\n",
            "Exception: jackson <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)>\n",
            "Exception: jackson <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)>\n",
            "Exception: jackson <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)>\n",
            "Exception: jackson <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)>\n",
            "Exception: jackson <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)>\n",
            "Exiting tunica\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: jackson <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)>\n",
            "Request failed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-23:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/threading.py\", line 926, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"<ipython-input-16-146f0c6e0e09>\", line 24, in run\n",
            "    jackson()\n",
            "  File \"<ipython-input-11-3340ac6c0a3b>\", line 10, in jackson\n",
            "    total_count = soup(count_html, \"html.parser\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/bs4/__init__.py\", line 246, in __init__\n",
            "    elif len(markup) <= 256 and (\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perry Saved CLEAN 02-20-2022_Perry_inmates_.csv\n",
            "Perry Saved RAW 02-20-2022_Perry_inmates_.csv\n",
            "Perry saved DAILY SUMMARY for Perry\n",
            "Exiting Perry\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting clay\n",
            "\n",
            "Exiting adams\n",
            "\n",
            "Hinds 0 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Lamar Saved CLEAN 02-20-2022_Lamar_inmates_.csv\n",
            "Lamar Saved RAW 02-20-2022_Lamar_inmates_.csv\n",
            "Lamar saved DAILY SUMMARY for Lamar\n",
            "Exiting Lamar\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting madison\n",
            "\n",
            "Hinds 50 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting jones\n",
            "\n",
            "Hinds 100 650\n",
            "Exiting kemper\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 150 650\n",
            "Hancock Saved CLEAN 02-20-2022_Hancock_inmates_.csv\n",
            "Hancock Saved RAW 02-20-2022_Hancock_inmates_.csv\n",
            "Hancock saved DAILY SUMMARY for Hancock\n",
            "Exiting Hancock\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Marion Saved CLEAN 02-20-2022_Marion_inmates_.csv\n",
            "Marion Saved RAW 02-20-2022_Marion_inmates_.csv\n",
            "Marion saved DAILY SUMMARY for Marion\n",
            "Exiting Marion\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 200 650\n",
            "Yazoo Saved CLEAN 02-20-2022_Yazoo_inmates_.csv\n",
            "Yazoo Saved RAW 02-20-2022_Yazoo_inmates_.csv\n",
            "Yazoo saved DAILY SUMMARY for Yazoo\n",
            "Exiting Yazoo\n",
            "\n",
            "Forrest Saved CLEAN 02-20-2022_Forrest_inmates_.csv\n",
            "Forrest Saved RAW 02-20-2022_Forrest_inmates_.csv\n",
            "Forrest saved DAILY SUMMARY for Forrest\n",
            "Exiting Forrest\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 250 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 300 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Fieldsets empty for page: https://www.pearlrivercounty.net/sheriff/files/ICUD0047.HTM\n",
            "Hinds 350 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "DeSoto Saved CLEAN 02-20-2022_DeSoto_inmates_.csv\n",
            "DeSoto Saved RAW 02-20-2022_DeSoto_inmates_.csv\n",
            "DeSoto saved DAILY SUMMARY for DeSoto\n",
            "Exiting DeSoto\n",
            "\n",
            "Hinds 400 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 450 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 500 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Harrison Saved CLEAN 02-20-2022_Harrison_inmates_.csv\n",
            "Harrison Saved RAW 02-20-2022_Harrison_inmates_.csv\n",
            "Harrison saved DAILY SUMMARY for Harrison\n",
            "Exiting Harrison\n",
            "\n",
            "Hinds 550 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 600 650\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting hinds\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Fieldsets empty for page: https://www.pearlrivercounty.net/sheriff/files/ICUD0171.HTM\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting PearlRiver\n",
            "\n",
            "Exiting main thread\n"
          ]
        }
      ],
      "source": [
        "towns = list(jail_captcha.keys())\n",
        "\n",
        "class WebScraperThread(threading.Thread):\n",
        "    def __init__(self, thread_id, town_name, validate_r):\n",
        "        threading.Thread.__init__(self)\n",
        "        self.thread_id = thread_id\n",
        "        self.town_name = town_name\n",
        "        self.validate_r = validate_r\n",
        "\n",
        "    def run(self):\n",
        "        print(\"Starting\", self.town_name)\n",
        "\n",
        "        if self.town_name in towns:\n",
        "            town_scraper(self.town_name, self.validate_r)\n",
        "        elif self.town_name == 'PearlRiver':\n",
        "            pearlRiver()\n",
        "        elif self.town_name == 'clay':\n",
        "            clay()\n",
        "        elif self.town_name == 'adams':\n",
        "            adams()\n",
        "        elif self.town_name == 'hinds':\n",
        "            hinds()\n",
        "        elif self.town_name == 'jackson':\n",
        "            jackson()\n",
        "        elif self.town_name == 'jones':\n",
        "            jones()\n",
        "        elif self.town_name == 'kemper':\n",
        "            kemper()\n",
        "        elif self.town_name == 'madison':\n",
        "            madison()\n",
        "        elif self.town_name == 'tunica':\n",
        "            tunica()\n",
        "        else:\n",
        "            raise Exception(\"Town not found:\", self.town_name)\n",
        "\n",
        "        print(\"Exiting\", self.town_name, end='\\n\\n')\n",
        "\n",
        "threads = []\n",
        "thread_id = 1\n",
        "\n",
        "# Create new threads for JailTracker jails\n",
        "for town_name in towns:\n",
        "    # Get captcha image and enter in captcha information to get validation key\n",
        "    captcha_matched = False\n",
        "    while not captcha_matched:\n",
        "        captcha_r = requests.get('https://omsweb.public-safety-cloud.com/jtclientweb/captcha/getnewcaptchaclient')\n",
        "        captchaKey = captcha_r.json()['captchaKey']\n",
        "        image = captcha_r.json()['captchaImage']\n",
        "        html = f'<img src=\"{image}\"/>'\n",
        "        display.display(display.HTML(html))\n",
        "        user_code = input()\n",
        "\n",
        "        jail_captcha[town_name]['user_code'] = str(user_code)\n",
        "        validate_r = requests.post( \n",
        "            'https://omsweb.public-safety-cloud.com/jtclientweb/Captcha/validatecaptcha',\n",
        "            json={'userCode': jail_captcha[town_name]['user_code'] , 'captchaKey': captchaKey}\n",
        "        )\n",
        "        captcha_matched = validate_r.json()['captchaMatched']\n",
        "\n",
        "        if not captcha_matched:\n",
        "            print(\"Incorrect captcha\")\n",
        "\n",
        "    thread = WebScraperThread(thread_id, town_name, validate_r)\n",
        "    print(\"Created new thread for\", town_name)\n",
        "    threads.append(thread)\n",
        "    thread_id += 1\n",
        "\n",
        "# Create new threads for remaining 9 jails\n",
        "for town_name in ['PearlRiver', 'clay', 'adams', 'hinds', 'jackson', 'jones', 'kemper', 'madison', 'tunica']:\n",
        "    thread = WebScraperThread(thread_id, town_name, 'None')\n",
        "    threads.append(thread)\n",
        "    thread_id += 1\n",
        "\n",
        "for t in threads:\n",
        "    t.start()\n",
        "    print(\"ThreadID:\", t.thread_id, \"TownName:\", t.town_name)\n",
        "\n",
        "# Wait for all threads to complete\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "print(\"Exiting main thread\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_url = \"https://services.co.jackson.ms.us/jaildocket/_inmateList.php?Function=list&Page=1\"\n",
        "headers={\n",
        "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:12.0) Gecko/20100101 Firefox/12.0',\n",
        "    'Accept': 'text/html',\n",
        "    'Accept-Encoding': 'gzip, deflate',\n",
        "    'Accept-Language': 'en-US',\n",
        "    'Referer': 'http://www.google.com/',\n",
        "}\n",
        "#     'Connection': 'keep-alive',\n",
        "#     'Sec-Fetch-Dest': 'document',\n",
        "#     'Sec-Fetch-Mode': 'navigate',\n",
        "#     'Sec-Fetch-Site': 'none',\n",
        "#     'Sec-Fetch-User': '?1',\n",
        "#     'Cache-Control': 'max-age=0',\n",
        "#     'sec-ch-ua-platform': '\"Windows\"',\n",
        "#     'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"98\", \"Microsoft Edge\";v=\"98\"',\n",
        "# }\n",
        "import certifi\n",
        "request = urllib.request.Request(count_url, headers=headers)\n",
        "response = urllib.request.urlopen(request, timeout=10, cafile=certifi.where())\n",
        "data = response.read() \n",
        "response.close()\n",
        "print(\"Jackson - Total Inmate Count:\", data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "Eg-KmkkuPsL-",
        "outputId": "2330a7b5-248d-4f85-a35f-bc799a386fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: cafile, capath and cadefault are deprecated, use a custom context instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "URLError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0;32m-> 1350\u001b[0;31m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1450\u001b[0m             self.sock = self._context.wrap_socket(self.sock,\n\u001b[0;32m-> 1451\u001b[0;31m                                                   server_hostname=server_hostname)\n\u001b[0m\u001b[1;32m   1452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m    869\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-27b3806d5aa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcertifi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcafile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcertifi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 543\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0;32m-> 1393\u001b[0;31m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0m\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0mhttps_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAbstractHTTPHandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_request_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mdo_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1350\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[1;32m   1351\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# timeout error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1091)>"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "JailWebScraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}