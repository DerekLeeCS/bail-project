{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bCRxhuoF9rJE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import re\n",
        "import json\n",
        "import threading\n",
        "from decimal import Decimal\n",
        "\n",
        "import time\n",
        "from datetime import datetime, date\n",
        "from pytz import timezone\n",
        "\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import requests\n",
        "from requests.structures import CaseInsensitiveDict\n",
        "import urllib.request\n",
        "from urllib.error import URLError\n",
        "import ssl\n",
        "import certifi\n",
        "from IPython import display\n",
        "\n",
        "from typing import Union"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QwEP_Mksqqxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b6319f-c74f-459c-f72b-41e6419d3ec6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLLFpjTJmiY9",
        "outputId": "30b2d1c7-953c-47b3-b717-8c681d2aa724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date used for scraping: 02-21-2022\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('/content/drive/MyDrive/'):\n",
        "    raise Exception(\"Error: Mount Google Drive before continuing!\")\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/Data Science for Social Good - Spring 2022/'\n",
        "\n",
        "# Define directory that contains intermediate SSL certificates\n",
        "CERT_DIR = BASE_DIR + 'certificates/'\n",
        "\n",
        "# Define directories to save data\n",
        "SCRAPE_DIR = BASE_DIR + 'scraped_files/'\n",
        "RAW_DIR = SCRAPE_DIR + 'RAW/'\n",
        "CLEAN_DIR = SCRAPE_DIR + 'CLEAN/'\n",
        "DAILY_SUMMARY_DIR = SCRAPE_DIR + 'DAILY_SUMMARY/'\n",
        "\n",
        "today_date = datetime.now(timezone('US/Eastern')).strftime(\"%m-%d-%Y\")\n",
        "\n",
        "# Create all directories on the given paths if needed\n",
        "os.makedirs(RAW_DIR + today_date, exist_ok=True)\n",
        "os.makedirs(CLEAN_DIR + today_date, exist_ok=True)\n",
        "os.makedirs(DAILY_SUMMARY_DIR, exist_ok=True)\n",
        "print(\"Date used for scraping:\", today_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add missing intermediate SSL certificates\n",
        "\n",
        "---\n",
        "\n",
        "This is required for Jackson, and should only be run once. We add the intermediate certificates found at: https://services.co.jackson.ms.us/jaildocket/_inmateList.php"
      ],
      "metadata": {
        "id": "ICGz60wGI8HI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(certifi.where(), 'a') as global_cert_file:\n",
        "    for filename in os.listdir(CERT_DIR):\n",
        "        with open(CERT_DIR + filename, 'r') as missing_cert_file:\n",
        "            cert_data = missing_cert_file.read()\n",
        "            global_cert_file.write('\\n' + cert_data)"
      ],
      "metadata": {
        "id": "33mAxzU5GgIK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0xjFRLk7M6CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_url(request: Union[str, urllib.request.Request]) -> str:\n",
        "    ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH, cafile=certifi.where())\n",
        "    response = urllib.request.urlopen(request, timeout=10, context=ctx)\n",
        "    data = response.read() \n",
        "    response.close()\n",
        "    return data\n",
        "\n",
        "def request_url(request: str, town: str) -> Union[str, None]:\n",
        "    NUM_ATTEMPTS = 6\n",
        "\n",
        "    if town in {'adams', 'madison', 'jackson', 'pearlRiver'}:\n",
        "        user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
        "        headers = {'User-Agent':user_agent} \n",
        "        request = urllib.request.Request(request, headers=headers)  # The assembled request\n",
        "\n",
        "    # Try the connection until success or NUM_ATTEMPTS is exceeded\n",
        "    for _ in range(NUM_ATTEMPTS):\n",
        "        try:\n",
        "            return read_url(request)\n",
        "        except URLError as str_error:\n",
        "            time.sleep(2)\n",
        "            print(\"Exception:\", town, str_error)\n",
        "\n",
        "    print(\"Request failed for\", town)\n",
        "    return None\n",
        "\n",
        "\n",
        "def get_filename(date: str, town_name: str) -> str:\n",
        "    return CLEAN_DIR + date + '/' + date + '_' + town_name + '.csv'\n",
        "\n",
        "\n",
        "jail_captcha = {\n",
        "    'DeSoto': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/DeSoto_County_Ms/'},\n",
        "    'Forrest': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Forrest_County_MS/'},\n",
        "    'Hancock': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/HANCOCK_COUNTY_MS/'},\n",
        "    'Harrison': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/HARRISON_COUNTY_JAIL_MS/'},\n",
        "    'Lamar': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Lamar_County_MS/'},\n",
        "    'Marion': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Marion_County_MS/'},\n",
        "    'Perry': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Perry_County_MS/'},\n",
        "    'Yazoo': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Yazoo_County_MS/'}\n",
        "}"
      ],
      "metadata": {
        "id": "QSOcMIogM6Ks"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Townscraper Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4xDTR2DF_Q-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def town_scraper(town, validate_r):\n",
        "    print(\"County:\", town)\n",
        "    captchaKey_aftervalidation = validate_r.json()['captchaKey']\n",
        "\n",
        "    # Get offender information\n",
        "    records_r = requests.post(\n",
        "        jail_captcha[town]['URL'],\n",
        "        json={'captchaKey': validate_r.json()['captchaKey']}\n",
        "    )\n",
        "    offenderViewKey = records_r.json()['offenderViewKey']\n",
        "    total = len(records_r.json()['offenders'])\n",
        "    print(town, \"No. of offenders:\", total)\n",
        "\n",
        "    # Loop through information and store in Dataframe\n",
        "    inmates = {}\n",
        "    RAW_inmates = {}\n",
        "\n",
        "    for offender in records_r.json()['offenders']:\n",
        "        inmates[offender['arrestNo']] = {}\n",
        "        inmates[offender['arrestNo']]['Arrest Number'] = offender['arrestNo']\n",
        "        for j in ['firstName', 'lastName', 'agencyName', 'originalBookDateTime']:\n",
        "            inmates[offender['arrestNo']][j] = offender[j]\n",
        "\n",
        "        RAW_inmates[offender['arrestNo']] = {}\n",
        "        RAW_inmates[offender['arrestNo']]['Arrest Number'] = offender['arrestNo']\n",
        "        RAW_inmates[offender['arrestNo']]['offenders'] = offender\n",
        "\n",
        "    df = pd.DataFrame(columns=['Arrest Number', 'firstName', 'lastName', 'agencyName', 'originalBookDateTime',\n",
        "                               'bondAmount', 'bondType', 'chargeDescription', 'chargeStatus',\n",
        "                               'crimeType', 'Bond Total Amount', 'charges', 'cases'])\n",
        "    RAW_df = pd.DataFrame(columns=['Arrest Number', 'charges', 'cases'])\n",
        "\n",
        "    arrestNos = {}\n",
        "    for arrestNo in list(inmates.keys()):\n",
        "        arrestNos[str(arrestNo)] = {}\n",
        "        arrestNos[str(arrestNo)][\"Number\"] = arrestNo\n",
        "        URL = jail_captcha[town]['URL'] + str(arrestNo) + '/offenderbucket/' + str(offenderViewKey)\n",
        "        response = requests.post(URL, json={'captchaImage': image, 'captchaKey': captchaKey_aftervalidation})\n",
        "\n",
        "        # Iterate through charges in response\n",
        "        bond_total = 0\n",
        "        for column in ['charges', 'cases', 'bondType', 'bondAmount', 'chargeDescription', 'chargeStatus', 'crimeType']:\n",
        "            inmates[str(arrestNo)][column] = []\n",
        "\n",
        "        RAW_inmates[str(arrestNo)]['charges'] = []\n",
        "        for charge in response.json()['charges']:\n",
        "            # print(charge)\n",
        "            inmates[str(arrestNo)]['charges'].append(charge)\n",
        "            RAW_inmates[str(arrestNo)]['charges'].append(charge)\n",
        "            bondAmt = charge['bondAmount']\n",
        "            if bondAmt == None:\n",
        "                bondAmt = 0\n",
        "            bond_total = bond_total + float(bondAmt)\n",
        "            for column in ['bondType', 'bondAmount', 'chargeDescription', 'chargeStatus', 'crimeType']:\n",
        "                inmates[str(arrestNo)][column].append(charge[column])\n",
        "\n",
        "        inmates[str(arrestNo)]['Bond Total Amount'] = bond_total\n",
        "\n",
        "        inmates[str(arrestNo)]['Potentially Bondable?'] = ''\n",
        "        arrestNos[str(arrestNo)]['Potentially Bondable?'] = ''\n",
        "        if all((x == 'WRITTEN BOND' or\n",
        "                x == 'SURETY BOND' or\n",
        "                x == 'OWN RECOGNIZANCE BOND' or\n",
        "                x == 'OFF BOND' or\n",
        "                x == 'SURETY BOND WITH CONDITIONS' or\n",
        "                x == 'SURETY'\n",
        "               ) for x in inmates[str(arrestNo)]['bondType']) and len(inmates[str(arrestNo)]['bondType']) > 0:\n",
        "            inmates[str(arrestNo)]['Potentially Bondable?'] = 'Yes'\n",
        "            arrestNos[str(arrestNo)]['Potentially Bondable?'] = 'Yes'\n",
        "\n",
        "        inmates[str(arrestNo)]['Sex/DV charge'] = ''\n",
        "        arrestNos[str(arrestNo)]['Sex/DV charge'] = ''\n",
        "        if any(re.search(\"sex\", str(x), re.IGNORECASE) for x in inmates[str(arrestNo)]['chargeDescription']):\n",
        "            inmates[str(arrestNo)]['Sex/DV charge'] = 'Yes'\n",
        "            arrestNos[str(arrestNo)]['Sex/DV charge'] = 'Yes'\n",
        "\n",
        "        inmates[str(arrestNo)]['Over 5k?'] = ''\n",
        "        arrestNos[str(arrestNo)]['Over 5k?'] = ''\n",
        "        if int(inmates[str(arrestNo)]['Bond Total Amount']) > 5000:\n",
        "            inmates[str(arrestNo)]['Over 5k?'] = 'Yes'\n",
        "            arrestNos[str(arrestNo)]['Over 5k?'] = 'Yes'\n",
        "\n",
        "        # Iterate through cases in response\n",
        "        RAW_inmates[str(arrestNo)]['cases'] = []\n",
        "        for case in response.json()['cases']:\n",
        "            RAW_inmates[str(arrestNo)]['cases'].append(case)\n",
        "\n",
        "        # Iterate through offenderSpecialFields in response\n",
        "        total = []\n",
        "        for item in response.json()['offenderSpecialFields']:\n",
        "            temp = {}\n",
        "            temp[item['labelText']] = item['offenderValue']\n",
        "            total.append(temp)\n",
        "        RAW_inmates[str(arrestNo)]['offenderSpecialFields'] = total\n",
        "\n",
        "        # -----------------------------------------------------\n",
        "        # Get offenderViewKey for next iteration of for loop  \n",
        "        offenderViewKey = response.json()['offenderViewKey']\n",
        "\n",
        "        # Add inmate to dataframe\n",
        "        df = df.append(inmates[str(arrestNo)], ignore_index=True)\n",
        "        RAW_df = RAW_df.append(RAW_inmates[str(arrestNo)], ignore_index=True)\n",
        "\n",
        "    # Save inmate info for town to Google Drive\n",
        "    file_name = str(today_date) + \"_\" + town + '_inmates_' + '.csv'\n",
        "    df.to_csv(CLEAN_DIR + today_date + \"/\" + file_name)\n",
        "    print(town, \"Saved CLEAN\", file_name)\n",
        "\n",
        "    # Save RAW inmate info for town to Google Drive\n",
        "    file_name = str(today_date) + \"_\" + town + '_inmates_' + '.csv'\n",
        "    RAW_df.to_csv(RAW_DIR + today_date + \"/\" + file_name)\n",
        "    print(town, \"Saved RAW\", file_name)\n",
        "\n",
        "    daily_summary = pd.DataFrame(columns=[\"Date\", \"Arrest Numbers\", \"Total arrest Numbers\"])\n",
        "    temp = {}\n",
        "    temp[\"Date\"] = today_date\n",
        "    temp[\"Arrest Numbers\"] = arrestNos\n",
        "    temp[\"Total arrest Numbers\"] = len(arrestNos)\n",
        "    daily_summary = daily_summary.append(temp, ignore_index=True)\n",
        "\n",
        "    daily_summary_filename = DAILY_SUMMARY_DIR + str(town) + '.csv'\n",
        "    try:\n",
        "        daily_summary = pd.read_csv(daily_summary_filename).append(daily_summary)\n",
        "    except FileNotFoundError:\n",
        "        print(\"Daily summary file not found; creating new file:\", daily_summary_filename)\n",
        "\n",
        "    daily_summary = daily_summary[[\"Date\", \"Arrest Numbers\", \"Total arrest Numbers\"]]\n",
        "    daily_summary.to_csv(daily_summary_filename)\n",
        "    print(town, \"saved DAILY SUMMARY for\", town)\n",
        "\n",
        "# Audio notification so you know when to input the captcha code again\n",
        "# output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')\n"
      ],
      "metadata": {
        "id": "Vix_8hbpsnU9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PearlRiver Script - website without bond amounts\n",
        "\n",
        "---\n",
        "\n",
        "Looks like this isn't used?\n",
        "\n"
      ],
      "metadata": {
        "id": "LdOKe3GJ_L-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def townScraperPearl():\n",
        "    town = 'PearlRiver'\n",
        "    jail = 'PEARLRIVERMS'\n",
        "\n",
        "    print(\"County:\", town)\n",
        "\n",
        "    headers = CaseInsensitiveDict()\n",
        "    headers[\"Content-Type\"] = \"application/json\"\n",
        "    headers[\"Content-Length\"] = \"0\"\n",
        "    result = requests.post(\"http://inmates.bluhorse.com/Default.aspx/GetPasskey\", headers=headers).json()['d']\n",
        "    keyAns = result.split(\"|||\");\n",
        "    key = keyAns[0]\n",
        "    ans = keyAns[1]\n",
        "\n",
        "    InmateList = 'http://inmates.bluhorse.com/' + \"InmateService.svc/GetInmates2?Jail=\" + jail + \"&key=\" + key + \"&ans=\" + ans\n",
        "    GetInmateBookNo = requests.get(InmateList).json()['GetInmates2Result']\n",
        "    total = len(GetInmateBookNo)\n",
        "    print(town, \"No. of offenders:\", total)\n",
        "\n",
        "    isLogin = 'false'\n",
        "    rootURL = \"http://inmates.bluhorse.com/InmateService.svc\"\n",
        "\n",
        "    WebFields_url = 'http://inmates.bluhorse.com/' + \"InmateService.svc/GetJailInfo1?Jail=\" + jail + \"&key=\" + key + \"&Answer=\" + ans\n",
        "    WebFields = requests.get(WebFields_url).json()['GetJailInfo1Result']['WebFields']\n",
        "\n",
        "    inmates = {}\n",
        "    df = pd.DataFrame(columns=[\"Arrest Number\", 'firstName', 'lastName', 'agencyName', 'originalBookDateTime',\n",
        "                               'bondAmount', 'bondType', 'chargeDescription', 'chargeStatus',\n",
        "                               'crimeType', 'Bond Total Amount', 'charges', 'cases'])\n",
        "\n",
        "    for offender in GetInmateBookNo:\n",
        "        bookNO = offender['BookNo']\n",
        "\n",
        "        inmates[bookNO] = {}\n",
        "        for column in ['charges', 'cases', 'bondType', 'bondAmount', 'chargeDescription', 'chargeStatus', 'crimeType']:\n",
        "            inmates[bookNO][column] = []\n",
        "\n",
        "        inmates[bookNO][\"Arrest Number\"] = offender['BookNo']\n",
        "        inmates[bookNO]['firstName'] = offender['FName']\n",
        "        inmates[bookNO]['lastName'] = offender['LName']\n",
        "\n",
        "    for i in GetInmateBookNo:\n",
        "        BookNO = i['BookNo']\n",
        "\n",
        "        GetInmateCharges = requests.get(\n",
        "            rootURL + \"/GetInmateCharges?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmate = requests.get(\n",
        "            rootURL + \"/GetInmate?Jail=\" + jail + \"&bookno=\" + BookNO + \"&key=\" + key + \"&answer=\" + ans + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmate_Arrest_Info = requests.get(\n",
        "            rootURL + \"/GetInmate_Arrest_Info?Jail=\" + jail + \"&bookno=\" + BookNO + \"&key=\" + key + \"&answer=\" + ans + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmateHolds = requests.get(\n",
        "            rootURL + \"/GetInmateHolds?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmateCourtHistory = requests.get(\n",
        "            rootURL + \"/GetInmateCourtHistory?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "        GetInmateBonds = requests.get(\n",
        "            rootURL + \"/GetInmateBonds?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "\n",
        "        GetInmateChargesResult = GetInmateCharges.json()['GetInmateChargesResult']\n",
        "        GetInmateResult = GetInmate.json()['GetInmateResult']\n",
        "        GetInmate_Arrest_InfoResult = GetInmate_Arrest_Info.json()['GetInmate_Arrest_InfoResult']\n",
        "        GetInmateHoldsResult = GetInmateHolds.json()['GetInmateHoldsResult']\n",
        "        GetInmateCourtHistoryResult = GetInmateCourtHistory.json()['GetInmateCourtHistoryResult']\n",
        "        GetInmateBondsResult = GetInmateBonds.json()['GetInmateBondsResult']\n",
        "\n",
        "        inmates[BookNO]['agencyName'] = GetInmateResult['ArrestAgency']\n",
        "        inmates[BookNO]['originalBookDateTime'] = GetInmateResult['ArrestDate']\n",
        "        # inmates[BookNO]['cases'] = \n",
        "\n",
        "        bond_total = 0\n",
        "        for charge in GetInmateChargesResult:\n",
        "            inmates[BookNO]['charges'].append(charge)\n",
        "            bondAmt = charge['Bond']\n",
        "            if bondAmt == None:\n",
        "                bondAmt = 0\n",
        "            bond_total = bond_total + float(bondAmt)\n",
        "\n",
        "            # inmates[BookNO]['bondType'].append(charge['ChargeType'])\n",
        "            inmates[BookNO]['bondAmount'].append(charge['Bond'])\n",
        "            inmates[BookNO]['chargeDescription'].append(charge['Description'])\n",
        "            inmates[BookNO]['chargeStatus'].append(charge['Disposition'].strip('\\n').strip('\\r'))\n",
        "            inmates[BookNO]['crimeType'].append(charge['ChargeType'])\n",
        "\n",
        "            inmates[BookNO]['Bond Total Amount'] = bond_total\n",
        "\n",
        "        inmates[BookNO]['GetInmateChargesResult'] = GetInmateChargesResult\n",
        "        inmates[BookNO]['GetInmateBondsResult'] = GetInmateBondsResult\n",
        "        inmates[BookNO]['GetInmateResult'] = GetInmateResult\n",
        "        inmates[BookNO]['GetInmate_Arrest_InfoResult'] = GetInmate_Arrest_InfoResult\n",
        "        inmates[BookNO]['GetInmateHoldsResult'] = GetInmateHoldsResult\n",
        "        inmates[BookNO]['GetInmateCourtHistoryResult'] = GetInmateCourtHistoryResult\n",
        "\n",
        "        df = df.append(inmates[BookNO], ignore_index=True)\n",
        "\n",
        "    # Save inmate info for town to Google Drive\n",
        "    df.to_csv(get_filename(date=today_date, town_name=(town + '_inmates_')))\n"
      ],
      "metadata": {
        "id": "v6IkY85Atdum"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PearlRiver Script - website with bonds\n",
        "\n",
        "---\n",
        "\n",
        "Notes:\n",
        "\n",
        "*   We will often have to retry our GET requests. It may be because the scraper is getting temporarily blocked/throttled. This doesn't mean there is an error, but it does slow down the scraping.\n",
        "*   Some pages we scrape are blank, and we print out a link to that page when it happens."
      ],
      "metadata": {
        "id": "7BNaN1iD_VRb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8G0gLY9x7sh0"
      },
      "outputs": [],
      "source": [
        "def pearlRiver():\n",
        "    pearlRiver_html = request_url('https://www.pearlrivercounty.net/sheriff/files/ICURRENT.HTM', \"pearlRiver\")\n",
        "    mad_soup = soup(pearlRiver_html, \"html.parser\")\n",
        "    prefix = 'https://www.pearlrivercounty.net/sheriff/files/ICUD'\n",
        "\n",
        "    # Determine the number of people detained\n",
        "    inmate_num = int(mad_soup.find_all('b')[4].get_text())\n",
        "\n",
        "    # Generate a list of links to intake reports\n",
        "    # Intake reports are in the form https://www.pearlrivercounty.net/sheriff/files/ICUD0001.HTM\n",
        "    links = []\n",
        "    for i in range(inmate_num):\n",
        "        links.append(prefix + str(i + 1).rjust(4, '0') + '.HTM')\n",
        "\n",
        "    # Initialize lists\n",
        "    # Note that the index is offset by one from the list of individuals in the url (starts at 1 rather than zero)\n",
        "    names, ages, birthdays, arrest_dates, heights, weights, race, sex, hair_colors = ([] for _ in range(9))\n",
        "    eye_colors, facial_hair, complexions, off_dates, case_nums, intake_dates, intake_times = ([] for _ in range(7))\n",
        "    intake_nums, arrest_agencies, written_bonds, cash_bonds, bondable, bondable_names, offenses = ([] for _ in range(7))\n",
        "    num_bondable = 0\n",
        "    index = 0\n",
        "\n",
        "    for link in links:\n",
        "        # print(link)\n",
        "        inmate_html = request_url(link, \"pearlRiver\")\n",
        "        inmate_soup = soup(inmate_html, \"html.parser\")\n",
        "        fieldsets = inmate_soup.find_all(\"fieldset\")\n",
        "\n",
        "        # Some pages are empty so we have this here to prevent any errors\n",
        "        if len(fieldsets) == 0:\n",
        "            print(\"Fieldsets empty for page:\", link)\n",
        "            continue\n",
        "\n",
        "        # Find names and split among respective lists according to length\n",
        "        # Note that this produces an array for the first name when there are three names, is this an issue?\n",
        "        name_dob_age = fieldsets[0].find_all(\"tr\")\n",
        "        name = name_dob_age[0].get_text()[9:]\n",
        "        names.append(name)\n",
        "\n",
        "        # Find age and birthday from same fieldset as name\n",
        "        dob_age = name_dob_age[3].find_all(\"td\")\n",
        "        birthdays.append(dob_age[0].get_text()[5:].strip())\n",
        "        ages.append(dob_age[1].get_text()[5:].strip())\n",
        "\n",
        "        # Scrape the \"description\" box\n",
        "        appearance_fields = fieldsets[1].find_all(\"tr\")\n",
        "        heights.append(appearance_fields[0].find_all(\"td\")[0].get_text()[5:].strip())\n",
        "        weights.append(appearance_fields[0].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "        race.append(appearance_fields[1].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "        sex.append(appearance_fields[1].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "        hair_colors.append(appearance_fields[2].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "        eye_colors.append(appearance_fields[2].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "        facial_hair.append(appearance_fields[3].find_all(\"td\")[0].get_text()[7:].strip())\n",
        "        complexions.append(appearance_fields[3].find_all(\"td\")[1].get_text()[7:].strip())\n",
        "\n",
        "        # Scrape the \"intake/booking\" box\n",
        "        intake_fields = fieldsets[2].find_all(\"tr\")\n",
        "        off_date = intake_fields[0].get_text()[11:].split()[0]\n",
        "        if off_date == \"00/00/0000\":\n",
        "            off_date = \"\"\n",
        "        off_dates.append(off_date)\n",
        "        case_num = intake_fields[0].get_text()[11:].split()[3]\n",
        "        if case_num == \"OTHER\":\n",
        "            case_num = \"\"\n",
        "        case_nums.append(case_num)\n",
        "\n",
        "        intake_dates.append(intake_fields[1].get_text().split()[2])\n",
        "        intake_times.append(intake_fields[1].get_text().split()[4])\n",
        "        intake_nums.append(intake_fields[2].get_text()[11:].strip())\n",
        "        arrest_agencies.append(intake_fields[3].get_text()[19:].strip())\n",
        "\n",
        "        # Bond Information box\n",
        "        written_bond = (float(fieldsets[4].find_all(\"tr\")[0].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "        written_bonds.append(written_bond)\n",
        "        bondable_ = bool(written_bond)\n",
        "        bondable.append(bondable_)\n",
        "        if bondable_:\n",
        "            num_bondable += 1\n",
        "            bond_names = [name.strip(), index]\n",
        "            bondable_names.append(bond_names)\n",
        "        cash_bonds.append(float(fieldsets[4].find_all(\"tr\")[1].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "\n",
        "        # Offense text box\n",
        "        offs = fieldsets[5].find_all(\"tr\")[2:]\n",
        "        desc = []\n",
        "        court = []\n",
        "        bond_amt_ = []\n",
        "        for o in offs:\n",
        "            off = o.find_all(\"td\")\n",
        "            desc.append(off[0].get_text())\n",
        "            court.append(off[1].get_text().strip() + \" COURT\")\n",
        "            bond_amt = off[2].get_text().strip().replace(\",\", \"\")\n",
        "            if bond_amt == \"\":\n",
        "                bond_amt = \"0\"\n",
        "            bond_amt_.append(float(bond_amt.strip(\"$s\")))\n",
        "\n",
        "        off_dict = {\"Description\": desc, \"Court\": court, \"Bond amount\": bond_amt_}\n",
        "        offenses.append(off_dict)\n",
        "        index += 1\n",
        "\n",
        "    # Generate a dataframe with dictionary dic, then print dataframe as specified\n",
        "    dic = {\"Name\": names, \"Ages\": ages, \"Birthdays\": birthdays,\n",
        "           \"Height\": heights, \"Weight\": weights, \"Race\": race, \"Sex\": sex, \"Hair color\": hair_colors,\n",
        "           \"Eye color\": eye_colors, \"Facial hair\": facial_hair, \"Complexion\": complexions,\n",
        "           \"Off date\": off_dates, \"Case number\": case_nums, \"Intake date\": intake_dates, \"Intake number\": intake_nums,\n",
        "           \"Arresting agency\": arrest_agencies,\n",
        "           \"Bondable?\": bondable, \"Written bond\": written_bonds, \"Cash bond\": cash_bonds,\n",
        "           \"Offense(s)\": offenses}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='PearlRiver'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clay Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IW6qryIG_YhW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5mp4WpQWnMkz"
      },
      "outputs": [],
      "source": [
        "def clay():\n",
        "    # Open the first page of the roster to generate links to inmate pages\n",
        "    roster_prefix = \"http://www.claysheriffms.org/roster.php\"\n",
        "    u_clay = requests.get(roster_prefix)\n",
        "    clay_soup = soup(u_clay.text, \"html.parser\")\n",
        "    u_clay.close()\n",
        "\n",
        "    inmate_prefix = \"http://www.claysheriffms.org/\"\n",
        "    inmate_links, name, bookNo, age, sex, race, arrest_agency, arrest_date, bond, bondable, offenses = ([] for _ in range(11))\n",
        "    num_bondable = 0\n",
        "\n",
        "    num_inmates = int(clay_soup.select(\".ptitles\")[0].get_text().split()[2].replace(\"(\", \"\").replace(\")\", \"\"))\n",
        "    num_pages = math.ceil(num_inmates / 10)\n",
        "\n",
        "    # Step through the pages of the roster and scrape for urls to inmate pages\n",
        "    for page in range(num_pages):\n",
        "        u_clay = requests.get(roster_prefix + \"?grp=\" + str(page * 10))\n",
        "        clay_soup = soup(u_clay.text, \"html.parser\")\n",
        "        u_clay.close()\n",
        "\n",
        "        inmate_table = clay_soup.select(\".inmateTable\")\n",
        "        for i in inmate_table:\n",
        "            inmate_links.append(inmate_prefix + i.select(\".text2\")[-1].get(\"href\"))\n",
        "\n",
        "    # Iterate through list of links to inmate pages and find information\n",
        "    for inmate in inmate_links:\n",
        "        u_inmate = requests.get(inmate)\n",
        "        inmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "        u_inmate.close()\n",
        "\n",
        "        table = inmate_soup.find_all(\"table\")[6].select(\".text2\")\n",
        "        name.append(' '.join(inmate_soup.select('.ptitles')[0].get_text().split()))\n",
        "        bookNo.append(table[0].get_text().strip())\n",
        "        age.append(int(table[1].get_text().strip()))\n",
        "        sex.append(table[2].get_text().strip())\n",
        "        race.append(table[3].get_text().strip())\n",
        "        arrest_agency.append(table[4].get_text().strip())\n",
        "        arrest_date.append(table[5].get_text().split()[0])\n",
        "\n",
        "        bond_ = float(table[8].get_text().strip(\"$\"))\n",
        "        bond.append(bond_)\n",
        "        bondable.append(bool(bond_))\n",
        "        if (bool(bond_)):\n",
        "            num_bondable += 1\n",
        "\n",
        "        offenses.append(str(table[7]).strip(\"<span class=\\\"text2\\\">\").strip(\"</\").split(\"<br/>\"))\n",
        "\n",
        "    # Generate a dictionary, then create a dataframe to print\n",
        "    dic = {\"Name\": name, \"Booking number\": bookNo, \"Arrest agency\": arrest_agency, \"Arrest date\": arrest_date,\n",
        "           \"Bondable?\": bondable, \"Bond\": bond, \"Age\": age, \"Sex\": sex, \"Race\": race, \"Offense(s)\": offenses}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Clay'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adams Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "byk-b-sj_ck7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "G0fynMX1znuH"
      },
      "outputs": [],
      "source": [
        "def adams():\n",
        "    Adams_html = request_url('http://www.adamscosheriff.org/inmate-roster/', \"adams\")\n",
        "    adams_soup = soup(Adams_html, \"html.parser\")\n",
        "    page_num = adams_soup.select(\".page-numbers\")[-2].get_text()\n",
        "    page_num = adams_soup.select(\".page-numbers\")[-2].get_text()\n",
        "\n",
        "    names, book_Nos, age, gender, race, booking_date, charges, bonds, bondable, bondable_names = ([] for _ in range(10))\n",
        "    num_bondable = 0\n",
        "    num_errs = 0\n",
        "\n",
        "    for page in range(int(page_num)):\n",
        "        page_html = request_url('http://www.adamscosheriff.org/inmate-roster/page/' + str(page + 1) + \"/\", \"adams\")\n",
        "        page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "        page_profile_links = page_soup.select(\".profile-link\")\n",
        "\n",
        "        for profile in page_profile_links:\n",
        "            try:\n",
        "                inmate_html = request_url(profile.a.get(\"href\"), \"adams\")\n",
        "                inmate_soup = soup(inmate_html, \"html.parser\")\n",
        "\n",
        "                p_vals = inmate_soup.find_all(\"p\")\n",
        "                name = (p_vals[0].get_text().strip(\"Full Name:\").strip())\n",
        "                names.append(name)\n",
        "                book_Nos.append((p_vals[1].get_text().strip(\"Booking Number:\").strip()))\n",
        "                age.append(p_vals[2].get_text().strip(\"Age:\").strip())\n",
        "                gender.append(p_vals[3].get_text().strip(\"Gender:\").strip())\n",
        "                race.append(p_vals[4].get_text().strip(\"Race:\").strip())\n",
        "                booking_date.append(p_vals[7].get_text().strip(\"Booking Date:\").strip())\n",
        "                charges.append(p_vals[8].get_text().strip(\"Charges:\").strip())\n",
        "                b = p_vals[9].get_text().strip(\"Bond:\").replace(\",\", \"\").strip()\n",
        "                b_able = False\n",
        "                if b == \"\":\n",
        "                    bond = 0.0\n",
        "                else:\n",
        "                    bond = float(b)\n",
        "                    b_able = True\n",
        "                    num_bondable += 1\n",
        "                    bondable_names.append(name)\n",
        "                bonds.append(bond)\n",
        "                bondable.append(b_able)\n",
        "\n",
        "            except IndexError:\n",
        "                num_errs += 1\n",
        "                print(\"Clay - IndexError\")\n",
        "\n",
        "    dic = {\"Name\": names, \"Booking Number\": book_Nos, \"Age\": age, \"Gender\": gender, \"Race\": race,\n",
        "           \"Booking Date\": booking_date, \"Charges\": charges, \"Bondable?\": bondable, \"Bond\": bonds}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Adams'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hinds Script"
      ],
      "metadata": {
        "id": "m4UNyZSA_gJ-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AZ-_t3epzdm0"
      },
      "outputs": [],
      "source": [
        "def hinds():\n",
        "    # print(\"here\")\n",
        "    # Iterate through all pages of the database to find links to the inmate pages\n",
        "    # Is there a way to do this that doesn't open the first page twice? Especially without rewriting a bunch of code outside the loop\n",
        "    url_link = 'http://www.co.hinds.ms.us/pgs/apps/inmate/inmate_list.asp?name_sch=&SS1=1&search_by_city=&search_by=&ScrollAction=Page+1'\n",
        "    Hinds_html = request_url(url_link, \"hinds\")\n",
        "    hinds_soup = soup(Hinds_html, \"html.parser\")\n",
        "\n",
        "    num_pages = int(hinds_soup.h3.get_text().split()[3])\n",
        "    hinds_prefix = \"http://www.co.hinds.ms.us/pgs/apps/inmate/inmate_list.asp?name_sch=&SS1=1&search_by_city=&search_by=&ScrollAction=Page+\"\n",
        "    inmate_prefix = \"http://www.co.hinds.ms.us/pgs/apps/inmate/\"\n",
        "    inmate_links = []\n",
        "\n",
        "    name, address, dob, sex, race, height, weight, eye_col, hair_col, arrest_agency = ([] for _ in range(10))\n",
        "    arrest_date, offense1, offense2, offense3, offense4, pin, location = ([] for _ in range(7))\n",
        "\n",
        "    # Find links to all inmate pages\n",
        "    for i in range(num_pages):\n",
        "        Hinds_html = request_url(hinds_prefix + str(i + 1), \"hinds\")\n",
        "        hinds_soup = soup(Hinds_html, \"html.parser\")\n",
        "        lin = hinds_soup.find_all(\"a\")\n",
        "\n",
        "        last_link = len(lin) - 6\n",
        "        page_links = lin[6:last_link]\n",
        "\n",
        "        for link in page_links:\n",
        "            inmate_links.append(inmate_prefix + link.get('href'))\n",
        "\n",
        "    increment = 0\n",
        "    for inmate_link in inmate_links:\n",
        "        if increment % 50 == 0: print(\"Hinds\", increment, len(inmate_links))\n",
        "        try:\n",
        "            Hinds_html = request_url(inmate_link, \"hinds\")\n",
        "            inmate_soup = soup(Hinds_html, \"html.parser\")\n",
        "\n",
        "            # Generate a list of all the classes which contain relevant information\n",
        "            left_txt = inmate_soup.select(\".normaltxtleft\")\n",
        "\n",
        "            # Generate information available, exclude those which do not seem to have anything listed\n",
        "            name.append(left_txt[0].get_text().strip())\n",
        "            address.append(\" \".join(left_txt[1].get_text().split()))\n",
        "            dob.append(left_txt[3].get_text().strip())\n",
        "            sex.append(left_txt[5].get_text().strip())\n",
        "            race.append(left_txt[7].get_text().strip())\n",
        "            height.append(left_txt[9].get_text().strip())\n",
        "            weight.append(left_txt[11].get_text().strip())\n",
        "            eye_col.append(left_txt[13].get_text().strip())\n",
        "            hair_col.append(left_txt[15].get_text().strip())\n",
        "            arrest_agency.append(left_txt[17].get_text().strip())\n",
        "            arrest_date.append(left_txt[19].get_text().strip())\n",
        "            offense1.append(left_txt[20].get_text().strip())\n",
        "            offense2.append(left_txt[27].get_text().strip())\n",
        "            offense3.append(left_txt[34].get_text().strip())\n",
        "            offense4.append(left_txt[41].get_text().strip())\n",
        "            pin.append(left_txt[49].get_text().strip())\n",
        "            location.append(left_txt[51].get_text().strip())\n",
        "            increment += 1\n",
        "\n",
        "        except IndexError:\n",
        "            print(\"Hinds - Index error at \", str(increment + 1), len(inmate_link))\n",
        "            increment += 1\n",
        "\n",
        "    dic = {\"Name\": name, \"Address\": address, \"DoB\": dob, \"Sex\": sex, \"Race\": race, \"Height\": height, \"Weight\": weight,\n",
        "           \"Eye color\": eye_col,\n",
        "           \"Hair color\": hair_col, \"Arresting agency\": arrest_agency, \"Arrest date\": arrest_date, \"Pin\": pin,\n",
        "           \"Location\": location,\n",
        "           \"First offense\": offense1, \"Second offense\": offense2, \"Third offense\": offense3, \"Fourth offense\": offense4}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Hinds'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jackson Script\n",
        "\n",
        "---\n",
        "\n",
        "Currently has an SSL error when attempting to connect to the website"
      ],
      "metadata": {
        "id": "z1PTD-q9_ig-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GrCjhOnazV0v"
      },
      "outputs": [],
      "source": [
        "def jackson():\n",
        "    # Regex formatting to calculate total bond\n",
        "    regex = re.compile('Bond:\\ \\$[0-9]*.[0-9]*')\n",
        "    regex_money = re.compile('\\$[0-9]*.[0-9]*')\n",
        "    regex_num = re.compile('[0-9]*.[0-9]*')\n",
        "\n",
        "    # Obtain the total count of inmates\n",
        "    count_url = \"https://services.co.jackson.ms.us/jaildocket/_inmateList.php?Function=count\"\n",
        "    count_html = request_url(count_url, 'jackson')\n",
        "    total_count = soup(count_html, \"html.parser\")\n",
        "    print(\"Jackson - Total Inmate Count:\", total_count)\n",
        "\n",
        "    # To state details of inmates\n",
        "    inmates = {}\n",
        "    inmate_ID_list = []\n",
        "    page = 0\n",
        "    y = []\n",
        "    # Iterate through the pages of inmates and get all the inmate IDs\n",
        "    while len(y) > 0 or page == 0:  # Increase the page count\n",
        "        page = page + 1\n",
        "        inmate_ID = \"https://services.co.jackson.ms.us/jaildocket/_inmateList.php?Function=list&Page=\" + str(page)\n",
        "        inmate_ID = read_url(inmate_ID)\n",
        "        y = json.loads(soup(inmate_ID, \"html.parser\").prettify())\n",
        "        for i in y:\n",
        "            inmate_ID_list.append(i['ID_Number'].strip())\n",
        "            for k in range(10):\n",
        "                del i[str(k)]\n",
        "            del i['RowNum']\n",
        "            del i['Name_Suffix']\n",
        "            inmates[i['ID_Number'].strip()] = i\n",
        "    print(\"Jackson - Total Count of ID Numbers Obtained:\", len(inmate_ID_list))\n",
        "    print(\"Jackson - # of Pages of inmates on website:\", page)\n",
        "\n",
        "    bond_count = 0\n",
        "    bondable_count = 0\n",
        "    # Iterate through inmate cards with the inmate IDs and store in inmates dict\n",
        "    for inmate_ID in inmate_ID_list:\n",
        "        try:\n",
        "            my_url = 'https://services.co.jackson.ms.us/jaildocket/inmate/_inmatedetails.php?id=' + inmate_ID\n",
        "\n",
        "            # Open connection, grepping the page\n",
        "            page_html = read_url(my_url)\n",
        "            page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "            # Obtain inmate details (race, height, ... whether they are bondable)\n",
        "            container = page_soup.select(\"[class~=iltext] p\")\n",
        "            name = []\n",
        "            bondable = \"No\"\n",
        "            for i in container:\n",
        "                item = ' '.join(i.string.split())\n",
        "                if item == 'Bondable':\n",
        "                    bondable_count = bondable_count + 1\n",
        "                    bondable = \"Yes\"\n",
        "                name.append(item)\n",
        "\n",
        "            # Obtain their offense charge and bond amount\n",
        "            container = page_soup.select(\"[class~=offenseItem] p\")\n",
        "            offense = []\n",
        "            for i in container:\n",
        "                item = ' '.join(i.string.split())\n",
        "                offense.append(item)\n",
        "\n",
        "            # Calculate the total bond amount for the inmate\n",
        "            total = 0\n",
        "            bonds = regex_money.findall(str(regex.findall(str(offense))))\n",
        "            for b in bonds:\n",
        "                total = total + Decimal(re.sub(r'[^\\d.]', '', b))\n",
        "\n",
        "            # Store all values in dictionary for the inmate\n",
        "            inmates[inmate_ID][\"Total Bond($)\"] = total\n",
        "            inmates[inmate_ID][\"Bondable?\"] = bondable\n",
        "            inmates[inmate_ID][\"inmate_info\"] = name\n",
        "            inmates[inmate_ID][\"inmate_offense\"] = offense\n",
        "\n",
        "            # Calculate the amount of inmates that are Bondable\n",
        "            if inmates[inmate_ID][\"Total Bond($)\"] > 0:  bond_count = bond_count + 1\n",
        "\n",
        "        except:\n",
        "            print(\"Jackson ERROR\", inmate_ID)\n",
        "\n",
        "    print(\"Jackson - # of inmates with bond:\", bond_count)\n",
        "    print(\"Jackson - # of inmates that are bondable:\", bondable_count)\n",
        "\n",
        "    # Store Values in CSV\n",
        "    csv_columns = list(list(inmates.values())[0].keys())\n",
        "    dict_data = list(inmates.values())\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dict_data)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Jackson'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jones Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ri4DV3W2_lyo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lIB-JkvrzOrR"
      },
      "outputs": [],
      "source": [
        "def jones():\n",
        "    # Initialize fields\n",
        "    Fname, Lname, bookNum, ages, gender, race, addresses, agency, book_date, charges, bonds, bondable = ([] for _ in range(12))\n",
        "    num_bondable = 0\n",
        "\n",
        "    u_jones = requests.get(\"https://www.jonesso.com/roster.php\")\n",
        "    jones_soup = soup(u_jones.text, \"html.parser\")\n",
        "    u_jones.close()\n",
        "\n",
        "    num_inmates = int(jones_soup.find_all(\"h2\")[0].get_text().replace(\"Inmate Roster (\", \"\").replace(\")\", \"\").strip())\n",
        "    num_pages = math.ceil(num_inmates / 20)\n",
        "    page_range = range(num_pages + 1)[1:]\n",
        "    inmate_links = []\n",
        "    page_prefix = \"https://www.jonesso.com/roster.php?&grp=\"\n",
        "    inmate_prefix = \"https://www.jonesso.com/\"\n",
        "\n",
        "    for page in page_range:\n",
        "        # print(page, page_range)\n",
        "        u_page = requests.get(page_prefix + str(page * 20))\n",
        "        page_soup = soup(u_page.text, \"html.parser\")\n",
        "        u_page.close()\n",
        "\n",
        "        a_tags = (page_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].find_all(\"a\"))\n",
        "        for a in a_tags:\n",
        "            inmate_links.append(a.get(\"href\"))\n",
        "\n",
        "    for inmate in inmate_links:\n",
        "        try:\n",
        "            cgs = []\n",
        "            u_inmate = requests.get(inmate_prefix + inmate)\n",
        "            inmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "            u_inmate.close()\n",
        "\n",
        "            name = inmate_soup.select(\".ptitles\")[0].get_text().split()\n",
        "            fname_var = name[0]\n",
        "            lname_var = name[1]\n",
        "            rows = inmate_soup.find(attrs={\"id\": \"cms-body-content\"}).select(\".row\")[0].select(\".row\")\n",
        "            booknum_var = rows[0].find_all(\"div\")[1].get_text()\n",
        "            age_var = rows[1].get_text().strip().strip(\"Age:\").strip()\n",
        "            gender_var = rows[2].get_text().strip().strip(\"Gender:\").strip()\n",
        "            race_var = rows[3].get_text().strip().strip(\"Race:\").strip()\n",
        "            addresses_var = rows[4].get_text().strip().strip(\"Address:\").strip()\n",
        "            agency_var = rows[5].get_text().strip(\"Arresting Agency:\").strip()\n",
        "            book_date_var = rows[6].get_text().replace(\"Booking Date:\", \"\").strip().split()[0]\n",
        "            off_str = str(rows[8]).strip(\"<div class=\\\"row\\\">\").strip().strip(\n",
        "                \"<div class=\\\"cell inmate_profile_data_content\\\"><span class=\\\"text2\\\">\")\n",
        "            off_str = off_str.replace(\"<br/></span></div>\\n</\", \"\").strip().split(\"<br/>\")\n",
        "            bond_var = rows[9].get_text().strip().strip(\"Bond:\").replace(\"$\", \"\").strip()\n",
        "            for off in off_str:\n",
        "                cgs.append(off.strip())\n",
        "\n",
        "            Fname.append(fname_var)\n",
        "            Lname.append(lname_var)\n",
        "            bookNum.append(booknum_var)\n",
        "            ages.append(age_var)\n",
        "            gender.append(gender_var)\n",
        "            race.append(race_var)\n",
        "            addresses.append(addresses_var)\n",
        "            agency.append(agency_var)\n",
        "            book_date.append(book_date_var)\n",
        "            charges.append(cgs)\n",
        "            bond = int(bond_var)\n",
        "            bonds.append(bond)\n",
        "            if bond != 0:\n",
        "                bondable.append(True)\n",
        "                num_bondable += 1\n",
        "            else:\n",
        "                bondable.append(False)\n",
        "        except:\n",
        "            print(\"Jones Error\", inmate)\n",
        "\n",
        "    # print(\"Jones - There are currently\", num_bondable, \"bondable detainees.\")\n",
        "\n",
        "    dic = {\"First name\": Fname, \"Last name\": Lname, \"Booking Number\": bookNum, \"Age\": ages, \"Gender\": gender,\n",
        "           \"Race\": race, \"Address\": addresses, \"Arrest agency\": agency, \"Book date\": book_date, \"Charges\": charges,\n",
        "           \"Bond\": bonds, \"Bondable?\": bondable}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Jones'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kemper Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "npyUSYg2_o-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "afzz9Al6y_jE"
      },
      "outputs": [],
      "source": [
        "def kemper():\n",
        "    first_names, middle_names, last_names, booking_nums, ages, gender, race, addresses, book_dates, charges = ([] for _ in range(10))\n",
        "\n",
        "    u_Kemper = requests.get(\"https://www.kempercountysheriff.com/roster.php?&grp=10\")\n",
        "    kemper_soup = soup(u_Kemper.text, \"html.parser\")\n",
        "    u_Kemper.close()\n",
        "\n",
        "    # Scrape the first page to get the number of inmates, compute the number of pages, then generate links\n",
        "    page_links = []\n",
        "    inmate_links = []\n",
        "    num_inmates = int(kemper_soup.h2.get_text().strip(\"Inmate Roster (\").strip(\")\"))\n",
        "    num_pages = math.ceil(num_inmates / 10)\n",
        "\n",
        "    for n in range(num_pages):\n",
        "        page_links.append(\"https://www.kempercountysheriff.com/roster.php?&grp=\" + str((n + 1) * 10))\n",
        "\n",
        "    # Scrape each page for inmate links\n",
        "    for page in page_links:\n",
        "        u_page = requests.get(page)\n",
        "        page_soup = soup(u_page.text, \"html.parser\")\n",
        "        u_page.close()\n",
        "\n",
        "        inmate_table = page_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].find_all(\"a\")\n",
        "        for inmate in inmate_table:\n",
        "            inmate_links.append(\"https://www.kempercountysheriff.com/\" + inmate.get(\"href\"))\n",
        "\n",
        "    for inmate in inmate_links:\n",
        "        try:\n",
        "            u_inmate = requests.get(inmate)\n",
        "            inmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "            u_inmate.close()\n",
        "\n",
        "            name = inmate_soup.select(\".ptitles\")[0].get_text().split()\n",
        "            fname_var = name[0]\n",
        "            lname_var = name[-1]\n",
        "            mname_var = name[1:-1]\n",
        "            rows = inmate_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].select(\".row\")\n",
        "            bookNum_var = rows[1].find_all(\"div\")[1].get_text()\n",
        "            age_var = rows[2].get_text().replace(\"Age:\", \"\").strip()\n",
        "            gen_var = rows[3].get_text().replace(\"Gender:\", \"\").strip()\n",
        "            race_var = rows[4].get_text().replace(\"Race:\", \"\").strip()\n",
        "\n",
        "            if len(rows) == 9:\n",
        "                book_var = rows[5].get_text().split()[2]\n",
        "                chargeOne_var = rows[7].get_text().strip()\n",
        "            else:\n",
        "                add_var = rows[5].get_text().replace(\"Address:\", \"\").strip()\n",
        "                bookdate_var = rows[6].get_text().split()[2]\n",
        "                charge_var = rows[8].get_text().strip()\n",
        "\n",
        "            first_names.append(fname_var)\n",
        "            last_names.append(lname_var)\n",
        "            middle_names.append(mname_var)\n",
        "\n",
        "            # Note that not all pages have the address listed, this messes up indexing\n",
        "            booking_nums.append(bookNum_var)\n",
        "            ages.append(age_var)\n",
        "            gender.append(gen_var)\n",
        "            race.append(race_var)\n",
        "            if len(rows) == 9:\n",
        "                addresses.append(\"\")\n",
        "                book_dates.append(book_var)\n",
        "                charges.append(chargeOne_var)\n",
        "            else:\n",
        "                addresses.append(add_var)\n",
        "                book_dates.append(bookdate_var)\n",
        "                charges.append(charge_var)\n",
        "\n",
        "        except:\n",
        "            print(\"Kemper Error\", inmate)\n",
        "\n",
        "    dic = {\"First name\": first_names, \"Middle name\": middle_names, \"Last name\": last_names,\n",
        "           \"Booking number\": booking_nums, \"Age\": ages, \"Gender\": gender, \"Race\": race,\n",
        "           \"Address\": addresses, \"Booking date\": book_dates, \"Charges\": charges}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Kemper'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Madison Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KAhtgAww_sM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vefx1gAay3OB"
      },
      "outputs": [],
      "source": [
        "def madison():\n",
        "    # Open connection and read page\n",
        "    Madison_html = request_url('http://mydcstraining.com/agencyinfo/MS/4360/inmate/ICURRENT.HTM', \"madison\")\n",
        "    mad_soup = soup(Madison_html, \"html.parser\")\n",
        "    prefix = 'http://mydcstraining.com/agencyinfo/MS/4360/inmate/ICUD'\n",
        "\n",
        "    # Determine the number of people detained\n",
        "    inmate_num = int(mad_soup.find_all('b')[4].get_text())\n",
        "\n",
        "    # Generate a list of links to intake reports\n",
        "    links = []\n",
        "    for i in range(inmate_num):\n",
        "        links.append(prefix + str(i + 1).rjust(4, '0') + '.HTM')\n",
        "\n",
        "    # Initialize lists\n",
        "    # Note that the index is offset by one from the list of inmates in the url (starts at 1 rather than zero)\n",
        "    names, ages, birthdays, arrest_dates, heights, weights, race, sex, hair_colors = ([] for _ in range(9))\n",
        "    eye_colors, facial_hair, complexions, off_dates, case_nums, intake_dates, intake_times = ([] for _ in range(7))\n",
        "    intake_nums, arrest_agencies, written_bonds, cash_bonds, bondable, bondable_names, offenses = ([] for _ in range(7))\n",
        "    num_bondable = 0\n",
        "    index = 0\n",
        "\n",
        "    for link in links:\n",
        "        try:\n",
        "            inmate_html = request_url(link, \"madison\")\n",
        "            inmate_soup = soup(inmate_html, \"html.parser\")\n",
        "            fieldsets = inmate_soup.find_all(\"fieldset\")\n",
        "\n",
        "            # Find names and split among respective lists according to length\n",
        "            # Note that this produces an array for the first name when there are three names, is this an issue?\n",
        "            name_dob_age = fieldsets[0].find_all(\"tr\")\n",
        "            name = name_dob_age[0].get_text()[9:]\n",
        "            names.append(name)\n",
        "\n",
        "            # Find age and birthday from same fieldset as name\n",
        "            dob_age = name_dob_age[3].find_all(\"td\")\n",
        "            birthdays.append(dob_age[0].get_text()[5:].strip())\n",
        "            ages.append(dob_age[1].get_text()[5:].strip())\n",
        "\n",
        "            # Scrape the \"description\" box\n",
        "            appearance_fields = fieldsets[1].find_all(\"tr\")\n",
        "            heights.append(appearance_fields[0].find_all(\"td\")[0].get_text()[5:].strip())\n",
        "            weights.append(appearance_fields[0].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "            race.append(appearance_fields[1].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "            sex.append(appearance_fields[1].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "            hair_colors.append(appearance_fields[2].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "            eye_colors.append(appearance_fields[2].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "            facial_hair.append(appearance_fields[3].find_all(\"td\")[0].get_text()[7:].strip())\n",
        "            complexions.append(appearance_fields[3].find_all(\"td\")[1].get_text()[7:].strip())\n",
        "\n",
        "            # Scrapes the \"intake/booking\" box\n",
        "            intake_fields = fieldsets[2].find_all(\"tr\")\n",
        "            off_date = intake_fields[0].get_text()[11:].split()[0]\n",
        "            if off_date == \"00/00/0000\":\n",
        "                off_date = \"\"\n",
        "            off_dates.append(off_date)\n",
        "            case_num = intake_fields[0].get_text()[11:].split()[3]\n",
        "            if case_num == \"OTHER\":\n",
        "                case_num = \"\"\n",
        "            case_nums.append(case_num)\n",
        "\n",
        "            intake_dates.append(intake_fields[1].get_text().split()[2])\n",
        "            intake_times.append(intake_fields[1].get_text().split()[4])\n",
        "            intake_nums.append(intake_fields[2].get_text()[11:].strip())\n",
        "            arrest_agencies.append(intake_fields[3].get_text()[19:].strip())\n",
        "\n",
        "            # Bond Information box\n",
        "            written_bond = (float(fieldsets[4].find_all(\"tr\")[0].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "            written_bonds.append(written_bond)\n",
        "            bondable_ = bool(written_bond)\n",
        "            bondable.append(bondable_)\n",
        "            if bondable_:\n",
        "                num_bondable += 1\n",
        "                bond_names = [name.strip(), index]\n",
        "                bondable_names.append(bond_names)\n",
        "            cash_bonds.append(\n",
        "                float(fieldsets[4].find_all(\"tr\")[1].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "\n",
        "            # Offense text box\n",
        "            offs = fieldsets[5].find_all(\"tr\")[2:]\n",
        "            desc = []\n",
        "            court = []\n",
        "            bond_amt_ = []\n",
        "            for o in offs:\n",
        "                off = o.find_all(\"td\")\n",
        "                desc.append(off[0].get_text())\n",
        "                court.append(off[1].get_text().strip() + \" COURT\")\n",
        "                bond_amt = off[2].get_text().strip().replace(\",\", \"\")\n",
        "                if bond_amt == \"\":\n",
        "                    bond_amt = \"0\"\n",
        "                bond_amt_.append(float(bond_amt.strip(\"$s\")))\n",
        "\n",
        "            off_dict = {\"Description\": desc, \"Court\": court, \"Bond amount\": bond_amt_}\n",
        "            offenses.append(off_dict)\n",
        "            index += 1\n",
        "        except:\n",
        "            print(\"Madison - Error:\", link)\n",
        "\n",
        "    # Generate a dataframe with dictionary dic, then prints dataframe as specified\n",
        "    dic = {\"Name\": names, \"Ages\": ages, \"Birthdays\": birthdays,\n",
        "           \"Height\": heights, \"Weight\": weights, \"Race\": race, \"Sex\": sex, \"Hair color\": hair_colors,\n",
        "           \"Eye color\": eye_colors, \"Facial hair\": facial_hair,\n",
        "           \"Complexion\": complexions,\n",
        "           \"Off date\": off_dates, \"Case number\": case_nums, \"Intake date\": intake_dates, \"Intake number\": intake_nums,\n",
        "           \"Arresting agency\": arrest_agencies,\n",
        "           \"Bondable?\": bondable, \"Written bond\": written_bonds, \"Cash bond\": cash_bonds,\n",
        "           \"Offense(s)\": offenses}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Madison'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tunica Script\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "b18R_Dvf_uvm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "myLQsez6ytvd"
      },
      "outputs": [],
      "source": [
        "def tunica():\n",
        "    first_names, middle_names, last_names, booking_nums, ages, gender, race = ([] for _ in range(7))\n",
        "    arrest_agency, book_date, charges, bonds, bondable, page_links, inmate_links = ([] for _ in range(7))\n",
        "    num_bondable = 0\n",
        "\n",
        "    u_Tunica = requests.get(\"https://www.tunicamssheriff.com/roster.php\")\n",
        "    tunica_soup = soup(u_Tunica.text, \"html.parser\")\n",
        "    u_Tunica.close()\n",
        "\n",
        "    page_prefix = \"https://www.tunicamssheriff.com/roster.php?&grp=\"\n",
        "    inmate_prefix = \"https://www.tunicamssheriff.com/\"\n",
        "    num_inmates = int(tunica_soup.select(\".ptitles\")[0].get_text()[15:-1])\n",
        "    num_pages = math.ceil(num_inmates / 10)\n",
        "    for p in range(num_pages):\n",
        "        page_links.append(page_prefix + str((p + 1) * 10))\n",
        "\n",
        "    for page in page_links:\n",
        "        u_page = requests.get(page)\n",
        "        page_soup = soup(u_page.text, \"html.parser\")\n",
        "        u_page.close()\n",
        "\n",
        "        links = page_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].select(\"a\")\n",
        "        for link in links:\n",
        "            inmate_links.append(inmate_prefix + link.get(\"href\"))\n",
        "\n",
        "    for inmate in inmate_links:\n",
        "        try:\n",
        "            u_inmate = requests.get(inmate)\n",
        "            inmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "            u_inmate.close()\n",
        "\n",
        "            name = (inmate_soup.select(\".ptitles\")[0].get_text().split())\n",
        "            rows = inmate_soup.find_all(attrs={\"id\": \"cms-body-content\"})[0].select(\".row\")[0].select(\".row\")\n",
        "\n",
        "            booking_num = int(rows[0].find_all(\"div\")[1].get_text())\n",
        "            age = int(rows[1].find_all(\"div\")[1].get_text())\n",
        "            gender_person = rows[2].find_all(\"div\")[1].get_text()\n",
        "            race_person = rows[3].find_all(\"div\")[1].get_text()\n",
        "            arrestAgency = rows[4].find_all(\"div\")[1].get_text()\n",
        "            bookDate = rows[5].find_all(\"div\")[1].get_text().split()[0]\n",
        "            offs_str = (\n",
        "                str(rows[7].select(\".text2\")[0]).replace(\"<span class=\\\"text2\\\">\", \"\").replace(\"</span>\", \"\").split(\n",
        "                    \"<br/>\"))\n",
        "\n",
        "            firstname = name[0]\n",
        "            lastname = name[-1]\n",
        "            middlename = name[1:-1]\n",
        "\n",
        "            first_names.append(firstname)\n",
        "            last_names.append(lastname)\n",
        "            middle_names.append(middlename)\n",
        "            booking_nums.append(booking_num)\n",
        "            ages.append(age)\n",
        "            gender.append(gender_person)\n",
        "            race.append(race_person)\n",
        "            arrest_agency.append(arrestAgency)\n",
        "            book_date.append(bookDate)\n",
        "\n",
        "            offs = []\n",
        "            for off in offs_str:\n",
        "                offs.append(off.strip())\n",
        "            charges.append(offs)\n",
        "            try:\n",
        "                bd = rows[8].get_text().replace(\"Bond:\", \"\").strip()\n",
        "                if \"$\" in bd:\n",
        "                    bonds.append(float(bd.strip(\"$\")))\n",
        "                    bondable.append(True)\n",
        "                    num_bondable += 1\n",
        "                else:\n",
        "                    bonds.append(0.0)\n",
        "                    bondable.append(False)\n",
        "            except IndexError:\n",
        "                bonds.append(0.0)\n",
        "                bondable.append(False)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    dic = {\"First name\": first_names, \"Middle name\": middle_names, \"Last name\": last_names,\n",
        "           \"Booking number\": booking_nums, \"Age\": ages, \"Gender\": gender, \"Race\": race,\n",
        "           \"Arrest agency\": arrest_agency, \"Booking date\": book_date, \"Charges\": charges,\n",
        "           \"Bond\": bonds, \"Bondable?\": bondable}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv(get_filename(date=today_date, town_name='Tunica'), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape Data\n",
        "\n",
        "---\n",
        "\n",
        "Note that captchas are case-sensitive\n"
      ],
      "metadata": {
        "id": "FYZ94qrO_xiB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Y_EaNPdCI9U",
        "outputId": "f4eb77df-4807-49fa-da89-df01cfa681ae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAM6SURBVGhD7ZnPSxtBFMdX6NX2Dyh4qaT3HpuCPQiNSI7eTZOTkounUqwgEhpS8SS5FFwkgYCePKqJBPonSP6e18z+nHn73WSS7G422T18Dr5942E+vnnzRmM0eqWc9JALSRm5kJSRC0kZuZCUkQtJGZkW8vv9Lwv0bVkkIuTv5y8KKCdpXBmCbx/fKpx13bwnatcvaMjWxknsQriMMNDaeDAVGaUy3/CxhPKmJyf4PV6WcmQhIRy0blFkEQK/EoIMWzvZEYJAUjhonS6yCLHRtdYTzJO5r29mVwgHCeGgdQguo1So0j3ICzC4oJqukLtD+rS7MYFD6mnkp1YIAknhoHWjbtWTYQmpmzgvwBxNHWz0yR3Ic3i5Ljp5RboaprhCdEBCEOLocWUIdI6r+bmhk11DEsIqQ+GRrioGHVw/erGVFoJAQjjpECLy7KqQ42snJIgJpXDw2nnQEDI8p4MQUZkQclZQjywkhIN/lw5ThIgeUzmnFzkmkQEh6qAnQEcWksLhazDhQnqn4/jpDctXyYAQf8hz0bllISGI4FokxG7ebmzSrSsTQqx5Qjq2tOcQBhLCCQpB4P4hWLKQDtW2PtC7rW2HY7qFeYvDq0TvpmVSe0oeksLpIUkhfcQS8tz6QRuF7wpG2aRnlhwpnWNHwj41+iLmynF/jh55HikVdqg9wHkWTlVNeu9CICEcTwroJ0ZRbH7hkkw32L30pcjxSJEq46jjxW+PnNhek/pKfnToVIr1hjVNWCjBHjJJCu8nhrf59Qc7ODBJSHLjlRn/QnToN/ed6timr80+iMdXJR7dqiJGZtaqUAm/ZQmsm5b3TaAOh56QYuufE3ygiiTEj0dFnxp7TiWEClHjq8VkIe5ziSrFzwFNPW4hvJFj1lcIyhnjNHlJiCoiKSGru/EhaL72+q+8MkVbiFm3N9/uF8keWXE28ETR/n8IOrJ8DHfzvaYeuxDpNmUBGni/SY0Oi2WEJTT1Md4M4iIPhOMjbV2qZg78a68zCPIhMRYhY9QqkUngyptiPCG+ADPQ3P3jLGJ4pcxQGX/e/LRA31YZa1K3Nj1sWo9LxoKsrRAUXAVyISkjF5KTAK/0H+Z0jkEMD8yJAAAAAElFTkSuQmCC\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "aeCK\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAANPSURBVGhD7ZkxaxsxFID7Bzqkc4fSuZCxLSmYDp2yNWQxeLFJh0BoFg8ZDI3BkwcHnHgoxGDiQNOEZkiGQLIE/Afyi14t30l60j3ppMudLTs3fMPpPWm4z09PJ796enwAzNt3bYEeWwWq61/J8VBQhGAZOjhvWWEylkoIhpLCofKXhdClGIVgKCkcKj9kVkKIDiWGQeWGSMhSMgnBUGIYVG4ocCEhSnm2EAwlhkHlLpoXIQRDieFQ+fMm1CopTAgmRCGMFyskZEKTsjgh/46h8vkHvP6Uge1juKPWzAAXEooUIeSg8iUBTsydWEilc62MD/fUl1//jeY8XkN72yLk5hCOah+hjTjqXyTzNIITQskwoS+QmamQuiaDYRfCGEHdWiEDGGYUEoIU65ZFCTFBzc9CupA0/IUwAhbSg5+NN7DeWIupwhjFKRkm1HXdWJQQRghSVCHjaixhA7q3bIzL4c9mKCEU1FxMKUQ8oMro9ETCuBOPNZtwL3LdoISY4HMKEXK2K57TJC1aihByP9iIq2MNvg/ORYIcT68SFygZJvIQMqM9iGJYDB/TMAo52YFf7z84c3qizXckFnIO3absG7QQdTxvKCEmqPkS25Z1AZf7MjY8w/McSYjZESe+SWtTjX1rwQTPdSAWojdymiKFcPCWRckwIdew95C7toyZqsROH07xS0dCGLqUXusPmpsOKWQeL96ESw+hhFBQQib9LSlk/9D7F5wm5OmqBT1bPAVyy8rSwPMia1OnhJgQQmq7Xi8rIkVIatyOaOriNDWDaOC3TeiOtbECyPOURckwQa9F4Vkhnn1ECJHfIBz8QTjd0uZUNYUce1Gc9RBKiAk8N0ITor1wvYf4nrakkClqlWDyOfK6MA8hPIZPWZQMEyYhqoxN+Hsl13dFETJDr5S59pMR1LUref02OB1ViHqSQjHHhk4JMSFkNPrkWi4khSyEpIgEeyNiHoUmpLYFlzdRTJ6w5Jg/fVKGCd8qCURInkRCou1I/RCckenbA2PuIZQQiuSakhUUUjT2ps64a8g4JYSB8zGlEG/8hMzw6CmlEG/8hfhcn5RCvEkRkrg68Tv+lkJ8sdz2JmU888OwxILn/yG+t7ycUkhglEKC4gH+A0X93VICxU+ZAAAAAElFTkSuQmCC\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eTbB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAMASURBVGhD7Zkxa9tAGIYLhf6CLvkJ6RK6dOrSpRQcPBZCoMRLlzRDhjaDwSUQQsEdPTnBk8DQ1RBIEdT5BSG0NCQE8gM8Zcr4VSdLyvnu1Z0lnc7X+IZnsPWdDPf4u/dOenJ/NyGPOxgR8vVZE37vKU5lIUyGF2IOL8QxvBDH8EIcw1ioeylmWGIhp/T9zWt6x3N4CursspxCfu0nElr042pCt0Erk/Ip+I3HWGIJhfCdIQqZfsbj7LB8QrLucEOAiBEhjDJSws4KrbzMo0FHl8KYcFeu64xma3R4IRoue9QsOMmpyL0QX8eAEE/5OKBbOKY64+4avdrp0w24hli8kIjrQWNWyGaPrkFdChPSHJzDa1qsdsgJHTQiIY0NCub8HSeE3N+NaI8XEpH772cdpRGmxKKQm+FGJIMJieiewBqROYSE9Hm1Qc9fcLRDqa6akAgpH3YplOrO6Wiz6FIlYE3IBQU7iYyYNo1h3SxqIT+/TQWsblP/b7S0HG9nUtaP/0j11aRMJ5uXIi1LTFrREBexJeSszcmYsjW8wLUcCiFcZ4hCks/imHq7hAkDO6+iWBIy7ka5MRSkzBHu+ULS7lAIEKksRNElLPhLBzmPDSFXfdqKJ19cttbo4AzUc2RCnr79ArErJAJ2CQt9lCklsCCEhXk68b0PrZhMiibcYyFIhA7xRgwjQiLQgbFSkPPULoRtdduZCEmIZgusXbKQDBVGpEiHRUPdwahZiCiCfRcfDjMh6nAvnSFIhgp+rB7xXPJ/CEEyYlimcEJUW2Cjoc5AMlSge9QjpL5HJ6IIvhvyyAt340LyliwkQ0ctHWIQUQRbmvImeubUzsjZAlsTkgcSoQPdZ1GkMh62urju4blWCg5340IYlUM92foiGSrgvSzBukN3Epe6BGyBaxFSFvX7kYTOCMpQgX7LJNlE6x4gzvE4xSkhVUEyUlB9deSTeIwoRtplCXBL3aMS8hjwQhwDCAHvP1LeB+VfDHnmIr9DPAvBC3EML8QxvBCnmNA/qkqze+5iQVAAAAAASUVORK5CYII=\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fYfA\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAM9SURBVGhD7dk9SxxBGMBxSUi6QD5BmpALpkyRyiZwhYXWgVQRC0kQvC5NKg9PgpBGLs2hHhwiabwqsATFQxHSaXGFhZ3f4don+5Cbu3l5ZnZmZ293jt3iX3gzu8L9mBl3XRjdDKEqnIIBefnxB/l52QoCBDFY1HiZCmqFVCABgWAVSqAgZUYJCgQrO0pwIFiZUYIEwSqQPDrtwaP1XWON0+n8MqIUs0IkmIX1Nvy8UOcxkBBRHnaa8O1Fw6E23BL3kStoyzqDhgDSg4icFw7Kq2dvJ/GfKzAbv4Xx2w1uLG5/51oYlwseBCsShYegQEbRMexzX7gMgskovw7Fcb65AMHyBkmEYFmAjA7bAgg5Z9zcgGBGlG4L3r3+4FyzK97HGoJVgSSsFBmm1oKBPCfqwKeaCOKEwJcCxHSOzBUIlozSh+b4y9aCcPNSQ7CczxDzX1sKyP3uEtTfPLZr80i41r70IJgZxQ7EG4KVACJgLB/DAzdGpV0hCszqNtxPxo/g++ITzZhNfiBYGhAZAetJZ4hzMgiVBQRLv2VF2/DZ+KVLKItrcCWMm/IHwWgU0wrpTyDEazwiV8g1nC9zn8UlPX+wPEDiumvTccx6C3MHifbUOQxERDGA4IG/0nFczQnptixl5TThPJKuJfIDufkDJ6v8KlmCE4tf6g4Sz//ahztiTEXRgYw/zwskTnmK9zlD7ECGcLXJzcGsVokbyN1BGxb2zsgxTESRQORyBEmzdXmDmA9/XQ4gF32o4xwDCKYFKXCFkOMJW5c3iHKO2Bzulm97GQbOqR/8VcelGAoNEpfnGcLlsnXlC2Lx/xBdLiAYCYKrJGMQ9TU89eCnbl06lGJWiKGnz98rUfN0mUGGMGhJIFEHmq3L6c+WJf8/RIJRtq7/yW9+CzpD6CgMU9Q99GcI3eCL+oKxyLIHSf06RR+FYcoaBM+U2hb0rP5UzydPkLTPIdlEYZiaXnsJvRW7FZR3fiDynBmsDuu41+4Uhqm0W+ws8gIRHgo9D/M8ojBMUfeYdVoQ5WwQvnBpqypyZWQQhWGKukdWKSAKhKk5h0iKwtBFXZ8m/ZZVpY0C0UVdb6oCySgKg0XNpxvCP8jtlUKVkdHoAAAAAElFTkSuQmCC\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DLER\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAANmSURBVGhD7do/a9tAGMfxju6QQluvfRWmtCWLO9XQKUNInaEBd0o9pBRCh1CDhy5twGRpSAmGLDF4yRJCSVLICzB+S1c/sk9+9Oh30p3+OFKr4TtYurPBn5xOtvNg+meiqopTBeLYj7cf/dD5tJUW5HPtmRc6l3UcgYfGpq2UIBqDh8alCQFQaGyWlXaFvHjZ8coSBgFQaGxelQ5EQ+j0cQnjgoMQKDQ270oDYoKQ2cIgAAqNXWWlALHF4CEYBECh+W6dqS9rz9XrR6ztMzAuvkKDJIGQ0RuOYNDYRA0+zQHWNtVgNFHX+5s+yvv933hORIUEyQpCJmEof85wR71pPwzVau+oc/a8wdjKkCCLx3ieucKBpMFACJR8k6NgbgbrC4h1dXQRfo1AenWkAJAVBiQpBAKglmNO1EG75mPwv3gEc9Stqc7gks2P6F8BueofqqeNnh+H4Mep+m74zUEAlBw3zwzCkzDyfDCwietefVfXcI5dC5Cx6mxtq8fvlj35Og4NzjoOQfE3/3Q3CIMAdPw5w1mAXPRUZ3FcwkTi5LJCht/mAFt7qj+7Zl4N9nyU5uAWTkqbhMCr4U71N8wQ/PmiiwGZbeatbk/d8GOzrGDyANErQ4Lox2hSmjjEdDxSTbYKNAgCoNDlKz4zyHmvplq9EzE+WCRMHiB6NeQFoOMQHgYdZyAIgNJYHMwtBHLpbd762MEQzQsXgikjCIRYhAAofV5u/s3+XWC+XUEQlGmjNyVhqMxBfJjusboCA5PkCoH2EH91NA5Vf7x8DvvQCgkjoX0kLgSDxrmUywrhEBoDAVBoD6HkXdaHn+HXscuwh3h3VgIlZj8JtbhkZQmTKYiEQACUP0ds6qhklymeAYQCX5fY7ideYA9JC5MZiDUCz3CXlW0RILPoTouDWH1loovY1CWMLU5qkEQQugKATMUdFx5jyOIuyxUmMUhiBF4hQMJjvHGRm7z7Vye2MM4gCIBCY2NbBYjlPqG/5eU5Xb4cioKxAkEAFBrrVJ4ght83dMuVEr5kyZLcEtskYSjrFZIphG4ll6zi5wSiN22dPJ8m+Um8vjHK7ENpWTOCrBJCVm/8Uqdg3v+QD+LDdI9zg6iKL/CLIYeoMO4nD6SCKE7+CqkgitG9/JNDlbkKpGBVIIVqov4CUqPGkwHm2i8AAAAASUVORK5CYII=\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fPKf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAMxSURBVGhD7Zm/T9tAFMer/gdlqNRfUqcOVf8DxsKIxNIFlmTN1iFUKiU7TMhLgoSUJVLCAIIOqEgoC0PUia0DUgT8DZZYX312zj6fv2ffxY5tHA+f6d7Zyvvk3rs7vxg/2VRTHmohJaMWosHrT++1QfNNWFohKJlZgN5lQqWEoARlAXrXoii9EJSgLEDvKgOFCEEJygL0rudG6YWg+VWm0JL1+evLyoJ+rw61EInv716FQDE6oN+rQ+mbeh7IEkRQ/CJZWiEo+RwUnxdLIwQlnoPii6LSQlDyOSi+DBQj5GZIH9d/SQzJQrGGoORzUHzZyF2IdQAECIKaN+H4JFDiOSg+Cy5+b1Cr8TaGDeo+2DTovQFjIm0aSM+WhExpsNWg/S8q9mhwJ8bbdNoBcVvndCnE+AiJXxs9hsagKAUo+RwUvyiiYqIJZkTEdHp0AeIYeIWMu1KSoyJkfDGdCRwfPz3SbosnPSrkZGTNxizanYrzPFDyOXKsy2SHVr59iGV7AuaZ8NCjnw0h2apE/22HhfT+RGNmKEuW/M+3+lMYx3HjlTIYYSHySkBCUPI5wXMTiIjZpB/3IG4e8hQyvjsnSxCiLEMuEzrWWEVBWZrRuqYTd8yThRLPQc/T44q2Q0J26BDGzUGuQkA/OR6jOAdW4mJXx4zpNa2JQhxQ8jnwGcZURoiD3Etg0pm45NXh4zR2lHwOnJOKKglxS5EgBJUlVtpiy1l8LxBXS1DCsqRSQmy67O8JQqLNnTVzZSkTECV4ZWtITbmnMA5u4fz5yVGILmmExDd3ZwUlrI4Q6AAITu2mh8N4KrZCGPIWmK8ItnqStsMB4rY3fNYItrwe8hklHRUUgps7a+ZdOkXxiNAOSz6N31KzFuKiJwQ1977mVtdHTHq8kLpkaSA3d53rlDDqkhUvKy3mQg4tTWlFCjE7uSsQy5awxQ1O8PgeKx2mQpz49hGN4JhEoUIcxOaus9VVEblCYWS+3eWYCRmdbdKKdQXHZPK77VXhN3eDZl44BkLuj2iVxSQIyfF7SBLe/Zb+VrcE6N72chkOq2f/ouM5YSjkGaHxPURFLaTGpxZSKmz6D2TyoGLiTRxlAAAAAElFTkSuQmCC\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V8LH\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAPWSURBVGhD7ZkxSxxBFMfzGWyCH8HkA6QKhAQbIZeY9CKeioiWKkQxxSF4WFnpWYggghErrQ4bC6sr/QCCn8LyuW93Z2fmvTezs3drbk+3+CHcvr075nfv/2bWd0/3XaipDiMr5P3PTzHStVGmFlIxaiEVoxZSMWohFWMkhbxWGUgtpGLUQipGLaRi1EIqxn8T8vfrHwupJpRaSB9QARTpnhBeswzEEHIBVyvTMPNb5uASa47gQLiGSItuEte1juLP+vxr2kJ/h3zekJCEXosu9gb0SM3T5Qap4UJ0fSIxEcpluND327w5IbwLFuDqltZwcVuHF6wmAd9PkGogCaGo2jcopAuPhwvWYquosbhtw5ZZ41r0qJvcsmQkIRLs3uN1mBz7EczeMbl/UG42YXviI8x+SJmYgusboc6DPNSDFpvPHBVLdg3vsIedOfg2/t3N5C48kFpJCCX7DCZmHe7U+7Wa9rUv+rMGobeaSmhswmP82j50lJzVfVbvQhYSQSNJXmy7hnUSil1pp1+Q0N2FJSpifA7Ou0LtfQfa0XUaV5IUChWCUCnLrWIdzDhtZl2x3T7LXn9sT4mv+3AKYYObLmy82AveTsLo88aVIGVpR6q/gPNJLoQiCZHAz10eaxhSbGHFOIPrRtoJEZ1T45ohanai6Z2jCreQnOGuFtvdSXJcUaT4ap/wGnwtT4imA3vpgktCKIMJMaKJCvFek/EI4cNd/9qNnRPtJBVbvriySH79tpQ1nfmRDNU1/QihHSAJkeDv6YAMcjuabCEhseUVwoa7WmCUkM0LuZNy48pEmieLHXg6WUv+pnV9CyGDm84QSQjFfn8T36KXLUQY3BhJvRaPL7Nm6/Aous9/9qDIOy/dKeEyELcQW0YT/ombiGJno2yHhWS7LKR0IXyxZ3CQSwPerEGks0sOd4svJERiXndeCJIQilp4PStKniEJNJKkU3nImSSEZHtrSUkjq6wO4bLcXZKHTwoj8JAYIISeSeQosjupWFyZuHZd5QkRrg8gxYd5DrGjzE2QEGsn5YoiM7b6iKuYdIhL0VWukCge500hJRwOGZ7ziYcwIUYkuaMopMYD7rSyRyZ2dBWTgRQXUnSm5NFPdyCBQtJIyjlXhNTIoAA9wGOwW4gQemB0M+QO6eOErggW8nLgwVB+hqWiSwkxd11+coSwRyflzRCrMwrKQIYsJDmly8+vEPuhYtwxxkHRiedpL5dRzmN4S0REyJlDYohC9JxwC0m6xBLik1Lw/yGlxNQA8SQxBCHSs6sEW4wWxoTEhMbXaFGBGVJjUgupFF14BnJL6L/o4GqmAAAAAElFTkSuQmCC\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WXB9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAGQAAAAkCAYAAAB/up84AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAP7SURBVGhD7ZitTyNBFMBPg7zL+fOIO3UWkiaH4c4QDBTBGUCQJsDhEGwwRREaBAlgigBTEoqvaUKCQaH7n7zbt9vZffPmzexHl7K0K34J7QwlvF/f134a9LtQUR4qISWjElIyKiEloxJSMiohJeNDCPn8bSZAOps0TCFXDfg3/yszl1fscwpCyaBI9yYFe4YYYhrQ43cePGjOL76pEMo0iHGUrBZcDoNtFULuZRJyvQffa380ftT2oCPdFeBiJklOAUK68NKspxbS2f9tCiCCDq71+y4mUUxuIb2mBy/qtV+6OmmEkMCvnNxoZ6KolEySmJxC/LM1IiQVN9BaD4MuCXk62RgK2YDWA/29bHx0MfmEYMPPKeTrxhcxE4oSMui3obm9aIhBZpYXYbONd1qw6f+MryVmlxtwSz6zf1rXzw9b0ZnE81YNrpaO4FU4SyKHkOH7mYWEZQmFRFLWj+EpOIuzJ35vNG4Pw+BJYqJ77YYWaCQUJhFKtJ8rzuBxzhcytwq9e+ncTQYhjBxCBg/HmpAoI07C3pKnf9jRs8AmRolTLJy22efQz9MzR+LVW/VloBCfrTPxjouxZgjChURi9s/F+6NgKzVczAK9Ywu6n012WYo29JaGMgJ24Fm8Z2eMPSSWMeifw0EtbvCRlILKVUTXcwabi0HwnlmWsC/Vodnl7zMudoiMkDsvSaJOPiF4NoKQFV8GlqeD4airSSk4U3hJkoItidHuoNhtD/r0PYHnLb9veExKxuaeUwjbQxB/F7lsur8NSggGPloAha09y3KI/P25FmGcs8Y9ywMbBLseZJJNDJa+xHJ1fwR3QfB52arB44Vw30JuIZzervvxiZKB8NFWjbwKvqO4oDIk8P+gzX12WS89Ktg8k7iYpHKFzVwFXmvsSIbmXoyQ4EFkHTqO/YFmhzlN6T0lixCOJIWCwY6/7WRy4plkGQAQ+vdCcNSlDVyNvor0I7BdiPG0Vwp4GzprSpo7g7IIyVqyXEhSKPGSJ2eSyiCnGL+Z8+YdLIdEStrmbgoxRKQkocnrQvg2HgspdhdRhNs7BlqSQlFCkIXTlv97yZNZb0nIAOwpREjaEdhRsopDycDFECesIPBkxI0fLI762MSOsZNgI/cbvCSF4npMwsXQjJBI09zHK2T4WgmgvMViqKOXJIRPTpKQpEclWJq4GATPjOaeYgR+FyHvBZ2kbBs5zSTr1q6IRt3wtSQma3N/cyFlkRFAJilrKSLbvatcIZgdUrOWxERSEkbg6RJCmru9FKW5Q8pRQoAlMa6Ja8qEhCXJ2NYZ7jvmJi6KYVMWF4NI/WTqhLw3VIh0PpamXpGeSkjJqISUjEpIyaiElIou/AfGC0X/de059gAAAABJRU5ErkJggg==\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E8WA\n",
            "StartingThreadID: DeSoto\n",
            "County: DeSoto\n",
            " 1 TownName: DeSoto\n",
            "StartingThreadID:  2 TownName: Forrest\n",
            "StartingForrest\n",
            "County: Forrest\n",
            "ThreadID: 3 TownName: Hancock\n",
            " Hancock\n",
            "County: Hancock\n",
            "Starting Harrison\n",
            "County: Harrison\n",
            "ThreadID: 4 TownName: Harrison\n",
            "Starting ThreadID: 5 TownName: Lamar\n",
            "Lamar\n",
            "County: Lamar\n",
            "Starting Marion\n",
            "County: Marion\n",
            "ThreadID: 6 TownName: Marion\n",
            "StartingThreadID: 7 TownName: Perry\n",
            " Perry\n",
            "County: Perry\n",
            "Starting Yazoo\n",
            "County: Yazoo\n",
            "ThreadID: 8 TownName: Yazoo\n",
            "Starting ThreadID: 9 TownName: PearlRiver\n",
            "PearlRiver\n",
            "StartingThreadID: 10 TownName: clay\n",
            "Starting adams\n",
            "ThreadID: 11  clay\n",
            "TownName: adams\n",
            "StartingThreadID: 12 TownName: hinds\n",
            " hinds\n",
            "StartingThreadID: 13 TownName: jackson\n",
            "StartingThreadID: 14 TownName: jones\n",
            " jones\n",
            " jackson\n",
            "Starting kemper\n",
            "ThreadID: 15 TownName: kemper\n",
            "Starting madison\n",
            "ThreadID: 16 TownName: madison\n",
            "Starting ThreadID: 17 TownName: tunica\n",
            "tunica\n",
            "Jackson - Total Inmate Count: 481\n",
            "Perry No. of offenders: 22\n",
            "Lamar No. of offenders: 84\n",
            "Yazoo No. of offenders: 257\n",
            "Hancock No. of offenders: 205\n",
            "Marion No. of offenders: 253\n",
            "Forrest No. of offenders: 258\n",
            "DeSoto No. of offenders: 455\n",
            "Harrison No. of offenders: 650\n",
            "Jackson - Total Count of ID Numbers Obtained: 481\n",
            "Jackson - # of Pages of inmates on website: 34\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting tunica\n",
            "\n",
            "Perry Saved CLEAN 02-21-2022_Perry_inmates_.csv\n",
            "Perry Saved RAW 02-21-2022_Perry_inmates_.csv\n",
            "Perry saved DAILY SUMMARY for Perry\n",
            "Exiting Perry\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting adams\n",
            "\n",
            "Hinds 0 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting clay\n",
            "\n",
            "Hinds 50 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting madison\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 100 651\n",
            "Lamar Saved CLEAN 02-21-2022_Lamar_inmates_.csv\n",
            "Lamar Saved RAW 02-21-2022_Lamar_inmates_.csv\n",
            "Lamar saved DAILY SUMMARY for Lamar\n",
            "Exiting Lamar\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 150 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Jackson - # of inmates with bond: 229\n",
            "Jackson - # of inmates that are bondable: 116\n",
            "Exiting jackson\n",
            "\n",
            "Hinds 200 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting jones\n",
            "\n",
            "Hinds 250 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 300 651\n",
            "Hancock Saved CLEAN 02-21-2022_Hancock_inmates_.csv\n",
            "Hancock Saved RAW 02-21-2022_Hancock_inmates_.csv\n",
            "Hancock saved DAILY SUMMARY for Hancock\n",
            "Exiting Hancock\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Fieldsets empty for page: https://www.pearlrivercounty.net/sheriff/files/ICUD0047.HTM\n",
            "Exiting kemper\n",
            "\n",
            "Hinds 350 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Marion Saved CLEAN 02-21-2022_Marion_inmates_.csv\n",
            "Marion Saved RAW 02-21-2022_Marion_inmates_.csv\n",
            "Marion saved DAILY SUMMARY for Marion\n",
            "Exiting Marion\n",
            "\n",
            "Hinds 400 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Yazoo Saved CLEAN 02-21-2022_Yazoo_inmates_.csv\n",
            "Forrest Saved CLEAN 02-21-2022_Forrest_inmates_.csv\n",
            "Yazoo Saved RAW 02-21-2022_Yazoo_inmates_.csv\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Forrest Saved RAW 02-21-2022_Forrest_inmates_.csv\n",
            "Yazoo saved DAILY SUMMARY for Yazoo\n",
            "Exiting Yazoo\n",
            "\n",
            "Forrest saved DAILY SUMMARY for Forrest\n",
            "Exiting Forrest\n",
            "\n",
            "Hinds 450 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 500 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 550 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 600 651\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Hinds 650 651\n",
            "Exiting hinds\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "DeSoto Saved CLEAN 02-21-2022_DeSoto_inmates_.csv\n",
            "DeSoto Saved RAW 02-21-2022_DeSoto_inmates_.csv\n",
            "DeSoto saved DAILY SUMMARY for DeSoto\n",
            "Exiting DeSoto\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Harrison Saved CLEAN 02-21-2022_Harrison_inmates_.csv\n",
            "Harrison Saved RAW 02-21-2022_Harrison_inmates_.csv\n",
            "Harrison saved DAILY SUMMARY for Harrison\n",
            "Exiting Harrison\n",
            "\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Fieldsets empty for page: https://www.pearlrivercounty.net/sheriff/files/ICUD0171.HTM\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exception: pearlRiver <urlopen error timed out>\n",
            "Exiting PearlRiver\n",
            "\n",
            "Exiting main thread\n"
          ]
        }
      ],
      "source": [
        "towns = list(jail_captcha.keys())\n",
        "\n",
        "class WebScraperThread(threading.Thread):\n",
        "    def __init__(self, thread_id, town_name, validate_r):\n",
        "        threading.Thread.__init__(self)\n",
        "        self.thread_id = thread_id\n",
        "        self.town_name = town_name\n",
        "        self.validate_r = validate_r\n",
        "\n",
        "    def run(self):\n",
        "        print(\"Starting\", self.town_name)\n",
        "\n",
        "        if self.town_name in towns:\n",
        "            town_scraper(self.town_name, self.validate_r)\n",
        "        elif self.town_name == 'PearlRiver':\n",
        "            pearlRiver()\n",
        "        elif self.town_name == 'clay':\n",
        "            clay()\n",
        "        elif self.town_name == 'adams':\n",
        "            adams()\n",
        "        elif self.town_name == 'hinds':\n",
        "            hinds()\n",
        "        elif self.town_name == 'jackson':\n",
        "            jackson()\n",
        "        elif self.town_name == 'jones':\n",
        "            jones()\n",
        "        elif self.town_name == 'kemper':\n",
        "            kemper()\n",
        "        elif self.town_name == 'madison':\n",
        "            madison()\n",
        "        elif self.town_name == 'tunica':\n",
        "            tunica()\n",
        "        else:\n",
        "            raise Exception(\"Town not found:\", self.town_name)\n",
        "\n",
        "        print(\"Exiting\", self.town_name, end='\\n\\n')\n",
        "\n",
        "threads = []\n",
        "thread_id = 1\n",
        "\n",
        "# Create new threads for JailTracker jails\n",
        "for town_name in towns:\n",
        "    # Get captcha image and enter in captcha information to get validation key\n",
        "    captcha_matched = False\n",
        "    while not captcha_matched:\n",
        "        captcha_r = requests.get('https://omsweb.public-safety-cloud.com/jtclientweb/captcha/getnewcaptchaclient')\n",
        "        captchaKey = captcha_r.json()['captchaKey']\n",
        "        image = captcha_r.json()['captchaImage']\n",
        "        html = f'<img src=\"{image}\"/>'\n",
        "        display.display(display.HTML(html))\n",
        "        user_code = input()\n",
        "\n",
        "        jail_captcha[town_name]['user_code'] = str(user_code)\n",
        "        validate_r = requests.post( \n",
        "            'https://omsweb.public-safety-cloud.com/jtclientweb/Captcha/validatecaptcha',\n",
        "            json={'userCode': jail_captcha[town_name]['user_code'] , 'captchaKey': captchaKey}\n",
        "        )\n",
        "        captcha_matched = validate_r.json()['captchaMatched']\n",
        "\n",
        "        if not captcha_matched:\n",
        "            print(\"Incorrect captcha\")\n",
        "\n",
        "    thread = WebScraperThread(thread_id, town_name, validate_r)\n",
        "    threads.append(thread)\n",
        "    thread_id += 1\n",
        "\n",
        "# Create new threads for remaining 9 jails\n",
        "for town_name in ['PearlRiver', 'clay', 'adams', 'hinds', 'jackson', 'jones', 'kemper', 'madison', 'tunica']:\n",
        "    thread = WebScraperThread(thread_id, town_name, 'None')\n",
        "    threads.append(thread)\n",
        "    thread_id += 1\n",
        "\n",
        "for t in threads:\n",
        "    t.start()\n",
        "\n",
        "# Wait for all threads to complete\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "print(\"Exiting main thread\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "JailWebScraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}