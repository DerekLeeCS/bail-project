{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clean_data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import ast\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "from datetime import datetime\n",
        "from pytz import timezone"
      ],
      "metadata": {
        "id": "bkMKEM0VXs-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mdlv6fdjXIOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/drive/MyDrive/'):\n",
        "    raise Exception(\"ERROR: Mount Google Drive before continuing!\")\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/Data Science for Social Good - Spring 2022/data/'\n",
        "SCRAPE_DIR = BASE_DIR + 'scraped_files/'\n",
        "DATA_DIR = SCRAPE_DIR + 'DATA/'\n",
        "CLEAN_DIR = SCRAPE_DIR + 'CLEAN/'\n",
        "MERGED_DIR = SCRAPE_DIR + 'MERGED_COUNTIES/'\n",
        "\n",
        "CURRENT_DATE = datetime.now(timezone('US/Eastern')).strftime('%m-%d-%Y')"
      ],
      "metadata": {
        "id": "pQLkhi85XBId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formatter for Consistency\n",
        "\n",
        "---\n",
        "\n",
        "We aren't sure if there can be keys other than the ones listed below. \n",
        "\n",
        "For previously unseen keys, we use str.upper() to give them a default value because we want to be able to automate the web scraper, so we want to handle errors without stopping.\n",
        "\n",
        "Any previously unseen keys should be added to the dictionary along with a value to ensure consistency.\n",
        "\n",
        "---\n",
        "\n",
        "## **Ensure all tuples end with a comma**\n",
        "\n",
        "If we have a tuple of a single element, it should look like:  \n",
        "(MY_ELEMENT,)\n",
        "\n",
        "If we do (MY_ELEMENT) this will get parsed character by character, since (MY_ELEMENT) is equivalent to the string MY_ELEMENT."
      ],
      "metadata": {
        "id": "4_iQCJCaWl4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CASES_CHARGES_SHARED_KEYS = ['caseNo', 'bondAmount', 'courtTime', 'bondType']\n",
        "INCONSISTENT_COLUMNS = ['Race', 'Sex', 'Eye Color', 'Hair Color', 'Complexion', 'Facial Hair']\n",
        "UNKNOWN_VAL = 'N/A'\n",
        "\n",
        "# Some columns are inconsistently named despite representing the same type of data\n",
        "MISNAMED_COLUMN_MAPPINGS = {\n",
        "    'Intake_num': 'Booking Number',\n",
        "    'Arrest Agency': 'Arresting Agency',\n",
        "    'Hair': 'Hair Color',\n",
        "    'Eye': 'Eye Color',\n",
        "}\n",
        "\n",
        "# We use many-to-one mappings (Dict[Tuple, str]) for convenience - easier to add mappings\n",
        "# We convert them to one-to-one dictionaries later\n",
        "RACE_MAPPINGS = {\n",
        "    ('B', 'Black', 'BLACK', 'African American',): 'BLACK',\n",
        "    ('W', 'White', 'WHITE', 'Caucasian',): 'WHITE',\n",
        "    ('H', 'Hispanic', 'HISPANIC',): 'HISPANIC',\n",
        "    ('A', 'ASIAN', 'Indian', 'INDIAN',): 'ASIAN',\n",
        "    ('AMERICAN', 'Native American',): 'INDIGENOUS',  # Unsure about 'AMERICAN'\n",
        "    ('MIDDLE EASTERN',): 'MIDDLE EASTERN',\n",
        "    ('Other', 'OTHER',): 'OTHER',\n",
        "    ('U', 'UNKNOWN', 'UNDECIDED', 'N/A',): UNKNOWN_VAL,\n",
        "}\n",
        "\n",
        "GENDER_MAPPINGS = {\n",
        "    ('F', 'Female', 'FEMALE',): 'FEMALE',\n",
        "    ('M', 'Male', 'MALE',): 'MALE',\n",
        "    ('', 'N/A'): UNKNOWN_VAL,\n",
        "}\n",
        "\n",
        "EYE_COLOR_MAPPINGS = {\n",
        "    ('GRN', 'Green', 'GREEN',): 'GREEN',\n",
        "    ('HAZ', 'Hazel', 'HAZEL',): 'HAZEL',\n",
        "    ('BLU', 'Blue', 'BLUE',): 'BLUE',\n",
        "    ('BRO', 'Brown', 'BROWN',): 'BROWN',\n",
        "    ('DARK BROWN',): 'DARK BROWN',\n",
        "    ('GRY', 'Gray', 'GRAY',): 'GREY',\n",
        "    ('BLK', 'Black', 'BLACK',): 'BLACK',\n",
        "    ('MAROON',): 'MAROON',\n",
        "    ('Other', 'OTHER',): 'OTHER',\n",
        "    ('UNKN', 'Unknown', 'UNKNOWN',): UNKNOWN_VAL,\n",
        "}\n",
        "\n",
        "HAIR_COLOR_MAPPINGS = {\n",
        "    ('BLK', 'Black', 'BLACK',): 'BLACK',\n",
        "    ('BRO', 'Brown', 'BROWN',): 'BROWN',\n",
        "    ('Blue', 'BLUE',): 'BLUE',\n",
        "    ('GRY', 'Gray', 'GRAY', 'GREY', 'GRAY/GRA',): 'GREY',\n",
        "    ('WHI', 'White', 'WHITE',): 'WHITE',\n",
        "    ('Red', 'RED', 'RED/AUBU', 'RED OR A', 'Auburn', 'AUBURN',): 'RED',\n",
        "    ('BAL', 'Bald', 'BALD', 'BALD/BAL',): 'BALD',\n",
        "    ('SDY', 'Sandy', 'SANDY',): 'SANDY',\n",
        "    ('BLN', 'Blond', 'BLOND', 'Blonde', 'BLONDE',): 'BLONDE',\n",
        "    ('ORANGE',): 'ORANGE',\n",
        "    ('GREEN',): 'GREEN',\n",
        "    ('Pink', 'PINK',): 'PINK',\n",
        "    ('Purple', 'PLE', 'PURPLE',): 'PURPLE',\n",
        "    ('MULTICOLORED', 'MIXED',): 'MULTICOLORED',\n",
        "    ('FROSTED',): 'FROSTED',\n",
        "    ('Other', 'OTHER',): 'OTHER',\n",
        "    ('XXX', 'Unknown', 'UNKNOWN', 'NONE',): UNKNOWN_VAL,\n",
        "}\n",
        "\n",
        "COMPLEXION_MAPPINGS = {\n",
        "    ('ALBINO',): 'ALBINO',\n",
        "    ('FAIR',): 'FAIR',\n",
        "    ('LIGHT',): 'LIGHT',\n",
        "    ('DARK',): 'DARK',\n",
        "    ('BLACK',): 'BLACK',\n",
        "    ('MEDIUM',): 'MEDIUM',\n",
        "    ('OLIVE',): 'OLIVE',\n",
        "    ('RUDDY',): 'RUDDY',\n",
        "    ('DARK BRO',): 'DARK BROWN',\n",
        "    ('MEDIUM B',): 'MEDIUM BLACK',\n",
        "    ('LIGHT BR',): 'LIGHT BROWN',\n",
        "    ('YELLOW',): 'YELLOW',\n",
        "    ('UNKNOWN',): UNKNOWN_VAL,\n",
        "}\n",
        "\n",
        "FACIAL_HAIR_MAPPINGS = {\n",
        "    ('BEARD',): 'BEARD',\n",
        "    ('CLEAN SH', 'NONE'): 'CLEAN SHAVED',\n",
        "    ('FU MANCH',): 'FU MANCHU',\n",
        "    ('FULL BEA',): 'FULL BEARD',\n",
        "    ('FUZZ',): 'FUZZ',\n",
        "    ('GOATEE',): 'GOATEE',\n",
        "    ('LOWER LI',): 'LOWER LIP',\n",
        "    ('MUSTACHE',): 'MUSTACHE',\n",
        "    ('SIDEBURN',): 'SIDEBURN',\n",
        "    ('THIN BEA',): 'THIN BEARD',\n",
        "    ('UNSHAVEN', 'UNKNOWN', 'NOT APPL',): UNKNOWN_VAL,\n",
        "}"
      ],
      "metadata": {
        "id": "xKGGqV4ckoYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoCfjWFYWgGS"
      },
      "outputs": [],
      "source": [
        "class Formatter():\n",
        "    class FormatterDict(dict):\n",
        "        \"\"\"Custom class to handle missing keys by subclassing dict. Currently returns str.upper() for missing keys.\"\"\"\n",
        "        def __missing__(self, key: str) -> str:\n",
        "            print(\"WARNING: Missing key:\", key)\n",
        "            return str.upper(key)\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.race_mappings = self._convert_mapping_to_dict(RACE_MAPPINGS)\n",
        "        self.gender_mappings = self._convert_mapping_to_dict(GENDER_MAPPINGS)\n",
        "        self.eye_color_mappings = self._convert_mapping_to_dict(EYE_COLOR_MAPPINGS)\n",
        "        self.hair_color_mappings = self._convert_mapping_to_dict(HAIR_COLOR_MAPPINGS)\n",
        "        self.complexion_mappings = self._convert_mapping_to_dict(COMPLEXION_MAPPINGS)\n",
        "        self.facial_hair_mappings = self._convert_mapping_to_dict(FACIAL_HAIR_MAPPINGS)\n",
        "\n",
        "    def get_mappings(self, name: str) -> Dict:\n",
        "        if name == 'Race':\n",
        "            return self.race_mappings\n",
        "        elif name == 'Sex':\n",
        "            return self.gender_mappings\n",
        "        elif name == 'Eye Color':\n",
        "            return self.eye_color_mappings\n",
        "        elif name == 'Hair Color':\n",
        "            return self.hair_color_mappings\n",
        "        elif name == 'Complexion':\n",
        "            return self.complexion_mappings\n",
        "        elif name == 'Facial Hair':\n",
        "            return self.facial_hair_mappings\n",
        "        else:\n",
        "            raise Exception(\"ERROR: Unrecognized name\", name)\n",
        "\n",
        "    def format_column(self, column: pd.Series) -> pd.Series:\n",
        "        # Ignore columns that are completely null\n",
        "        if column.isnull().all() == True:\n",
        "            return column\n",
        "\n",
        "        # Remove any trailing whitespace\n",
        "        column = column.str.rstrip()\n",
        "\n",
        "        mappings = self.get_mappings(column.name)\n",
        "        return column.map(mappings, na_action='ignore')\n",
        "\n",
        "    def format_row(self, row: pd.Series) -> pd.Series:\n",
        "\n",
        "        def format_str(value: str) -> str:\n",
        "            if value == '':\n",
        "                return None\n",
        "            return value\n",
        "\n",
        "        def format_money(value: Union[int, str, float]) -> str:\n",
        "            # print(type(value)) \n",
        "            # Somehow gets int/float sometimes, function takes string but type(value) returns float/int\n",
        "            if type(value) not in [str, int, float]:\n",
        "                print(\"Money value has different data type \", type(value))\n",
        "                return value\n",
        "            if value == 'N/A':\n",
        "                value = '0.00'\n",
        "\n",
        "            # We sometimes have commas in the bondAmount (ie. Jackson) so we get rid of those and recombine\n",
        "            if \",\" in str(value):\n",
        "                temp = value.split(\",\")\n",
        "                value = \"\".join(temp)\n",
        "            return f'{float(value):g}'\n",
        "\n",
        "        def format_serialized_dict(column_name: str) -> pd.Series:\n",
        "            row[column_name] = ast.literal_eval(row[column_name])\n",
        "            row[column_name] = [{k: format_str(v) for k, v in x.items()} for x in row[column_name]]\n",
        "            for x in row[column_name]:\n",
        "                x['bondAmount'] = format_money(x['bondAmount'])\n",
        "            return row\n",
        "\n",
        "        # Both Cases and Charges do not appear as columns\n",
        "        if 'Cases' not in row.index and 'Charges' not in row.index:\n",
        "            return row\n",
        "\n",
        "        # If both columns are there (misses one of each)\n",
        "        elif set(['Cases', 'Charges']).issubset(row.index):\n",
        "\n",
        "            # Read serialized list of dicts\n",
        "            # We can't use the json package because the JSON isn't serialized properly\n",
        "            # We have: {'name': 'value', 'name_2': None}\n",
        "            # Should be: {\"name\": \"value\", \"name_2\": null}\n",
        "            row['Cases'] = ast.literal_eval(row['Cases'])\n",
        "            row['Charges'] = ast.literal_eval(row['Charges'])\n",
        "\n",
        "            if len(row['Cases']) == 0:\n",
        "                row.drop('Cases', inplace=True)\n",
        "                return row\n",
        "            if len(row['Cases']) != len(row['Charges']):\n",
        "                # print(\"Different length\", len(row['Cases']), len(row['Charges']), row['Cases'], row['Charges'])\n",
        "                \n",
        "                # Duplicate the entry in row['Cases'] to match row['Charges']\n",
        "                if len(row['Cases']) == 1:\n",
        "                    row['Cases'] *= len(row['Charges'])\n",
        "                else:\n",
        "                    # print(\"Not 1:X ratio\", len(row['Cases']), len(row['Charges']), row['Cases'], row['Charges'])\n",
        "                    \n",
        "                    row['Cases'] = [{k: format_str(v) for k, v in x.items()} for x in row['Cases']]\n",
        "                    row['Charges'] = [{k: format_str(v) for k, v in x.items()} for x in row['Charges']]\n",
        "\n",
        "                    combined = []\n",
        "                    for case, charge in zip_longest(row['Cases'], row['Charges']):\n",
        "\n",
        "                        # The caseNo is the same\n",
        "                        if case and charge and case['caseNo'] == charge['caseNo'] and charge['caseNo'] != '':\n",
        "\n",
        "                            # Remove shared keys if the values between the two columns do not match and to merge the remaining key value pairs\n",
        "                            shared_keys = case.keys() & charge.keys()\n",
        "                            if len(shared_keys) != 0:\n",
        "                                for key in shared_keys:\n",
        "                                    if case[key] != charge[key]:\n",
        "                                        case.pop(key)\n",
        "                                        charge.pop(key)\n",
        "                                    else:\n",
        "                                        # Keep one\n",
        "                                        case.pop(key)\n",
        "                            \n",
        "                            # Merge by adding the different key value pairs\n",
        "                            case.update(charge)\n",
        "                            combined.append(case)\n",
        "\n",
        "                        else:\n",
        "                            if case:\n",
        "                                combined.append(case)\n",
        "                            if charge:\n",
        "                                combined.append(charge)\n",
        "\n",
        "                    row['Charges'] = combined\n",
        "                    row.drop('Cases', inplace=True)\n",
        "                    # print(len(row['Charges']))\n",
        "                    return row\n",
        "            \n",
        "            # Equal cases and charges length case, everything is 1:1 and in order\n",
        "            # Update values for consistency\n",
        "            row['Cases'] = [{k: format_str(v) for k, v in x.items()} for x in row['Cases']]\n",
        "            row['Charges'] = [{k: format_str(v) for k, v in x.items()} for x in row['Charges']]\n",
        "\n",
        "            combined = []\n",
        "            for case, charge in zip(row['Cases'], row['Charges']):\n",
        "                case['bondAmount'] = format_money(case['bondAmount'])\n",
        "                charge['bondAmount'] = format_money(charge['bondAmount'])\n",
        "\n",
        "                # Remove shared keys\n",
        "                for key in CASES_CHARGES_SHARED_KEYS:\n",
        "                    if case[key] != charge[key]:\n",
        "                        #print(\"WARNING: Different\", key, case[key], charge[key])\n",
        "                        #print(case)\n",
        "                        #print(charge)\n",
        "\n",
        "                        case.pop(key)\n",
        "                        charge.pop(key)\n",
        "                    else:\n",
        "                        case.pop(key)\n",
        "\n",
        "                # Check if case and charge share any keys\n",
        "                shared_keys = case.keys() & charge.keys()\n",
        "                if len(shared_keys) != 0:\n",
        "                    print(\"WARNING: Duplicate keys\", shared_keys)\n",
        "                    for key in shared_keys:\n",
        "                        if case[key] != charge[key]:\n",
        "                            case.pop(key)\n",
        "                            charge.pop(key)\n",
        "                        else:\n",
        "                            case.pop(key)\n",
        "                case.update(charge)\n",
        "                combined.append(case)\n",
        "\n",
        "            row['Charges'] = combined\n",
        "            row.drop('Cases', inplace=True)\n",
        "\n",
        "            return row\n",
        "\n",
        "        # Only Cases exists as a column\n",
        "        elif 'Cases' in row.index and 'Charges' not in row.index:\n",
        "            if len(row['Cases']) == 0:\n",
        "                return row\n",
        "            \n",
        "            return format_serialized_dict('Cases')\n",
        "\n",
        "        # Only Charges exists as a column\n",
        "        elif 'Cases' not in row.index and 'Charges' in row.index:\n",
        "            if len(row['Charges']) == 0:\n",
        "                return row\n",
        "\n",
        "            return format_serialized_dict('Charges')\n",
        "\n",
        "    def format_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df.rename(MISNAMED_COLUMN_MAPPINGS, axis='columns', inplace=True)\n",
        "\n",
        "        # Merge Cases and Charges\n",
        "        df = df.apply(self.format_row, axis=1)\n",
        "\n",
        "        # Remove any columns not found in the dataframe\n",
        "        columns_to_fix = [col for col in INCONSISTENT_COLUMNS if col in df.columns]\n",
        "\n",
        "        # Fix inconsistent data formatting\n",
        "        df[columns_to_fix] = df[columns_to_fix].apply(self.format_column)\n",
        "        return df\n",
        "\n",
        "    def _convert_mapping_to_dict(self, many_to_one: Dict) -> Dict:\n",
        "        \"\"\"Convert a many-to-one dictionary to a one-to-one dictionary.\n",
        "        For example:\n",
        "        {('key_1', 'key_2'): 'val'} -> {'key_1': 'val', 'key_2': 'val'}\n",
        "        \"\"\"\n",
        "        one_to_one = self.FormatterDict()\n",
        "        for key_tuple, val in many_to_one.items():\n",
        "            # Safety check for user error\n",
        "            if type(key_tuple) != tuple:\n",
        "                raise Exception(\"ERROR: Key tuple entered incorrectly!\", key_tuple)\n",
        "\n",
        "            for key in key_tuple:\n",
        "                one_to_one[key] = val\n",
        "        return one_to_one"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "G29mdHru5mfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clear_directory(dir_name: str) -> None:\n",
        "    for sub_dir, dirs, files in os.walk(dir_name):\n",
        "        for filename in files:\n",
        "            os.remove(f'{sub_dir}/{filename}')"
      ],
      "metadata": {
        "id": "qaOumMnb5nsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Data\n",
        "\n",
        "---\n",
        "\n",
        "This will automatically clean the data for the current day. If needed, this can be expanded to clean data from previous days."
      ],
      "metadata": {
        "id": "rKVrclAkXfFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATES_TO_CLEAN = os.listdir(DATA_DIR)\n",
        "DATES_TO_CLEAN = [CURRENT_DATE]\n",
        "formatter = Formatter()\n",
        "\n",
        "for date_to_clean in DATES_TO_CLEAN:\n",
        "    print(\"Cleaning date:\", date_to_clean)\n",
        "    dir_to_clean = f'{DATA_DIR}{date_to_clean}'\n",
        "    for sub_dir, _, files in os.walk(dir_to_clean):\n",
        "        # Create directory if needed and remove existing one\n",
        "        new_dir = f'{CLEAN_DIR}{date_to_clean}'\n",
        "        os.makedirs(new_dir, exist_ok=True)\n",
        "        clear_directory(new_dir)\n",
        "\n",
        "        for filename in files:\n",
        "            cur_file = f'{sub_dir}/{filename}'\n",
        "            df = pd.read_csv(cur_file)\n",
        "\n",
        "            # Ignore the index column present in some files\n",
        "            # https://stackoverflow.com/a/43983654\n",
        "            df.drop(df.columns[df.columns.str.contains('^Unnamed')], axis=1, inplace=True)\n",
        "\n",
        "            df = formatter.format_df(df)\n",
        "            df.to_csv(f'{new_dir}/{filename}', index=False)\n",
        "\n",
        "print(\"Finished cleaning\")"
      ],
      "metadata": {
        "id": "3XQGhTkoXgaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge Data\n",
        "\n",
        "---\n",
        "\n",
        "This will automatically merge the data for the current day. If needed, this can be expanded to merge data from previous days."
      ],
      "metadata": {
        "id": "NodOJjKQ7L_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DATES_TO_MERGE = os.listdir(CLEAN_DIR)\n",
        "DATES_TO_MERGE = [CURRENT_DATE]\n",
        "\n",
        "for date_to_merge in DATES_TO_MERGE:\n",
        "    print(\"Merging:\", date_to_merge)\n",
        "    dir_to_merge = f'{CLEAN_DIR}{date_to_merge}'\n",
        "    for sub_dir, _, files in os.walk(dir_to_merge):\n",
        "        # Create directory if needed and remove existing one\n",
        "        new_dir = f'{MERGED_DIR}{date_to_merge}'\n",
        "        os.makedirs(new_dir, exist_ok=True)\n",
        "        clear_directory(new_dir)\n",
        "\n",
        "        df_list = []\n",
        "        for filename in files:\n",
        "            cur_file = f'{sub_dir}/{filename}'\n",
        "            df = pd.read_csv(cur_file)\n",
        "            \n",
        "            # Add the county to the dataframe\n",
        "            df['County'] = filename.split('_')[-1].split('.')[0]\n",
        "            df_list.append(df)\n",
        "\n",
        "        df_merged = pd.concat(df_list, axis=0)\n",
        "        df_merged.to_csv(f'{new_dir}/{date_to_merge}_Merged.csv', index=False)\n",
        "        print(\"Merged\", len(df_list), \"counties\")\n",
        "\n",
        "print(\"Finished merging\")"
      ],
      "metadata": {
        "id": "SU3Dg7Th7Msf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}