{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clean_data.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "from pytz import timezone"
      ],
      "metadata": {
        "id": "bkMKEM0VXs-7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdlv6fdjXIOp",
        "outputId": "a61c4d48-5e87-49ca-e90f-a7da61ac4379"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('/content/drive/MyDrive/'):\n",
        "    raise Exception(\"Error: Mount Google Drive before continuing!\")\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/Data Science for Social Good - Spring 2022/data/'\n",
        "SCRAPE_DIR = BASE_DIR + 'scraped_files/'\n",
        "DATA_DIR = SCRAPE_DIR + 'DATA/'\n",
        "CLEAN_DIR = SCRAPE_DIR + 'CLEAN/'\n",
        "\n",
        "current_date = datetime.now(timezone('US/Eastern')).strftime(\"%m-%d-%Y\")"
      ],
      "metadata": {
        "id": "pQLkhi85XBId"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formatter for Consistency\n",
        "\n",
        "---\n",
        "\n",
        "We aren't sure if there can be keys other than the ones listed below. \n",
        "\n",
        "For previously unseen keys, we use str.upper() to give them a default value because we want to be able to automate the web scraper, so we want to handle errors without stopping.\n",
        "\n",
        "Any previously unseen keys should be added to the dictionary along with a value to ensure consistency.\n",
        "\n",
        "---\n",
        "\n",
        "## **Ensure all tuples end with a comma**\n",
        "\n",
        "If we have a tuple of a single element, it should look like:  \n",
        "(MY_ELEMENT,)\n",
        "\n",
        "If we do (MY_ELEMENT), this will get parsed character by character, since (MY_ELEMENT) is equivalent to the string MY_ELEMENT."
      ],
      "metadata": {
        "id": "4_iQCJCaWl4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "KoCfjWFYWgGS"
      },
      "outputs": [],
      "source": [
        "INCONSISTENT_COLUMNS = ['Race', 'Eye Color', 'Sex', 'Hair Color']\n",
        "UNKNOWN_VAL = 'N/A'\n",
        "\n",
        "\n",
        "class DefaultUpperDict(dict):\n",
        "    \"\"\"Custom class to handle missing values by subclassing dict. Currently returns str.upper() for missing keys.\"\"\"\n",
        "    def __missing__(self, key: str) -> str:\n",
        "        print(\"WARNING: Missing key:\", key)\n",
        "        return str.upper(key)\n",
        "\n",
        "\n",
        "# We use many-to-one mappings (Dict[Tuple, str]) for convenience - easier to add mappings\n",
        "# We convert them to one-to-one dictionaries later\n",
        "def format_column(column: pd.Series) -> pd.Series:\n",
        "    def convert_mapping_to_dict(many_to_one: Dict) -> Dict:\n",
        "        \"\"\"Convert a many-to-one dictionary to a one-to-one dictionary.\n",
        "        For example:\n",
        "        {('key_1', 'key_2'): 'val'} -> {'key_1': 'val', 'key_2': 'val'}\n",
        "        \"\"\"\n",
        "        one_to_one = DefaultUpperDict()\n",
        "        for key_tuple, val in many_to_one.items():\n",
        "            # Safety check for user error\n",
        "            if type(key_tuple) != tuple:\n",
        "                raise Exception(\"ERROR: Key tuple entered incorrectly!\", key_tuple)\n",
        "\n",
        "            for key in key_tuple:\n",
        "                one_to_one[key] = val\n",
        "        return one_to_one\n",
        "\n",
        "    if column.name == 'Race':\n",
        "        RACE_MAPPINGS = {\n",
        "            ('B', 'Black', 'BLACK',): 'BLACK',\n",
        "            ('W', 'White', 'WHITE',): 'WHITE',\n",
        "            ('H', 'Hispanic', 'HISPANIC',): 'HISPANIC',\n",
        "            ('A', 'INDIAN',): 'ASIAN',\n",
        "            ('Other',): 'OTHER',\n",
        "            ('U', 'UNKNOWN', 'N/A',): UNKNOWN_VAL,\n",
        "        }\n",
        "        mappings = convert_mapping_to_dict(RACE_MAPPINGS)\n",
        "    elif column.name == 'Eye Color':\n",
        "        EYE_COLOR_MAPPINGS = {\n",
        "            ('GRN', 'Green', 'GREEN',): 'GREEN',\n",
        "            ('HAZ', 'Hazel', 'HAZEL',): 'HAZEL',\n",
        "            ('BLU', 'Blue', 'BLUE',): 'BLUE',\n",
        "            ('BRO', 'Brown', 'BROWN',): 'BROWN',\n",
        "            ('DARK BROWN',): 'DARK BROWN',\n",
        "            ('GRY', 'Gray', 'GRAY',): 'GREY',\n",
        "            ('BLK', 'Black', 'BLACK',): 'BLACK',\n",
        "            ('MAROON',): 'MAROON',\n",
        "            ('UNKN', 'Unknown', 'UNKNOWN',): UNKNOWN_VAL,\n",
        "        }\n",
        "        mappings = convert_mapping_to_dict(EYE_COLOR_MAPPINGS)\n",
        "    elif column.name == 'Sex':\n",
        "        GENDER_MAPPINGS = {\n",
        "            ('F', 'Female', 'FEMALE',): 'FEMALE',\n",
        "            ('M', 'Male', 'MALE',): 'MALE',\n",
        "            ('',): UNKNOWN_VAL,\n",
        "        }\n",
        "        mappings = convert_mapping_to_dict(GENDER_MAPPINGS)\n",
        "    elif column.name == 'Hair Color':\n",
        "        HAIR_COLOR_MAPPINGS = {\n",
        "            ('BLK', 'Black', 'BLACK',): 'BLACK',\n",
        "            ('BRO', 'Brown', 'BROWN',): 'BROWN',\n",
        "            ('Blue', 'BLUE',): 'BLUE',\n",
        "            ('GRY', 'Gray', 'GRAY',): 'GREY',\n",
        "            ('WHI', 'White', 'WHITE',): 'WHITE',\n",
        "            ('Red', 'RED',): 'RED',\n",
        "            ('BAL', 'Bald', 'BALD',): 'BALD',\n",
        "            ('Auburn', 'AUBURN',): 'AUBURN',\n",
        "            ('Sandy', 'SANDY',): 'SANDY',\n",
        "            ('BLN', 'Blond', 'BLOND', 'Blonde', 'BLONDE',): 'BLONDE',\n",
        "            ('GREEN',): 'GREEN',\n",
        "            ('Pink', 'PINK',): 'PINK',\n",
        "            ('MULTICOLORED',): 'MULTICOLORED',\n",
        "            ('XXX', 'Unknown', 'NONE',): UNKNOWN_VAL,\n",
        "        }\n",
        "        mappings = convert_mapping_to_dict(HAIR_COLOR_MAPPINGS)\n",
        "    else:\n",
        "        raise Exception(\"ERROR: Unrecognized column name\", column.name)\n",
        "\n",
        "    # Remove any trailing whitespace\n",
        "    column = column.str.rstrip()\n",
        "\n",
        "    return column.map(mappings, na_action='ignore')\n",
        "\n",
        "def format_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Remove any columns not found in the dataframe\n",
        "    columns_to_fix = [col for col in INCONSISTENT_COLUMNS if col in df.columns]\n",
        "\n",
        "    # Fix inconsistent data formatting\n",
        "    df[columns_to_fix] = df[columns_to_fix].apply(format_column)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean Data\n",
        "\n",
        "---\n",
        "\n",
        "This will automatically clean the data for the current day. If needed, this can be expanded to clean data from previous days."
      ],
      "metadata": {
        "id": "rKVrclAkXfFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATES_TO_CLEAN = [current_date]\n",
        "\n",
        "for date_to_clean in DATES_TO_CLEAN:\n",
        "    print(\"Cleaning date:\", date_to_clean)\n",
        "    dir_to_clean = DATA_DIR + current_date\n",
        "    for sub_dir, dirs, files in os.walk(dir_to_clean):\n",
        "        for filename in files:\n",
        "            cur_file = f'{sub_dir}/{filename}'\n",
        "\n",
        "            df = pd.read_csv(cur_file)\n",
        "            df = format_df(df)\n",
        "\n",
        "            # Create directory if needed\n",
        "            new_dir = f'{CLEAN_DIR}{date_to_clean}'\n",
        "            os.makedirs(new_dir, exist_ok=True)\n",
        "\n",
        "            df.to_csv(f'{new_dir}/{filename}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XQGhTkoXgaY",
        "outputId": "d9630be7-575c-4887-a33d-4dda80a29905"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning date: 03-04-2022\n",
            "Cleaning date: 03-05-2022\n",
            "Cleaning date: 03-06-2022\n",
            "Cleaning date: 03-08-2022\n",
            "Cleaning date: 03-09-2022\n",
            "Cleaning date: 03-10-2022\n",
            "Cleaning date: 03-11-2022\n",
            "Cleaning date: 03-12-2022\n",
            "Cleaning date: 03-13-2022\n",
            "Cleaning date: 03-14-2022\n",
            "Cleaning date: 03-15-2022\n",
            "Cleaning date: 03-16-2022\n",
            "Cleaning date: 03-17-2022\n",
            "Cleaning date: 03-18-2022\n",
            "Cleaning date: 03-19-2022\n",
            "Cleaning date: 03-20-2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XCJnSArah3l_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}